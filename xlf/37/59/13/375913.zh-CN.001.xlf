<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="https://stackoverflow.com/questions/375913">
    <body>
      <group id="375913">
        <trans-unit id="474f686377fbd8012f361b7bd0929092b87eaedd" translate="yes" xml:space="preserve">
          <source>(The key is that we see &lt;code&gt;I&lt;/code&gt; more than once. If we only see it once, that doesn't tell us much except that &lt;code&gt;f&lt;/code&gt; &amp;gt; 0.)</source>
          <target state="translated">（关键是我们见到 &lt;code&gt;I&lt;/code&gt; 不止一次。如果我们只见过一次，那么除了 &lt;code&gt;f&lt;/code&gt; &amp;gt; 0之外，这并不能告诉我们太多信息。）</target>
        </trans-unit>
        <trans-unit id="6bddfcbf9effc6af0dc2a4017b8af4d6d8c3b612" translate="yes" xml:space="preserve">
          <source>(not so good for multi-threads, function pointers)</source>
          <target state="translated">(对于多线程、函数指针来说不是那么好用)</target>
        </trans-unit>
        <trans-unit id="ba000b98349f454ff4d963d272eea56d9076b1f0" translate="yes" xml:space="preserve">
          <source>(wikipedia) Valgrind is in essence a virtual
  machine using just-in-time (JIT)
  compilation techniques, including
  dynamic recompilation. Nothing from
  the original program ever gets run
  directly on the host processor.
  Instead, Valgrind first translates the
  program into a temporary, simpler form
  called Intermediate Representation
  (IR), which is a processor-neutral,
  SSA-based form. After the conversion,
  a tool (see below) is free to do
  whatever transformations it would like
  on the IR, before Valgrind translates
  the IR back into machine code and lets
  the host processor run it.</source>
          <target state="translated">(wikipedia)Valgrind本质上是一个虚拟机,它使用了即时编译技术(JIT),包括动态重新编译。原始程序中的任何内容都不会直接在主处理器上运行。相反,Valgrind首先将程序翻译成一个临时的、更简单的形式,称为Intermediate Representation (IR),这是一个与处理器无关的、基于SSA的形式。在转换之后,一个工具(见下文)可以自由地在IR上做任何它想做的转换,然后Valgrind将IR翻译成机器代码,让主处理器运行。</target>
        </trans-unit>
        <trans-unit id="2f52699e83e4b55e1f403f7c6079898edd6b4dff" translate="yes" xml:space="preserve">
          <source>- Above function , there is a list of functions that call the function .</source>
          <target state="translated">-上面的函数,有一个调用函数的函数列表。</target>
        </trans-unit>
        <trans-unit id="97381d92a175352738f31c6b84ed1b50492ba724" translate="yes" xml:space="preserve">
          <source>- Below function , there is a list of functions that are called by the function .</source>
          <target state="translated">-在函数下面,有一个被函数调用的函数列表。</target>
        </trans-unit>
        <trans-unit id="2a06d6944b31ca287d1197fb91866bf150b9273f" translate="yes" xml:space="preserve">
          <source>- In each section, one function is marked with an index number.</source>
          <target state="translated">-在每一节中,都有一个功能用索引号标记。</target>
        </trans-unit>
        <trans-unit id="579a41d12eaf05b9059712857073fbbb03bff3f1" translate="yes" xml:space="preserve">
          <source>- how many seconds were spent in a function&amp;mdash;including and excluding calls to sub-functions,</source>
          <target state="translated">-一个功能花费了多少秒-包括但不包括对子功能的调用，</target>
        </trans-unit>
        <trans-unit id="2e6d2ff4ea07291f0fe68b6a1592baa107889c9d" translate="yes" xml:space="preserve">
          <source>- the average time per call.</source>
          <target state="translated">-每次通话的平均时间。</target>
        </trans-unit>
        <trans-unit id="09049524fc0fe3657417bd54beda8ddfcc7d00b5" translate="yes" xml:space="preserve">
          <source>- the number of calls,</source>
          <target state="translated">-呼叫次数。</target>
        </trans-unit>
        <trans-unit id="726e946052f1dbb2b5959a244967231f79036c38" translate="yes" xml:space="preserve">
          <source>- what percentage of the overall time was spent for the function,</source>
          <target state="translated">-为这一职能花费了多少时间;</target>
        </trans-unit>
        <trans-unit id="e299449c036492af510a743e9982f023ddc5e7c0" translate="yes" xml:space="preserve">
          <source>1- Flat profiling:</source>
          <target state="translated">1-平面剖面图。</target>
        </trans-unit>
        <trans-unit id="961cb4afe9142078530eb983e36df03834cb40bb" translate="yes" xml:space="preserve">
          <source>2- graph profiling</source>
          <target state="translated">2-图谱分析</target>
        </trans-unit>
        <trans-unit id="45d30831218fba6255f04ee5d6cb69901aae44b4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;main&lt;/code&gt; calls &lt;code&gt;fast&lt;/code&gt; and &lt;code&gt;maybe_slow&lt;/code&gt; 3 times, one of the &lt;code&gt;maybe_slow&lt;/code&gt; calls being slow</source>
          <target state="translated">&lt;code&gt;main&lt;/code&gt; 通话 &lt;code&gt;fast&lt;/code&gt; ， &lt;code&gt;maybe_slow&lt;/code&gt; 通话 3次，其中 &lt;code&gt;maybe_slow&lt;/code&gt; 通话速度很慢</target>
        </trans-unit>
        <trans-unit id="2b26d60b77140f5edaf92172d3607731805526d8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;perf&lt;/code&gt; seems to use exclusively Linux kernel sampling mechanisms. This makes it very simple to setup, but also not fully accurate.</source>
          <target state="translated">&lt;code&gt;perf&lt;/code&gt; 似乎仅使用Linux内核采样机制。 这使得设置非常简单，但也不完全准确。</target>
        </trans-unit>
        <trans-unit id="beaa2542bdd5cb48d735e110bbd66e279a970967" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;valgrind&lt;/code&gt; with the combination of &lt;code&gt;callrind&lt;/code&gt; and &lt;code&gt;kcachegrind&lt;/code&gt; should provide a decent estimation on the points above and once it's established that there are issues with some section of code, I'd suggest do a micro bench mark &lt;code&gt;google benchmark&lt;/code&gt; is a good place to start.</source>
          <target state="translated">&lt;code&gt;valgrind&lt;/code&gt; 与 &lt;code&gt;callrind&lt;/code&gt; 和 &lt;code&gt;kcachegrind&lt;/code&gt; 的结合应该对以上几点提供一个不错的估计，并且一旦确定某些代码段存在问题，我建议您做一个微型基准 &lt;code&gt;google benchmark&lt;/code&gt; 是一个不错的起点。</target>
        </trans-unit>
        <trans-unit id="558433641cab85097025b8b72bc823c8bf5e82ad" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;&lt;code&gt;perf&lt;/code&gt; from &lt;code&gt;linux-tools&lt;/code&gt;&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;来自 &lt;code&gt;linux-tools&lt;/code&gt; &lt;code&gt;perf&lt;/code&gt; &lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="1e49cc3868d9deb4e8335aa49828e8c34898c8a8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;&lt;em&gt;For CPU bound applications:&lt;/em&gt;&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;&lt;em&gt;对于受CPU约束的应用程序：&lt;/em&gt;&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="7196658c799f4043d28d713837910b362f91ab93" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;&lt;em&gt;For I/O bound applications:&lt;/em&gt;&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;&lt;em&gt;对于I / O绑定的应用程序：&lt;/em&gt;&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="ad2cef4043d111e1ae8c4692b0d3548a6613fcbd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Intel VTune is the best (free for educational purposes).&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;英特尔VTune是最好的（用于教育目的免费）。&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="a2b5b23586b55133642cbd2c4d931e703f33536e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Others:&lt;/strong&gt; AMD Codeanalyst (since replaced with AMD CodeXL), OProfile, 'perf' tools (apt-get install linux-tools)</source>
          <target state="translated">&lt;strong&gt;其他：&lt;/strong&gt; AMD Codeanalyst（已由AMD CodeXL取代），OProfile，&amp;ldquo;性能&amp;rdquo;工具（apt-get install linux-tools）</target>
        </trans-unit>
        <trans-unit id="9f805de8a6187ff8c605dc1b87c1dbf750af8806" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Survey of C++ profiling techniques&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;C ++分析技术概述&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="5935609263c49f484c816bb47bd583d8ac724622" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Use Valgrind, callgrind and kcachegrind:&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;使用Valgrind，callgrind和kcachegrind：&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="f76b5fe3b1c9563ed7d1caedae895977e4729ca3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Use google-perftools:&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;使用google-perftools：&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="eaf91a71c594a624723a2785654641c0d49b648f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Use gprof (add -pg):&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;使用gprof（添加-pg）：&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="e2938d732361a260a7c9ca67f5add9a7cbbbdb25" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gperftools&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;gperftools&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="b34e9a3fe97464e4681d8532723054a812e2edcd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gprof&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;gprof&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="a645022e82b11913d78a887f81582e01e0a2ca4c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;valgrind callgrind&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;valgrind callgrind&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="b68ca6f37b8016941f5283f36f3759df87ef2002" translate="yes" xml:space="preserve">
          <source>A few other buzzwords if &lt;code&gt;gprof&lt;/code&gt; does not do the job for you: &lt;a href=&quot;http://en.wikipedia.org/wiki/Valgrind&quot;&gt;Valgrind&lt;/a&gt;, Intel &lt;a href=&quot;http://en.wikipedia.org/wiki/VTune&quot;&gt;VTune&lt;/a&gt;, Sun &lt;a href=&quot;http://en.wikipedia.org/wiki/DTrace&quot;&gt;DTrace&lt;/a&gt;.</source>
          <target state="translated">如果 &lt;code&gt;gprof&lt;/code&gt; 不能满足您的要求，则还有其他一些流行词： &lt;a href=&quot;http://en.wikipedia.org/wiki/Valgrind&quot;&gt;Valgrind&lt;/a&gt; ，Intel &lt;a href=&quot;http://en.wikipedia.org/wiki/VTune&quot;&gt;VTune&lt;/a&gt; ，Sun &lt;a href=&quot;http://en.wikipedia.org/wiki/DTrace&quot;&gt;DTrace&lt;/a&gt; 。</target>
        </trans-unit>
        <trans-unit id="7c42306a937ace4fd5a87d18a701b72889554283" translate="yes" xml:space="preserve">
          <source>ADDED, to give an intuitive feel for the difference between measuring and random stack sampling:</source>
          <target state="translated">增加了,让人直观地感受到测量和随机堆栈采样的区别。</target>
        </trans-unit>
        <trans-unit id="9276e16e7cc14ecf9ae00f247b4196166d5d1aeb" translate="yes" xml:space="preserve">
          <source>ADDED: Let me make a Bayesian explanation of how it works.  Suppose there is some instruction &lt;code&gt;I&lt;/code&gt; (call or otherwise) which is on the call stack some fraction &lt;code&gt;f&lt;/code&gt; of the time (and thus costs that much). For simplicity, suppose we don't know what &lt;code&gt;f&lt;/code&gt; is, but assume it is either 0.1, 0.2, 0.3, ... 0.9, 1.0, and the prior probability of each of these possibilities is 0.1, so all of these costs are equally likely a-priori.</source>
          <target state="translated">添加：让我对它的工作方式进行贝叶斯解释。 假设有一条指令 &lt;code&gt;I&lt;/code&gt; （调用或其他方式）在调用堆栈上占时间的比例为 &lt;code&gt;f&lt;/code&gt; （因此花费了很多）。 为简单起见，假设我们不知道 &lt;code&gt;f&lt;/code&gt; 是多少，但是假设它是0.1、0.2、0.3，... 0.9、1.0，并且每种可能性的先验概率为0.1，因此所有这些成本均相等可能是先验的。</target>
        </trans-unit>
        <trans-unit id="830b0bd9a847a1c514fcf9b261f15a81a0e87159" translate="yes" xml:space="preserve">
          <source>Actually a bit surprised not many mentioned about &lt;a href=&quot;https://github.com/google/benchmark&quot;&gt;google/benchmark&lt;/a&gt; , while it is a bit cumbersome to pin the specific area of code, specially if the code base is a little big one, however I found this really helpful when used in combination with &lt;code&gt;callgrind&lt;/code&gt;</source>
          <target state="translated">其实，关于&lt;a href=&quot;https://github.com/google/benchmark&quot;&gt;google / benchmark的&lt;/a&gt;提及并不多，这让您感到惊讶，虽然固定代码的特定区域有些麻烦，特别是在代码库较大的情况下，但是我发现这与 &lt;code&gt;callgrind&lt;/code&gt; 结合使用时确实很有帮助</target>
        </trans-unit>
        <trans-unit id="d081153271ba73f6b08b5c733dfc600113ac8050" translate="yes" xml:space="preserve">
          <source>Added: It might not be obvious, but the stack sampling technique works equally well in the presence of recursion. The reason is that the time that would be saved by removal of an instruction is approximated by the fraction of samples containing it, regardless of the number of times it may occur within a sample.</source>
          <target state="translated">补充:这可能不是很明显,但堆栈采样技术在存在递归的情况下同样有效。原因是,无论一条指令在一个样本中可能出现多少次,删除一条指令所节省的时间都是由包含该指令的样本的分数来近似。</target>
        </trans-unit>
        <trans-unit id="c111026e3becc4657525d1504f2a98b1227fa293" translate="yes" xml:space="preserve">
          <source>After running with either of those methods, we get a &lt;code&gt;prof.out&lt;/code&gt; profile data file as output. We can view that file graphically as an SVG with:</source>
          <target state="translated">使用这些方法之一运行后，我们将获得一个 &lt;code&gt;prof.out&lt;/code&gt; 配置文件数据文件作为输出。 我们可以使用SVG以图形方式查看该文件：</target>
        </trans-unit>
        <trans-unit id="938c0204782973ed2cee8235fd6ed9aad9bd19a0" translate="yes" xml:space="preserve">
          <source>Also worth mentioning are</source>
          <target state="translated">值得一提的还有</target>
        </trans-unit>
        <trans-unit id="30c36d00bdcf5c119d2a78569796b3b993fd0320" translate="yes" xml:space="preserve">
          <source>Also, if we go on the bottom right &quot;Call Graph&quot; tab, we see a call graph which we can export by right clicking it to obtain the following image with unreasonable amounts of white border :-)</source>
          <target state="translated">另外,如果我们在右下角的 &quot;调用图 &quot;选项卡上,我们可以看到一个调用图,通过右键导出,可以得到下面的图片,并带有不合理的白边框:-)</target>
        </trans-unit>
        <trans-unit id="1ac3bae0296e72365db70cdbd1d168992f9eb4f9" translate="yes" xml:space="preserve">
          <source>Alternatively, we can also get some textual data with:</source>
          <target state="translated">另外,我们还可以通过一些文字数据来获取一些文字数据。</target>
        </trans-unit>
        <trans-unit id="6fe0b9fdb5d375e1bdddd4970a95aeb96c822b66" translate="yes" xml:space="preserve">
          <source>Alternatively, we can also observe the text output of the &lt;code&gt;gprof&lt;/code&gt; built-in binutils tool which we previously saved at:</source>
          <target state="translated">另外，我们还可以观察 &lt;code&gt;gprof&lt;/code&gt; 内置binutils工具的文本输出，该工具先前保存在以下位置：</target>
        </trans-unit>
        <trans-unit id="d9dc64edf60dc7f5a3736a9279e32f3c03a13aa5" translate="yes" xml:space="preserve">
          <source>Alternatively, we can build the library in at link time, dispensing passing &lt;code&gt;LD_PRELOAD&lt;/code&gt; at runtime:</source>
          <target state="translated">或者，我们可以在链接时构建库，在运行时分配通过 &lt;code&gt;LD_PRELOAD&lt;/code&gt; 的代码：</target>
        </trans-unit>
        <trans-unit id="2cf1c27c41cddb303f6c128115968d14017c5f27" translate="yes" xml:space="preserve">
          <source>Another objection I often hear is: &quot;&lt;em&gt;It will stop someplace random, and it will miss the real problem&lt;/em&gt;&quot;.
This comes from having a prior concept of what the real problem is.
A key property of performance problems is that they defy expectations.
Sampling tells you something is a problem, and your first reaction is disbelief.
That is natural, but you can be sure if it finds a problem it is real, and vice-versa.</source>
          <target state="translated">我经常听到的另一个反对意见是：&amp;ldquo; &lt;em&gt;它将在某个地方随机停止，并且将错过真正的问题&lt;/em&gt; &amp;rdquo;。 这源于对实际问题有一个先验的概念。 性能问题的一个关键特性是它们无法兑现预期。 抽样告诉您某些问题，而您的第一个反应是难以置信。 那是很自然的，但是您可以确定它是否发现了真正的问题，反之亦然。</target>
        </trans-unit>
        <trans-unit id="fd2b8eb720b92068f67a32658ccfc03d658f34d9" translate="yes" xml:space="preserve">
          <source>Another perf GUI interfaces which might be worth it include:</source>
          <target state="translated">另一 perf GUI界面,可能是值得的,包括。</target>
        </trans-unit>
        <trans-unit id="48122e4897dd407d27c90182539de52d682b669c" translate="yes" xml:space="preserve">
          <source>Another tool build upon Valgrind is Massif. I use it to profile heap memory usage. It works great. What it does is that it gives you snapshots of memory usage -- detailed information WHAT holds WHAT percentage of memory, and WHO had put it there. Such information is available at different points of time of application run.</source>
          <target state="translated">另一个基于Valgrind的工具是Massif。我用它来描述堆内存使用情况。它的作用很大。它的作用是,它能给你提供内存使用情况的快照----详细的信息是什么东西占据了什么比例的内存,以及谁把它放在那里。这些信息可以在应用程序运行的不同时间点上得到。</target>
        </trans-unit>
        <trans-unit id="26cfea3667d4951989c3328a2749151be15581ab" translate="yes" xml:space="preserve">
          <source>Arm MAP is the profiler for parallel, multithreaded or single threaded C, C++, Fortran and F90 codes.  It provides in-depth analysis and bottleneck pinpointing to the source line.  Unlike most profilers, it's designed to be able to profile pthreads, OpenMP or MPI for parallel and threaded code.</source>
          <target state="translated">Arm MAP是用于并行、多线程或单线程的C、C++、Fortran和F90代码的剖析器。它提供了深入的分析和瓶颈定位到源码线上。与大多数的剖析器不同,它的设计是为了能够对pthreads、OpenMP或MPI的并行和线程代码进行剖析。</target>
        </trans-unit>
        <trans-unit id="f607a78951293acbad31ccf19fd2a9c3fc0343e3" translate="yes" xml:space="preserve">
          <source>As a very quick summary for each section e.g.:</source>
          <target state="translated">作为对每一节的快速总结,例如:</target>
        </trans-unit>
        <trans-unit id="1361188e9146c170f0554ec525ed827cb7592cc2" translate="yes" xml:space="preserve">
          <source>As no one mentioned Arm MAP, I'd add it as personally I have successfully used Map to profile a C++ scientific program.</source>
          <target state="translated">由于没有人提到Arm MAP,我想补充一下,因为我个人已经成功地用Map来描述一个C++科学程序。</target>
        </trans-unit>
        <trans-unit id="33441507f930cc628d62c6420d02d66548f47be3" translate="yes" xml:space="preserve">
          <source>At runtime, we have to pass set the &lt;code&gt;LD_PRELOAD&lt;/code&gt; to point to &lt;code&gt;libprofiler.so&lt;/code&gt;, which you can find with &lt;code&gt;locate libprofiler.so&lt;/code&gt;, e.g. on my system:</source>
          <target state="translated">在运行时，我们必须将 &lt;code&gt;LD_PRELOAD&lt;/code&gt; 设置为指向 &lt;code&gt;libprofiler.so&lt;/code&gt; ，您可以在 &lt;code&gt;locate libprofiler.so&lt;/code&gt; ，例如在我的系统上：</target>
        </trans-unit>
        <trans-unit id="275c32d1f4761fd0f70f1e315294ca1da95e4155" translate="yes" xml:space="preserve">
          <source>At work we have a really nice tool that helps us monitoring what we want in terms of scheduling. This has been useful numerous times.</source>
          <target state="translated">在工作中,我们有一个非常好的工具,可以帮助我们监控我们想要的日程安排。这个工具已经用了无数次了。</target>
        </trans-unit>
        <trans-unit id="c762222f2d1b7501653b936c0a39cd0b1aa6a933" translate="yes" xml:space="preserve">
          <source>Be sure to add &lt;code&gt;-pg&lt;/code&gt; to compilation before profiling:</source>
          <target state="translated">在进行概要分析之前，请确保将 &lt;code&gt;-pg&lt;/code&gt; 添加到编译中：</target>
        </trans-unit>
        <trans-unit id="d55e4a545eefcf858705baa9f8981e8dc001934b" translate="yes" xml:space="preserve">
          <source>Because we compiled with &lt;code&gt;-pg&lt;/code&gt;, running the program produces a file &lt;code&gt;gmon.out&lt;/code&gt; file containing the profiling data.</source>
          <target state="translated">由于我们使用 &lt;code&gt;-pg&lt;/code&gt; 进行编译，因此运行该程序将生成一个包含分析数据的文件 &lt;code&gt;gmon.out&lt;/code&gt; 文件。</target>
        </trans-unit>
        <trans-unit id="0e20564692b92dee75c5d2d38aa97448c7b86382" translate="yes" xml:space="preserve">
          <source>But this has the downside that you have to first convert the data to the Common Trace Format, which can be done with &lt;code&gt;perf data --to-ctf&lt;/code&gt;, but it needs to be enabled at build time/have &lt;code&gt;perf&lt;/code&gt; new enough, either of which is not the case for the perf in Ubuntu 18.04</source>
          <target state="translated">但这有一个缺点，您必须首先将数据转换为通用跟踪格式，可以使用 &lt;code&gt;perf data --to-ctf&lt;/code&gt; 来完成，但是需要在构建时启用它/必须有足够的 &lt;code&gt;perf&lt;/code&gt; 新特性， Ubuntu 18.04中的性能不是这种情况</target>
        </trans-unit>
        <trans-unit id="7940ac6d6de4ba483d9863a1db60f99151f2c050" translate="yes" xml:space="preserve">
          <source>By default, this produces an extremely verbose output that explains what the output data means. Since I can't explain better than that, I'll let you read it yourself.</source>
          <target state="translated">默认情况下,这就会产生一个极其繁琐的输出,解释输出数据的含义。既然我无法解释得更好,那我就让你自己看吧。</target>
        </trans-unit>
        <trans-unit id="0e030095a99fb3257e11106bd611c56f45c2b3ed" translate="yes" xml:space="preserve">
          <source>Callgrind is a profiler build upon that. Main benefit is that you don't have to run your aplication for hours to get reliable result. Even one second run is sufficient to get rock-solid, reliable results, because Callgrind is a &lt;strong&gt;non-probing&lt;/strong&gt; profiler.</source>
          <target state="translated">Callgrind是基于此的探查器。 主要好处是您不必花费数小时即可完成可靠的结果。 因为Callgrind &lt;strong&gt;是非探测&lt;/strong&gt;轮廓仪，所以即使是一秒钟的运行也足以获得坚如磐石的可靠结果。</target>
        </trans-unit>
        <trans-unit id="5fea0991e44d8690969eca7425688fa735bb73f4" translate="yes" xml:space="preserve">
          <source>Can you find the most critical call stack easily with all those tiny unsorted spaghetti lines going over one another? There might be better &lt;code&gt;dot&lt;/code&gt; options I'm sure, but I don't want to go there now. What we really need is a proper dedicated viewer for it, but I haven't found one yet:</source>
          <target state="translated">将所有这些细小的未分类的意大利细面条线相互重叠，您能轻松找到最关键的呼叫堆栈吗？ 我确定可能会有更好的 &lt;code&gt;dot&lt;/code&gt; 选项，但我现在不想去那里。 我们真正需要的是一个合适的专用查看器，但我还没有找到一个：</target>
        </trans-unit>
        <trans-unit id="20f2f816399d50a15985c861ae31ff174b730293" translate="yes" xml:space="preserve">
          <source>Caveat: Programmers tend to be skeptical of this technique unless they've used it themselves. They will say that profilers give you this information, but that is only true if they sample the entire call stack, and then let you examine a random set of samples. (The summaries are where the insight is lost.) Call graphs don't give you the same information, because</source>
          <target state="translated">注意事项:程序员往往会对这种技术持怀疑态度,除非他们自己用过。他们会说,剖析器可以给你这些信息,但这只有在他们对整个调用堆栈进行采样,然后让你检查一组随机采样的情况下才是真的。(摘要是失去了洞察力的地方。)呼叫图不会给你同样的信息,因为</target>
        </trans-unit>
        <trans-unit id="099dc36893aed4eafd86bced522e4eb9937e5b98" translate="yes" xml:space="preserve">
          <source>Eclipse Trace Compass plugin: &lt;a href=&quot;https://www.eclipse.org/tracecompass/&quot;&gt;https://www.eclipse.org/tracecompass/&lt;/a&gt;</source>
          <target state="translated">Eclipse Trace Compass插件： &lt;a href=&quot;https://www.eclipse.org/tracecompass/&quot;&gt;https&lt;/a&gt; ： //www.eclipse.org/tracecompass/</target>
        </trans-unit>
        <trans-unit id="b0500648de7f8cd809c18dfe1a12b0af1850fcf1" translate="yes" xml:space="preserve">
          <source>First install gperftools with:</source>
          <target state="translated">首先安装gperftools与:</target>
        </trans-unit>
        <trans-unit id="f718a4c61c9b86b8efe3e4a514c5978b1cf993ee" translate="yes" xml:space="preserve">
          <source>First we have to remove the &lt;code&gt;-pg&lt;/code&gt; flag to go back to normal compilation, otherwise the run actually fails with &lt;a href=&quot;https://stackoverflow.com/questions/2146082/valgrind-profiling-timer-expired&quot;&gt;&lt;code&gt;Profiling timer expired&lt;/code&gt;&lt;/a&gt;, and yes, this is so common that I did and there was a Stack Overflow question for it.</source>
          <target state="translated">首先，我们必须删除 &lt;code&gt;-pg&lt;/code&gt; 标志以返回正常编译，否则运行实际上会因&lt;a href=&quot;https://stackoverflow.com/questions/2146082/valgrind-profiling-timer-expired&quot;&gt; &lt;code&gt;Profiling timer expired&lt;/code&gt; &lt;/a&gt;而失败，并且是的，这是如此常见，以至于我遇到了堆栈溢出问题。</target>
        </trans-unit>
        <trans-unit id="731b082498e3f526227d4ecd0e76236c0bf482bd" translate="yes" xml:space="preserve">
          <source>First, &lt;code&gt;time&lt;/code&gt; tells us that the execution time with and without &lt;code&gt;-pg&lt;/code&gt; were the same, which is great: no slowdown! I have however seen accounts of 2x - 3x slowdowns on complex software, e.g. as &lt;a href=&quot;https://gem5.atlassian.net/browse/GEM5-337&quot;&gt;shown in this ticket&lt;/a&gt;.</source>
          <target state="translated">首先， &lt;code&gt;time&lt;/code&gt; 告诉我们使用 &lt;code&gt;-pg&lt;/code&gt; 和不使用-pg的执行时间是相同的，这很棒：不放慢速度！ 但是，我看到复杂软件的速度下降了2到3倍，例如&lt;a href=&quot;https://gem5.atlassian.net/browse/GEM5-337&quot;&gt;此票证所示&lt;/a&gt; 。</target>
        </trans-unit>
        <trans-unit id="bcac6f94f0587cb573e61450977a4e3d79d79994" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;-O3&lt;/code&gt;, see here like in the graphical output that &lt;code&gt;maybe_slow&lt;/code&gt; and &lt;code&gt;fast&lt;/code&gt; don't have a known parent, which is what the documentation says that &lt;code&gt;&amp;lt;spontaneous&amp;gt;&lt;/code&gt; means.</source>
          <target state="translated">对于 &lt;code&gt;-O3&lt;/code&gt; ，如图形输出所示，在这里可能 &lt;code&gt;maybe_slow&lt;/code&gt; 和 &lt;code&gt;fast&lt;/code&gt; 没有已知的父代，这就是文档所说的 &lt;code&gt;&amp;lt;spontaneous&amp;gt;&lt;/code&gt; 的意思。</target>
        </trans-unit>
        <trans-unit id="06d456a661f5b9ba3d3687e35852bf5858fab879" translate="yes" xml:space="preserve">
          <source>For CPU, the reason for profiling in &lt;strong&gt;DEBUG&lt;/strong&gt; mode is because if your tried profiling in &lt;strong&gt;RELEASE&lt;/strong&gt; mode, the compiler is going to reduce math, vectorize loops, and inline functions which tends to glob your code into an un-mappable mess when it's assembled. &lt;strong&gt;An un-mappable mess means your profiler will not be able to clearly identify what is taking so long because the assembly may not correspond to the source code under optimization&lt;/strong&gt;. If you need the performance (e.g. timing sensitive) of &lt;strong&gt;RELEASE&lt;/strong&gt; mode, disable debugger features as needed to keep a usable performance.</source>
          <target state="translated">对于CPU，在&lt;strong&gt;DEBUG&lt;/strong&gt;模式下进行性能分析的原因是，如果在&lt;strong&gt;RELEASE&lt;/strong&gt;模式下尝试进行性能分析，则编译器将减少数学运算，向量化循环和内联函数，这在汇编代码时往往会使您的代码陷入无法映射的混乱状态。 &lt;strong&gt;不可映射的混乱意味着您的探查器将无法清楚地识别花费了很长时间的事情，因为程序集可能与优化后的源代码不符&lt;/strong&gt; 。 如果您需要&lt;strong&gt;RELEASE&lt;/strong&gt;模式的性能（例如，定时敏感），请根据需要禁用调试器功能以保持可用的性能。</target>
        </trans-unit>
        <trans-unit id="1fd1a4e3732d0165baa94d16658f7291d1e77cb2" translate="yes" xml:space="preserve">
          <source>For I/O-bound, the profiler can still identify I/O operations in &lt;strong&gt;RELEASE&lt;/strong&gt; mode because I/O operations are either externally linked to a shared library (most of the time) or in the worst case, will result in a sys-call interrupt vector (which is also easily identifiable by the profiler).</source>
          <target state="translated">对于受I / O限制的事件，探查器仍可以在&lt;strong&gt;RELEASE&lt;/strong&gt;模式下识别I / O操作，因为I / O操作（大多数情况下）是外部链接到共享库的，或者在最坏的情况下，将导致系统崩溃。调用中断向量（事件探查器也可以轻松识别）。</target>
        </trans-unit>
        <trans-unit id="da7a5db11d6d72fa7a5c6be30597dedf122892f3" translate="yes" xml:space="preserve">
          <source>For educational reasons, we will also do a run without optimizations enabled. Note that this is useless in practice, as you normally only care about optimizing the performance of the optimized program:</source>
          <target state="translated">出于教育意义,我们也会在不启用优化的情况下进行运行。注意,这在实践中是没有用的,因为你通常只关心优化程序的性能优化。</target>
        </trans-unit>
        <trans-unit id="59ad9936a7b0f4788c6ad3b0ea79e48e4a42d309" translate="yes" xml:space="preserve">
          <source>For single-threaded programs you can use &lt;strong&gt;igprof&lt;/strong&gt;, The Ignominous Profiler: &lt;a href=&quot;https://igprof.org/&quot;&gt;https://igprof.org/&lt;/a&gt; .</source>
          <target state="translated">对于单线程程序，可以使用&lt;strong&gt;igprof&lt;/strong&gt; ，即Ignominous Profiler： &lt;a href=&quot;https://igprof.org/&quot;&gt;https&lt;/a&gt; ://igprof.org/。</target>
        </trans-unit>
        <trans-unit id="e32962da9446269770cb3283319d83603bd59de6" translate="yes" xml:space="preserve">
          <source>HPCToolkit (&lt;a href=&quot;http://hpctoolkit.org/&quot;&gt;http://hpctoolkit.org/&lt;/a&gt;) - Open-source, works for parallel programs and has a GUI with which to look at the results multiple ways</source>
          <target state="translated">HPCToolkit（ &lt;a href=&quot;http://hpctoolkit.org/&quot;&gt;http://hpctoolkit.org/&lt;/a&gt; ）-开源，适用于并行程序，并具有可通过多种方式查看结果的GUI</target>
        </trans-unit>
        <trans-unit id="9717437adb9e366c40c178b8b430e79730bf5d20" translate="yes" xml:space="preserve">
          <source>Here is an off-the-cuff illustration of the difference between examining measurements and examining stack samples.
The bottleneck could be one big blob like this, or numerous small ones, it makes no difference.</source>
          <target state="translated">这里有一个现成的例子来说明检查测量和检查堆栈样品的区别。瓶颈可以是一个大的圆球,也可以是无数的小圆球,没有什么区别。</target>
        </trans-unit>
        <trans-unit id="84e40b82ddf20b2158fe8ae0544325f38d246cfd" translate="yes" xml:space="preserve">
          <source>Here, the &lt;code&gt;gprof&lt;/code&gt; tool reads the &lt;code&gt;gmon.out&lt;/code&gt; trace information, and generates a human readable report in &lt;code&gt;main.gprof&lt;/code&gt;, which &lt;code&gt;gprof2dot&lt;/code&gt; then reads to generate a graph.</source>
          <target state="translated">在这里， &lt;code&gt;gprof&lt;/code&gt; 工具读取 &lt;code&gt;gmon.out&lt;/code&gt; 跟踪信息，并在 &lt;code&gt;main.gprof&lt;/code&gt; 中生成人类可读的报告，然后 &lt;code&gt;gprof2dot&lt;/code&gt; 读取该报告以生成图形。</target>
        </trans-unit>
        <trans-unit id="966dcaa64447ac43f2c9e4440202155cac9e9802" translate="yes" xml:space="preserve">
          <source>Hope the idea is not obfuscated by the lack of sample code.</source>
          <target state="translated">希望这个想法不会因为没有样本代码而被模糊化。</target>
        </trans-unit>
        <trans-unit id="b3431131b2f01fa3ab4147cebf53065a9fed2f0e" translate="yes" xml:space="preserve">
          <source>How can I profile C++ code running on Linux</source>
          <target state="translated">如何对在Linux上运行的C++代码进行剖析</target>
        </trans-unit>
        <trans-unit id="e8d9679c58e452301713e88a029c43b1b26959f7" translate="yes" xml:space="preserve">
          <source>However, if you're in a hurry and you can manually interrupt your program under the debugger while it's being subjectively slow, there's a simple way to find performance problems.</source>
          <target state="translated">不过,如果你很着急,在调试器下手动中断程序时,如果你的程序主观上慢了,有一个简单的方法可以找到性能问题。</target>
        </trans-unit>
        <trans-unit id="469ae7f984de3b3fd05c657ff55c2e05a4bb68f3" translate="yes" xml:space="preserve">
          <source>I assume you're using GCC. The standard solution would be to profile with &lt;a href=&quot;http://www.math.utah.edu/docs/info/gprof_toc.html&quot;&gt;gprof&lt;/a&gt;.</source>
          <target state="translated">我认为您正在使用GCC。 标准解决方案是使用&lt;a href=&quot;http://www.math.utah.edu/docs/info/gprof_toc.html&quot;&gt;gprof进行分析&lt;/a&gt; 。</target>
        </trans-unit>
        <trans-unit id="2626ed11413be172901c236c0ce46711b1e4e409" translate="yes" xml:space="preserve">
          <source>I choose SVG output instead of PNG because the SVG is searchable with Ctrl + F and the file size can be about 10x smaller. Also, the width and height of the generated image can be humoungous with tens of thousands of pixels for complex software, and GNOME &lt;code&gt;eog&lt;/code&gt; 3.28.1 bugs out in that case for PNGs, while SVGs get opened by my browser automatically. gimp 2.8 worked well though, see also:</source>
          <target state="translated">我选择SVG输出而不是PNG，因为可以使用Ctrl + F搜索SVG，并且文件大小可以小10倍左右。 同样，对于复杂的软件，生成的图像的宽度和高度可能非常庞大，有成千上万的像素，而在这种情况下，GNOME &lt;code&gt;eog&lt;/code&gt; 3.28.1会针对PNG进行修正，而SVG会由我的浏览器自动打开。 虽然gimp 2.8运作良好，但另请参阅：</target>
        </trans-unit>
        <trans-unit id="219d1034d60f6bcc0c754cc1eb67709e077e407e" translate="yes" xml:space="preserve">
          <source>I enable &lt;code&gt;--dump-instr=yes --collect-jumps=yes&lt;/code&gt; because this also dumps information that enables us to view a per assembly line breakdown of performance, at a relatively small added overhead cost.</source>
          <target state="translated">我启用 &lt;code&gt;--dump-instr=yes --collect-jumps=yes&lt;/code&gt; ,因为这还会转储信息，使我们能够以相对较小的开销成本查看每条装配线的性能明细。</target>
        </trans-unit>
        <trans-unit id="161bfcf11ea394e1742c7e0a58a6044adc301c18" translate="yes" xml:space="preserve">
          <source>I have a C++ application, running on Linux, which I'm in the process of optimizing. How can I pinpoint which areas of my code are running slowly?</source>
          <target state="translated">我有一个C++程序,运行在Linux上,我正在进行优化。如何才能确定我的代码中哪些地方运行缓慢?</target>
        </trans-unit>
        <trans-unit id="3fa235219dffe6626130e2fa7a49a904b35f5be5" translate="yes" xml:space="preserve">
          <source>I have used HPCToolkit and VTune and they are very effective at finding the long pole in the tent and do not need your code to be recompiled (except that you have to use -g -O or RelWithDebInfo type build in CMake to get meaningful output). I have heard TAU is similar in capabilities.</source>
          <target state="translated">我用过HPCToolkit和VTune,它们在找长杆子的时候非常有效,不需要你的代码重新编译(除了你必须在CMake中使用-g -O或RelWithDebInfo类型的构建才能得到有意义的输出)。我听说TAU的功能也是类似的。</target>
        </trans-unit>
        <trans-unit id="d652bdc8e2d715973adba2c14a8aeac6ea5b5520" translate="yes" xml:space="preserve">
          <source>I haven't tried it yet but I've heard good things about &lt;a href=&quot;https://github.com/gperftools/gperftools&quot;&gt;google-perftools&lt;/a&gt;. It is definitely worth a try.</source>
          <target state="translated">我还没有尝试过，但是我听说过有关&lt;a href=&quot;https://github.com/gperftools/gperftools&quot;&gt;google-perftools的&lt;/a&gt;好消息 。 绝对值得一试。</target>
        </trans-unit>
        <trans-unit id="9adb36d7693cd006e9f81ed5125c898080cee016" translate="yes" xml:space="preserve">
          <source>I recommend in next window to click on &quot;Self&quot; column header, otherwise it shows that &quot;main()&quot; is most time consuming task. &quot;Self&quot; shows how much each function itself took time, not together with dependents.</source>
          <target state="translated">我建议在下一个窗口中点击 &quot;Self &quot;列标题,否则会显示 &quot;main()&quot;是最耗时的任务。&quot;Self &quot;显示的是每个函数本身的耗时量,而不是和依赖的函数一起显示。</target>
        </trans-unit>
        <trans-unit id="0f482aa371b33a0d0d5638abd01afc2e298063e0" translate="yes" xml:space="preserve">
          <source>I think &lt;code&gt;fast&lt;/code&gt; is not showing on that graph because kcachegrind must have simplified the visualization because that call takes up too little time, this will likely be the behavior you want on a real program. The right click menu has some settings to control when to cull such nodes, but I couldn't get it to show such a short call after a quick attempt. If I click on &lt;code&gt;fast&lt;/code&gt; on the left window, it does show a call graph with &lt;code&gt;fast&lt;/code&gt;, so that stack was actually captured. No one had yet found a way to show the complete graph call graph: &lt;a href=&quot;https://stackoverflow.com/questions/33769323/make-callgrind-show-all-function-calls-in-the-kcachegrind-callgraph&quot;&gt;Make callgrind show all function calls in the kcachegrind callgraph&lt;/a&gt;</source>
          <target state="translated">我认为该图上没有 &lt;code&gt;fast&lt;/code&gt; 显示，因为kcachegrind必须简化了可视化，因为该调用占用的时间太少，这很可能是您在实际程序上想要的行为。 右键菜单中有一些设置可以控制何时剔除此类节点，但是经过快速尝试后，我无法让它显示出如此短的通话时间。 如果我在左侧窗口中单击 &lt;code&gt;fast&lt;/code&gt; ，它的确显示了一个带有 &lt;code&gt;fast&lt;/code&gt; 的调用图，因此该堆栈实际上已被捕获。 还没有人找到一种显示完整图形调用图的方法： &lt;a href=&quot;https://stackoverflow.com/questions/33769323/make-callgrind-show-all-function-calls-in-the-kcachegrind-callgraph&quot;&gt;使callgrind显示kcachegrind&lt;/a&gt;调用图中的所有函数调用</target>
        </trans-unit>
        <trans-unit id="9b793d633b0f9e951abdde18b8e2544cd6ea68b3" translate="yes" xml:space="preserve">
          <source>I would use Valgrind and Callgrind as a base for my profiling tool suite. What is important to know is that Valgrind is basically a Virtual Machine:</source>
          <target state="translated">我会用Valgrind和Callgrind作为我的剖析工具套件的基础。需要知道的是,Valgrind基本上是一个虚拟机。</target>
        </trans-unit>
        <trans-unit id="6263011d671a27f31d355d03e88575c38e353dc8" translate="yes" xml:space="preserve">
          <source>I'm not sure if there is a nice way to do line-by-line profiling with gprof: &lt;a href=&quot;https://stackoverflow.com/questions/9608949/gprof-time-spent-in-particular-lines-of-code&quot;&gt;`gprof` time spent in particular lines of code&lt;/a&gt;</source>
          <target state="translated">我不确定是否有很好的方法用gprof进行逐行分析：gprof &lt;a href=&quot;https://stackoverflow.com/questions/9608949/gprof-time-spent-in-particular-lines-of-code&quot;&gt;在特定代码行上花费的时间&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bf065948e194a11f5b37f951703925e3fcde2b09" translate="yes" xml:space="preserve">
          <source>I've been using Gprof the last couple of days and have already found three significant limitations, one of which I've not seen documented anywhere else (yet):</source>
          <target state="translated">最近几天我一直在使用Gprof,已经发现了三个明显的限制,其中一个是我在其他地方没有看到过的记录。</target>
        </trans-unit>
        <trans-unit id="9d8fb54a20b1fdf3a9f7cf18def00021f5a8cb9f" translate="yes" xml:space="preserve">
          <source>IMHO identifying the piece that is causing bottleneck is the key here. I'd however try and answer the following questions first and choose tool based on that</source>
          <target state="translated">IMHO确定造成瓶颈的部件是关键。不过,我会尝试先回答以下问题,然后根据这些问题来选择工具。</target>
        </trans-unit>
        <trans-unit id="dc4e7b05ff6973f04bea57e88cf21f888a6f86ea" translate="yes" xml:space="preserve">
          <source>If you don't have a profiler, use the poor man's profiler. Hit pause while debugging your application. Most developer suites will break into assembly with commented line numbers. You're statistically likely to land in a region that is eating most of your CPU cycles.</source>
          <target state="translated">如果你没有profiler,请使用穷人的profiler。在调试你的应用程序时,点击暂停。大多数开发者套件都会用注释行号分解成汇编。据统计,你很可能会降落在一个吞噬你大部分CPU周期的区域。</target>
        </trans-unit>
        <trans-unit id="41540dddcb3ba901435fd0caa48e769fcf817d8a" translate="yes" xml:space="preserve">
          <source>If your goal is to use a profiler, use one of the suggested ones.</source>
          <target state="translated">如果你的目标是使用剖析器,请使用建议的剖析器之一。</target>
        </trans-unit>
        <trans-unit id="be4b7a0ea0d89ea3adbdc8138fbc2d0f2de5e625" translate="yes" xml:space="preserve">
          <source>In our example, outputs were for &lt;code&gt;-O0&lt;/code&gt;:</source>
          <target state="translated">在我们的示例中，输出为 &lt;code&gt;-O0&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="431412748fb4a5baa6b65245373a7cfce7f17b4e" translate="yes" xml:space="preserve">
          <source>In this answer, I will use several different tools to a analyze a few very simple test programs, in order to concretely compare how those tools work.</source>
          <target state="translated">在这个答案中,我将使用几个不同的工具来分析几个非常简单的测试程序,以便具体比较这些工具的工作原理。</target>
        </trans-unit>
        <trans-unit id="635f12c9ec4169045833041b2619bfd5f883a755" translate="yes" xml:space="preserve">
          <source>Intel VTune (&lt;a href=&quot;https://software.intel.com/en-us/vtune&quot;&gt;https://software.intel.com/en-us/vtune&lt;/a&gt;) - If you have intel compilers this is very good</source>
          <target state="translated">英特尔VTune（ &lt;a href=&quot;https://software.intel.com/en-us/vtune&quot;&gt;https://software.intel.com/zh-cn/vtune&lt;/a&gt; ）-如果您拥有英特尔&amp;reg;编译器，这将非常好</target>
        </trans-unit>
        <trans-unit id="4642763219e0fa4ea52daaf7bd5c245926adf1ae" translate="yes" xml:space="preserve">
          <source>It doesn't work properly on multi-threaded code, unless you use a &lt;a href=&quot;http://sam.zoy.org/writings/programming/gprof.html&quot;&gt;workaround&lt;/a&gt;</source>
          <target state="translated">除非您使用&lt;a href=&quot;http://sam.zoy.org/writings/programming/gprof.html&quot;&gt;解决方法&lt;/a&gt; ，否则它在多线程代码上无法正常工作</target>
        </trans-unit>
        <trans-unit id="0a9760c557903dfa3fa680464b187e1b7fe3f2d6" translate="yes" xml:space="preserve">
          <source>It is a sampling profiler, along the lines of the... long... answer by Mike Dunlavey, which will gift wrap the results in a browsable call stack tree, annotated with the time or memory spent in each function, either cumulative or per-function.</source>
          <target state="translated">它是一个采样分析器,类似于Mike Dunlavey的.....长....答案,它将把结果以可浏览的调用堆栈树的形式包装成一个可浏览的结果,并标注了每个函数所花费的时间或内存,可以是累计的,也可以是每个函数的内存。</target>
        </trans-unit>
        <trans-unit id="2ba19127395eb53bf0dd24f844c9a45fd905b0fe" translate="yes" xml:space="preserve">
          <source>It says &lt;a href=&quot;http://www.cs.utah.edu/dept/old/texinfo/as/gprof.html&quot;&gt;here&lt;/a&gt; that &quot;... the number-of-calls figures are derived by counting, not sampling. They are completely accurate...&quot;. Yet I find my call graph giving me 5345859132+784984078 as call stats to my most-called function, where the first number is supposed to be direct calls, and the second recursive calls (which are all from itself). Since this implied I had a bug, I put in long (64-bit) counters into the code and did the same run again. My counts: 5345859132 direct, and 78094395406 self-recursive calls.  There are a lot of digits there, so I'll point out the recursive calls I measure are 78bn, versus 784m from Gprof: a factor of 100 different. Both runs were single threaded and unoptimised code, one compiled &lt;code&gt;-g&lt;/code&gt; and the other &lt;code&gt;-pg&lt;/code&gt;.</source>
          <target state="translated">它&lt;a href=&quot;http://www.cs.utah.edu/dept/old/texinfo/as/gprof.html&quot;&gt;在这里&lt;/a&gt;说：&amp;ldquo; ...呼叫次数数字是通过计数而不是抽样得出的。它们是完全准确的...&amp;rdquo;。 但是我发现自己的调用图给了我5345859132 + 784984078作为我最被调用函数的调用统计信息，其中第一个数字应该是直接调用，而第二个递归调用（全部来自其本身）。 由于这意味着我有一个错误，因此我在代码中放入了长（64位）计数器，然后再次执行了相同的操作。 我的计算是：直接5345859132，以及78094395406自递归调用。 那里有很多数字，所以我要指出，我测量的递归调用为780亿，而Gprof则为7.84亿：相差100倍。 两次运行都是单线程且未经优化的代码，一个是 &lt;code&gt;-g&lt;/code&gt; ，另一个是 &lt;code&gt;-pg&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7cb7e9d2c15a7ea8f0115ba5bd906e6333b013da" translate="yes" xml:space="preserve">
          <source>It will generate a file called &lt;code&gt;callgrind.out.x&lt;/code&gt;. You can then use &lt;code&gt;kcachegrind&lt;/code&gt; tool to read this file. It will give you a graphical analysis of things with results like which lines cost how much.</source>
          <target state="translated">它将生成一个名为 &lt;code&gt;callgrind.out.x&lt;/code&gt; 的文件。 然后，您可以使用 &lt;code&gt;kcachegrind&lt;/code&gt; 工具读取此文件。 它将为您提供图形化的事物分析结果，例如哪些行花费多少。</target>
        </trans-unit>
        <trans-unit id="76e28acade3cc2a27154df90bec362b0a631dfad" translate="yes" xml:space="preserve">
          <source>It's cross-platform and allows you not to measure performance of your application also in real-time. You can even couple it with a live graph.
Full disclaimer: I am the author.</source>
          <target state="translated">它是跨平台的,让你不仅可以实时测量你的应用程序的性能。你甚至可以将其与实时图表结合起来。充分的免责声明:我是作者。</target>
        </trans-unit>
        <trans-unit id="4b1a94bb5d0f0a36818c07bf5e21ac7db7f6e83a" translate="yes" xml:space="preserve">
          <source>It's in C++ and must be customized to your needs. Unfortunately I can't share code, just concepts.
You use a &quot;large&quot; &lt;code&gt;volatile&lt;/code&gt; buffer containing timestamps and event ID that you can dump post mortem or after stopping the logging system (and dump this into a file for example).</source>
          <target state="translated">它使用C ++，并且必须根据您的需求进行定制。 不幸的是，我不能共享代码，只能共享概念。 您使用了一个包含时间戳和事件ID的&amp;ldquo;大&amp;rdquo; &lt;code&gt;volatile&lt;/code&gt; 缓冲区，您可以转储事后检查或在停止日志记录系统后将其转储（例如，将其转储到文件中）。</target>
        </trans-unit>
        <trans-unit id="6866c589c53df9bdbafd58f6594aa3b9926e0556" translate="yes" xml:space="preserve">
          <source>Just halt it several times, and each time look at the call stack. If there is some code that is wasting some percentage of the time, 20% or 50% or whatever, that is the probability that you will catch it in the act on each sample. So that is roughly the percentage of samples on which you will see it. There is no educated guesswork required.
If you do have a guess as to what the problem is, this will prove or disprove it.</source>
          <target state="translated">只要停顿几次,每次都看一下调用栈。如果有一些代码浪费了一定比例的时间,20%或者50%或者其他什么的,这就是你在每次采样的时候都会发现它的概率。所以,这大概就是你会在哪些样本上看到它的百分比。这不需要有经验的猜测。如果你确实有一个猜测问题是什么,这将证明或否定它。</target>
        </trans-unit>
        <trans-unit id="c54056bc2b49a6ba4b6c533b7638afae9a9d85da" translate="yes" xml:space="preserve">
          <source>MAP is commercial software.</source>
          <target state="translated">MAP是商业软件。</target>
        </trans-unit>
        <trans-unit id="7f9d5d60edf957a9efb30c849d25e99af6c8e0ff" translate="yes" xml:space="preserve">
          <source>Measurement is horizontal; it tells you what fraction of time specific routines take.
Sampling is vertical.
If there is any way to avoid what the whole program is doing at that moment, &lt;em&gt;and if you see it on a second sample&lt;/em&gt;, you've found the bottleneck.
That's what makes the difference - seeing the whole reason for the time being spent, not just how much.</source>
          <target state="translated">测量是水平的； 它告诉您特定例程花费的时间比例。 采样是垂直的。 如果有什么办法可以避免整个程序在那时的工作， &lt;em&gt;并且在第二个示例中看到它&lt;/em&gt; ，那么您已经找到了瓶颈。 这就是与众不同的原因-查看花费时间的全部原因，而不仅仅是花多少时间。</target>
        </trans-unit>
        <trans-unit id="477fe7fb42d087f23d433845410b477d61ad6d95" translate="yes" xml:space="preserve">
          <source>N.B.</source>
          <target state="translated">N.B.</target>
        </trans-unit>
        <trans-unit id="5f635737b21ddea700a64d9ba1681ee8be0ba798" translate="yes" xml:space="preserve">
          <source>Newer kernels (e.g. the latest Ubuntu kernels) come with the new 'perf' tools (&lt;code&gt;apt-get install linux-tools&lt;/code&gt;) AKA &lt;a href=&quot;https://en.wikipedia.org/wiki/Perf_(Linux)&quot;&gt;perf_events&lt;/a&gt;.</source>
          <target state="translated">较新的内核（例如，最新的Ubuntu内核）附带了新的&amp;ldquo; perf&amp;rdquo;工具（即 &lt;code&gt;apt-get install linux-tools&lt;/code&gt; ），也&lt;a href=&quot;https://en.wikipedia.org/wiki/Perf_(Linux)&quot;&gt;称为perf_events&lt;/a&gt; 。</target>
        </trans-unit>
        <trans-unit id="130b95199b8439c21a930c61f8c68941d88450a0" translate="yes" xml:space="preserve">
          <source>Now it says P(f &amp;gt;= 0.5) is 26%, up from the prior assumption of 0.6%. So Bayes allows us to update our estimate of the probable cost of &lt;code&gt;I&lt;/code&gt;. If the amount of data is small, it doesn't tell us accurately what the cost is, only that it is big enough to be worth fixing.</source>
          <target state="translated">现在它说P（f&amp;gt; = 0.5）为26％，高于先前假设的0.6％。 因此，贝叶斯允许我们更新对 &lt;code&gt;I&lt;/code&gt; 的可能成本的估计。 如果数据量很小，它并不能准确地告诉我们成本是多少，而只是告诉我们它足够大才能值得解决。</target>
        </trans-unit>
        <trans-unit id="1cc1ff9a05639f9a0179486bc64f7be0e48efc59" translate="yes" xml:space="preserve">
          <source>Now we have some files named callgrind.out.* in current directory. To see profiling results use:</source>
          <target state="translated">现在我们在当前目录下有一些名为callgrind.out.*的文件。要查看分析结果,请使用:</target>
        </trans-unit>
        <trans-unit id="3440a6fe929867ee09b71eda8a51b92ff532c181" translate="yes" xml:space="preserve">
          <source>Now when it works and we want to start profiling we should run in another window:</source>
          <target state="translated">现在,当它工作了,我们想开始剖析时,我们应该在另一个窗口中运行。</target>
        </trans-unit>
        <trans-unit id="d3cb823c58e17cf865b583e5a20e385601ec48b1" translate="yes" xml:space="preserve">
          <source>Off the bat, &lt;code&gt;time&lt;/code&gt; tells us that the program took 29.5 seconds to execute, so we had a slowdown of about 15x on this example. Clearly, this slowdown is going to be a serious limitation for larger workloads. On the &quot;real world software example&quot; &lt;a href=&quot;https://gem5.atlassian.net/browse/GEM5-337&quot;&gt;mentioned here&lt;/a&gt;, I observed a slowdown of 80x.</source>
          <target state="translated">&lt;code&gt;time&lt;/code&gt; ， 时间告诉我们该程序需要29.5秒的时间来执行，因此在此示例中，速度降低了大约15倍。 显然，这种减速将成为较大工作负载的严重限制。 在&lt;a href=&quot;https://gem5.atlassian.net/browse/GEM5-337&quot;&gt;这里提到&lt;/a&gt;的&amp;ldquo;实际软件示例&amp;rdquo; 中 ，我观察到速度降低了80倍。</target>
        </trans-unit>
        <trans-unit id="a2a01fb750150dbf6d32038f978233485a2955b5" translate="yes" xml:space="preserve">
          <source>On the a more complex example it becomes clear what the graph means:</source>
          <target state="translated">在一个更复杂的例子中,图形的含义就变得很清楚了。</target>
        </trans-unit>
        <trans-unit id="7ea07b1121db66a159bd732b96f60b6003126f27" translate="yes" xml:space="preserve">
          <source>Once you have understood the data output format, you can reduce verbosity to show just the data without the tutorial with the &lt;code&gt;-b&lt;/code&gt; option:</source>
          <target state="translated">理解了数据输出格式后，您可以减少冗长程度以仅显示数据，而无需使用带有 &lt;code&gt;-b&lt;/code&gt; 选项的教程：</target>
        </trans-unit>
        <trans-unit id="dbba8950c83f558b0a5b23878c05f60878612272" translate="yes" xml:space="preserve">
          <source>One cool thing about &lt;code&gt;perf&lt;/code&gt; is the FlameGraph tool from Brendan Gregg which displays the call stack timings in a very neat way that allows you to quickly see the big calls. The tool is available at: &lt;a href=&quot;https://github.com/brendangregg/FlameGraph&quot;&gt;https://github.com/brendangregg/FlameGraph&lt;/a&gt; and is also mentioned on his perf tutorial at: &lt;a href=&quot;http://www.brendangregg.com/perf.html#FlameGraphs&quot;&gt;http://www.brendangregg.com/perf.html#FlameGraphs&lt;/a&gt; When I ran &lt;code&gt;perf&lt;/code&gt; without &lt;code&gt;sudo&lt;/code&gt; I got &lt;a href=&quot;https://github.com/brendangregg/FlameGraph/issues/132&quot;&gt;&lt;code&gt;ERROR: No stack counts found&lt;/code&gt;&lt;/a&gt; so for now I'll be doing it with &lt;code&gt;sudo&lt;/code&gt;:</source>
          <target state="translated">关于性能的一件很酷的事情是Brendan Gregg的FlameGraph工具，该工具以非常简洁的方式显示了调用堆栈的时间，使您可以快速查看大调用。 该工具位于： &lt;a href=&quot;https://github.com/brendangregg/FlameGraph&quot;&gt;https&lt;/a&gt; : //github.com/brendangregg/FlameGraph，并且在他的perf教程中也提到了该工具： &lt;a href=&quot;http://www.brendangregg.com/perf.html#FlameGraphs&quot;&gt;http&lt;/a&gt; : //www.brendangregg.com/perf.html#FlameGraphs当我在没有 &lt;code&gt;sudo&lt;/code&gt; 的情况下运行 &lt;code&gt;perf&lt;/code&gt; 时 ，我得到了&lt;a href=&quot;https://github.com/brendangregg/FlameGraph/issues/132&quot;&gt; &lt;code&gt;ERROR: No stack counts found&lt;/code&gt; &lt;/a&gt;因此现在我将使用 &lt;code&gt;sudo&lt;/code&gt; 进行操作 ：</target>
        </trans-unit>
        <trans-unit id="e1e2d5fff1404430f80a9893a77239e1b369f4c6" translate="yes" xml:space="preserve">
          <source>P.P.S As a rough generality, the more layers of abstraction you have in your software, the more likely you are to find that that is the cause of performance problems (and the opportunity to get speedup).</source>
          <target state="translated">P.P.S 作为一个粗略的概括,你的软件中的抽象层数越多,你就越有可能发现这就是性能问题的原因(以及获得提速的机会)。</target>
        </trans-unit>
        <trans-unit id="bbd10fe9dd107bcf29e061a1dbca691e2b9849ec" translate="yes" xml:space="preserve">
          <source>P.S. This can also be done on multi-thread programs if there is a way to collect call-stack samples of the thread pool at a point in time, as there is in Java.</source>
          <target state="translated">P.S.如果有办法在某个时间点收集线程池的调用栈样本,这也可以在多线程程序上实现,就像Java中的那样。</target>
        </trans-unit>
        <trans-unit id="4d12df151686c7a7b5c9580f37caebf54fd9c01a" translate="yes" xml:space="preserve">
          <source>Previously called &quot;Google Performance Tools&quot;, source: &lt;a href=&quot;https://github.com/gperftools/gperftools&quot;&gt;https://github.com/gperftools/gperftools&lt;/a&gt; Sample based.</source>
          <target state="translated">以前称为&amp;ldquo; Google Performance Tools&amp;rdquo;，来源： &lt;a href=&quot;https://github.com/gperftools/gperftools&quot;&gt;https&lt;/a&gt; : //github.com/gperftools/gperftools基于示例。</target>
        </trans-unit>
        <trans-unit id="4830d80b5bfe85f29931b456dde61e5a468406a7" translate="yes" xml:space="preserve">
          <source>Related question &lt;a href=&quot;https://stackoverflow.com/questions/56672/how-do-you-profile-your-code&quot;&gt;here&lt;/a&gt;.</source>
          <target state="translated">相关问题&lt;a href=&quot;https://stackoverflow.com/questions/56672/how-do-you-profile-your-code&quot;&gt;在这里&lt;/a&gt; 。</target>
        </trans-unit>
        <trans-unit id="2577e776d1182d8b8493c8c47c6882960c233679" translate="yes" xml:space="preserve">
          <source>See also: &lt;a href=&quot;https://stackoverflow.com/questions/10874308/how-to-use-google-perf-tools&quot;&gt;How to use google perf tools&lt;/a&gt;</source>
          <target state="translated">另请参阅： &lt;a href=&quot;https://stackoverflow.com/questions/10874308/how-to-use-google-perf-tools&quot;&gt;如何使用Google Perf工具&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d6bdcd52d3599e74ca39aae086a3b346358703f1" translate="yes" xml:space="preserve">
          <source>See also: &lt;a href=&quot;https://stackoverflow.com/questions/46949407/gperftools-profile-file-not-dumped&quot;&gt;gperftools - profile file not dumped&lt;/a&gt;</source>
          <target state="translated">另请参阅： &lt;a href=&quot;https://stackoverflow.com/questions/46949407/gperftools-profile-file-not-dumped&quot;&gt;gperftools-未转储配置文件&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6298bf00768899707674b3da8584b8c3f21f0334" translate="yes" xml:space="preserve">
          <source>So then I try to benchmark the &lt;code&gt;-O0&lt;/code&gt; program to see if that shows anything, and only now, at last, do I see a call graph:</source>
          <target state="translated">因此，然后我尝试对 &lt;code&gt;-O0&lt;/code&gt; 程序进行基准测试，看它是否显示任何内容，直到现在，我才终于看到调用图：</target>
        </trans-unit>
        <trans-unit id="72cc02d3d1195fc59d61046450268e28e14c7f19" translate="yes" xml:space="preserve">
          <source>So this is what I recommend. Run program first:</source>
          <target state="translated">所以,我是这样推荐的。先运行程序。</target>
        </trans-unit>
        <trans-unit id="f6772b5f69ff63328b7702c6bb60c75468582ac8" translate="yes" xml:space="preserve">
          <source>So we compile and run as:</source>
          <target state="translated">因此,我们编译并运行为。</target>
        </trans-unit>
        <trans-unit id="cd0d4cbba9f07fc22442ddaca962fad827eac83d" translate="yes" xml:space="preserve">
          <source>So, even a very small number of samples can tell us a lot about the cost of instructions that it sees. (And it will see them with a frequency, on average, proportional to their cost. If &lt;code&gt;n&lt;/code&gt; samples are taken, and &lt;code&gt;f&lt;/code&gt; is the cost, then &lt;code&gt;I&lt;/code&gt; will appear on &lt;code&gt;nf+/-sqrt(nf(1-f))&lt;/code&gt; samples. Example, &lt;code&gt;n=10&lt;/code&gt;, &lt;code&gt;f=0.3&lt;/code&gt;, that is &lt;code&gt;3+/-1.4&lt;/code&gt; samples.)</source>
          <target state="translated">因此，即使是非常少量的样本也可以告诉我们有关它所看到的指令成本的很多信息。 （并且平均来看，它们的频率与成本成正比。如果抽取 &lt;code&gt;n&lt;/code&gt; 个样本，而 &lt;code&gt;f&lt;/code&gt; 为成本，那么 &lt;code&gt;I&lt;/code&gt; 将出现在 &lt;code&gt;nf+/-sqrt(nf(1-f))&lt;/code&gt; 样本上。 ， &lt;code&gt;n=10&lt;/code&gt; ， &lt;code&gt;f=0.3&lt;/code&gt; ，即 &lt;code&gt;3+/-1.4&lt;/code&gt; 样本。）</target>
        </trans-unit>
        <trans-unit id="08239f177013e0b069b1f213aca8eed8d3e8fd03" translate="yes" xml:space="preserve">
          <source>Suppose the prior assumptions are different. Suppose we assume P(f=0.1) is .991 (nearly certain), and all the other possibilities are almost impossible (0.001). In other words, our prior certainty is that &lt;code&gt;I&lt;/code&gt; is cheap. Then we get:</source>
          <target state="translated">假设先前的假设是不同的。 假设我们假设P（f = 0.1）为.991（几乎可以肯定），其他所有可能性几乎都是不可能的（0.001）。 换句话说，我们的先验是 &lt;code&gt;I&lt;/code&gt; 很便宜。 然后我们得到：</target>
        </trans-unit>
        <trans-unit id="d3b210cd62d3d15648f1f008cbd6a6ff99536cd2" translate="yes" xml:space="preserve">
          <source>TAU (&lt;a href=&quot;http://www.cs.uoregon.edu/research/tau/home.php&quot;&gt;http://www.cs.uoregon.edu/research/tau/home.php&lt;/a&gt;)</source>
          <target state="translated">TAU（ &lt;a href=&quot;http://www.cs.uoregon.edu/research/tau/home.php&quot;&gt;http://www.cs.uoregon.edu/research/tau/home.php&lt;/a&gt; ）</target>
        </trans-unit>
        <trans-unit id="ed5f5a3f654a364aa389b4161a87b5a87e7b8ec1" translate="yes" xml:space="preserve">
          <source>TODO on complex C++ software, I see some entries of type &lt;code&gt;&amp;lt;cycle N&amp;gt;&lt;/code&gt;, e.g. &lt;code&gt;&amp;lt;cycle 11&amp;gt;&lt;/code&gt; where I'd expect function names, what does that mean? I noticed there is a &quot;Cycle Detection&quot; button to toggle that on and off, but what does it mean?</source>
          <target state="translated">在复杂的C ++软件上的TODO中，我看到一些类型为 &lt;code&gt;&amp;lt;cycle N&amp;gt;&lt;/code&gt; 的条目，例如 &lt;code&gt;&amp;lt;cycle 11&amp;gt;&lt;/code&gt; ，我希望在其中找到函数名称，这是什么意思？ 我注意到有一个&amp;ldquo;循环检测&amp;rdquo;按钮可以将其打开和关闭，但这是什么意思？</target>
        </trans-unit>
        <trans-unit id="b13a96071f0d7fdcbc2465e3e45593db4c6ebbfd" translate="yes" xml:space="preserve">
          <source>TODO there are a log of &lt;code&gt;[unknown]&lt;/code&gt; functions in that example, why is that?</source>
          <target state="translated">TODO在该示例中有 &lt;code&gt;[unknown]&lt;/code&gt; 函数的日志，这是为什么呢？</target>
        </trans-unit>
        <trans-unit id="a19af3eb40fbc53ad472e0220f8b098385c16862" translate="yes" xml:space="preserve">
          <source>TODO: what happened on the &lt;code&gt;-O3&lt;/code&gt; execution? Is it simply that &lt;code&gt;maybe_slow&lt;/code&gt; and &lt;code&gt;fast&lt;/code&gt; were too fast and did not get any samples? Does it work well with &lt;code&gt;-O3&lt;/code&gt; on larger programs that take longer to execute? Did I miss some CLI option? I found out about &lt;code&gt;-F&lt;/code&gt; to control the sample frequency in Hertz, but I turned it up to the max allowed by default of &lt;code&gt;-F 39500&lt;/code&gt; (could be increased with &lt;code&gt;sudo&lt;/code&gt;) and I still don't see clear calls.</source>
          <target state="translated">TODO： &lt;code&gt;-O3&lt;/code&gt; 执行时发生了什么？ 难道仅仅是&amp;ldquo; &lt;code&gt;maybe_slow&lt;/code&gt; 和&amp;ldquo; &lt;code&gt;fast&lt;/code&gt; &amp;rdquo;太快而没有得到任何样本？ 在需要较长时间执行的大型程序上，它与 &lt;code&gt;-O3&lt;/code&gt; 配合使用是否很好？ 我错过了一些CLI选项吗？ 我发现有关 &lt;code&gt;-F&lt;/code&gt; 的信息以赫兹为单位来控制采样频率，但是我将其设置为默认情况下允许的 &lt;code&gt;-F 39500&lt;/code&gt; 最大值 （可以使用 &lt;code&gt;sudo&lt;/code&gt; 增大），但仍然看不到清晰的通话。</target>
        </trans-unit>
        <trans-unit id="6dc4e6a8f235d6662619c2b13f904dd1fd3359cb" translate="yes" xml:space="preserve">
          <source>TODO: why is &lt;code&gt;main&lt;/code&gt; missing from the &lt;code&gt;-O3&lt;/code&gt; output, even though I can see it on a &lt;code&gt;bt&lt;/code&gt; in GDB? &lt;a href=&quot;https://stackoverflow.com/questions/39041871/missing-function-from-gprof-output&quot;&gt;Missing function from GProf output&lt;/a&gt; I think it is because gprof is also sampling based in addition to its compiled instrumentation, and the &lt;code&gt;-O3&lt;/code&gt;&lt;code&gt;main&lt;/code&gt; is just too fast and got no samples.</source>
          <target state="translated">TODO：为什么即使在GDB中的 &lt;code&gt;bt&lt;/code&gt; 上也可以看到 &lt;code&gt;-O3&lt;/code&gt; 输出中缺少 &lt;code&gt;main&lt;/code&gt; ？ &lt;a href=&quot;https://stackoverflow.com/questions/39041871/missing-function-from-gprof-output&quot;&gt;GProf输出中缺少功能&lt;/a&gt;我想这是因为gprof除了其已编译的工具外还基于采样，并且 &lt;code&gt;-O3&lt;/code&gt; &lt;code&gt;main&lt;/code&gt; 太快并且没有采样。</target>
        </trans-unit>
        <trans-unit id="6e53b60f1e0dfa855a4eef9b666d7279dabbee1a" translate="yes" xml:space="preserve">
          <source>Tested in Ubuntu 18.04, gprof2dot 2019.11.30, valgrind 3.13.0, perf 4.15.18, Linux kernel 4.15.0, FLameGraph 1a0dc6985aad06e76857cf2a354bd5ba0c9ce96b, gperftools 2.5-2.</source>
          <target state="translated">在Ubuntu 18.04、gprof2dot 2019.11.30、 valgrind 3.13.0、 perf 4.15.18、Linux内核4.15.0、FLameGraph 1a0dc6985aad06e76857cf2a354bd5ba0c9ce96b、gperftools 2.5-2中测试。</target>
        </trans-unit>
        <trans-unit id="3a80a1a4fbc3469221b7d5afd5713915cd326c8f" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;-O0&lt;/code&gt; output is pretty much self-explanatory. For example, it shows that the 3 &lt;code&gt;maybe_slow&lt;/code&gt; calls and their child calls take up 97.56% of the total runtime, although execution of &lt;code&gt;maybe_slow&lt;/code&gt; itself without children accounts for 0.00% of the total execution time, i.e. almost all the time spent in that function was spent on child calls.</source>
          <target state="translated">&lt;code&gt;-O0&lt;/code&gt; 输出几乎是不言自明的。 例如，它显示了3个 &lt;code&gt;maybe_slow&lt;/code&gt; 调用及其子调用占总运行时间的97.56％，尽管不 &lt;code&gt;maybe_slow&lt;/code&gt; 自身执行占总执行时间的0.00％，即，几乎在该函数上花费的所有时间都是花在打孩子上。</target>
        </trans-unit>
        <trans-unit id="e42365e1a6bff7fc2be05d355cadfefbdf5ea497" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;probe&lt;/code&gt; function uses a few assembly lines to retrieve the clock timestamp ASAP and then sets an entry in the buffer. We also have an atomic increment to safely find an index where to store the log event.
Of course buffer is circular.</source>
          <target state="translated">&lt;code&gt;probe&lt;/code&gt; 功能使用几条装配线尽快恢复时钟时间戳，然后在缓冲区中设置一个条目。 我们还有一个原子增量，可以安全地找到存储日志事件的索引。 当然缓冲区是循环的。</target>
        </trans-unit>
        <trans-unit id="f1066d2d94872d1ccbce2036cf5aac073098024e" translate="yes" xml:space="preserve">
          <source>The answer to run &lt;code&gt;valgrind --tool=callgrind&lt;/code&gt; is not quite complete without some options. We usually do not want to profile 10 minutes of slow startup time under Valgrind and want to profile our program when it is doing some task.</source>
          <target state="translated">没有一些选项，运行 &lt;code&gt;valgrind --tool=callgrind&lt;/code&gt; 的答案并不十分完整。 我们通常不想在Valgrind下分析10分钟的缓慢启动时间，而不想在执行某些任务时分析我们的程序。</target>
        </trans-unit>
        <trans-unit id="36529a459a19fd1c86e51be05eccbd6859fb938f" translate="yes" xml:space="preserve">
          <source>The call graph gets confused by function pointers. Example: I have a function called &lt;code&gt;multithread()&lt;/code&gt; which enables me to multi-thread a specified function over a specified array (both passed as arguments). Gprof however, views all calls to &lt;code&gt;multithread()&lt;/code&gt; as equivalent for the purposes of computing time spent in children. Since some functions I pass to &lt;code&gt;multithread()&lt;/code&gt; take much longer than others my call graphs are mostly useless. (To those wondering if threading is the issue here: no, &lt;code&gt;multithread()&lt;/code&gt; can optionally, and did in this case, run everything sequentially on the calling thread only).</source>
          <target state="translated">调用图被函数指针弄糊涂了。 示例：我有一个名为 &lt;code&gt;multithread()&lt;/code&gt; 的函数，该函数使我可以在指定的数组（均作为参数传递）上对指定的函数进行多线程处理。 但是，Gprof会将对 &lt;code&gt;multithread()&lt;/code&gt; 的所有调用视为等效，以计算在子级上花费的时间。 因为我传递给 &lt;code&gt;multithread()&lt;/code&gt; 的某些函数比其他函数花费的时间长得多，所以我的调用图几乎没有用。 （让那些想知道线程是否是这里的问题的人：不， &lt;code&gt;multithread()&lt;/code&gt; 可以选择，并且在这种情况下确实可以只在调用线程上顺序运行所有内容）。</target>
        </trans-unit>
        <trans-unit id="a13b1906e442733e77eedc379dcc29d514b536bc" translate="yes" xml:space="preserve">
          <source>The concept is to define events in &lt;code&gt;tool_events_id.hpp&lt;/code&gt; like that :</source>
          <target state="translated">这个概念是在 &lt;code&gt;tool_events_id.hpp&lt;/code&gt; 中定义事件， 如下所示：</target>
        </trans-unit>
        <trans-unit id="cdbc70168923d9d12dc32d25f0bf32b6568e5382" translate="yes" xml:space="preserve">
          <source>The downside of this is that there seems to be no Ubuntu package, and building it requires Qt 5.10 while Ubuntu 18.04 is at Qt 5.9.</source>
          <target state="translated">这样做的坏处是,似乎没有Ubuntu的软件包,构建它需要Qt 5.10,而Ubuntu 18.04是Qt 5.9。</target>
        </trans-unit>
        <trans-unit id="9b0a146d516aab93931f86859127170f3626293f" translate="yes" xml:space="preserve">
          <source>The following test program is very simple and does the following:</source>
          <target state="translated">下面的测试程序非常简单,做了以下几个方面的测试。</target>
        </trans-unit>
        <trans-unit id="c15b8d0e65310a3d3e9195caeac933a4e6e84c5b" translate="yes" xml:space="preserve">
          <source>The important thing is that these tools can be &lt;strong&gt;system profiling&lt;/strong&gt; and not just process profiling - they can show the interaction between threads, processes and the kernel and let you understand the scheduling and I/O dependencies between processes.</source>
          <target state="translated">重要的是，这些工具可以是&lt;strong&gt;系统配置文件&lt;/strong&gt; ，而不仅仅是进程配置文件-它们可以显示线程，进程和内核之间的交互，并让您了解进程之间的调度和I / O依赖关系。</target>
        </trans-unit>
        <trans-unit id="b0d289675b93f88ef384de46c84952610710e353" translate="yes" xml:space="preserve">
          <source>The last column says that, for example, the probability that &lt;code&gt;f&lt;/code&gt; &amp;gt;= 0.5 is 92%, up from the prior assumption of 60%.</source>
          <target state="translated">最后一栏说，例如， &lt;code&gt;f&lt;/code&gt; &amp;gt; = 0.5的概率为92％，高于先前假设的60％。</target>
        </trans-unit>
        <trans-unit id="627640c346afb672f5025becd9584112a15ea55e" translate="yes" xml:space="preserve">
          <source>The nicest way to view this data I've found so far is to make pprof output the same format that kcachegrind takes as input (yes, the Valgrind-project-viewer-tool) and use kcachegrind to view that:</source>
          <target state="translated">到目前为止,我找到的最好的查看这些数据的方法是让 pprof 输出与 kcachegrind 的输入格式相同(是的,就是 Valgrind-project-viewer-tool),然后使用 kcachegrind 来查看。</target>
        </trans-unit>
        <trans-unit id="23eff0579ea840d749f7dacd94ef9acf84a98ca9" translate="yes" xml:space="preserve">
          <source>The program interface is:</source>
          <target state="translated">程序界面是。</target>
        </trans-unit>
        <trans-unit id="19c4249e44db6d866c57af334d73685ec61d153c" translate="yes" xml:space="preserve">
          <source>The run generates a profile data file named &lt;code&gt;callgrind.out.&amp;lt;pid&amp;gt;&lt;/code&gt; e.g. &lt;code&gt;callgrind.out.8554&lt;/code&gt; in my case. We view that file with:</source>
          <target state="translated">运行会生成一个名为 &lt;code&gt;callgrind.out.&amp;lt;pid&amp;gt;&lt;/code&gt; 的配置文件数据文件，例如我的情况下为callgrind.out.8554 。 我们通过以下方式查看该文件：</target>
        </trans-unit>
        <trans-unit id="ed9bf7107eccd71620c4c6d6bab4692580d8a7db" translate="yes" xml:space="preserve">
          <source>The slow call of &lt;code&gt;maybe_slow&lt;/code&gt; is 10x longer, and dominates runtime if we consider calls to the child function &lt;code&gt;common&lt;/code&gt;. Ideally, the profiling tool will be able to point us to the specific slow call.</source>
          <target state="translated">慢速调用 &lt;code&gt;maybe_slow&lt;/code&gt; 的时间要长10倍，如果我们认为对子函数的调用是common ，则它会占主导地位。 理想情况下，性能分析工具可以将我们指向特定的慢速通话。</target>
        </trans-unit>
        <trans-unit id="b6c8bf98ecf41059e3237cb533da10773d9fb4c5" translate="yes" xml:space="preserve">
          <source>The source for gprof2dot is at: &lt;a href=&quot;https://github.com/jrfonseca/gprof2dot&quot;&gt;https://github.com/jrfonseca/gprof2dot&lt;/a&gt;</source>
          <target state="translated">gprof2dot的来源位于： &lt;a href=&quot;https://github.com/jrfonseca/gprof2dot&quot;&gt;https&lt;/a&gt; : //github.com/jrfonseca/gprof2dot</target>
        </trans-unit>
        <trans-unit id="769adfccf68fa21aab6b3394b205656f5dab0061" translate="yes" xml:space="preserve">
          <source>Then suppose we take just 2 stack samples, and we see instruction &lt;code&gt;I&lt;/code&gt; on both samples, designated observation &lt;code&gt;o=2/2&lt;/code&gt;. This gives us new estimates of the frequency &lt;code&gt;f&lt;/code&gt; of &lt;code&gt;I&lt;/code&gt;, according to this:</source>
          <target state="translated">然后假设我们仅取2个堆栈样本，并且在两个样本上都看到指令 &lt;code&gt;I&lt;/code&gt; ，将其指定为观察值 &lt;code&gt;o=2/2&lt;/code&gt; 。 据此，我们可以得出 &lt;code&gt;I&lt;/code&gt; 的频率 &lt;code&gt;f&lt;/code&gt; 的新估计：</target>
        </trans-unit>
        <trans-unit id="7291eec49875f5d9df200c27efdf4f591116af3f" translate="yes" xml:space="preserve">
          <source>Then switch to RELEASE mode and comment out the questionable sections of your code (stub it with nothing) until you see changes in performance.</source>
          <target state="translated">然后切换到RELEASE模式,并对代码中存在问题的部分进行注释(什么也不做),直到你看到性能的变化。</target>
        </trans-unit>
        <trans-unit id="2e742db948961b9c3cb6ead916f107d3e8dda15c" translate="yes" xml:space="preserve">
          <source>Then, we can enable the gperftools CPU profiler in two ways: at runtime, or at build time.</source>
          <target state="translated">然后,我们可以通过两种方式启用gperftools CPU profiler:在运行时,或者在构建时。</target>
        </trans-unit>
        <trans-unit id="924edd37a29a71ab9fe4c8baa007eb8497e44c49" translate="yes" xml:space="preserve">
          <source>There are profilers now that sample the stack, even on wall-clock time, but &lt;em&gt;what comes out&lt;/em&gt; is measurements (or hot path, or hot spot, from which a &quot;bottleneck&quot; can easily hide). What they don't show you (and they easily could) is the actual samples themselves. And if your goal is to &lt;em&gt;find&lt;/em&gt; the bottleneck, the number of them you need to see is, &lt;em&gt;on average&lt;/em&gt;, 2 divided by the fraction of time it takes.
So if it takes 30% of time, 2/.3 = 6.7 samples, on average, will show it, and the chance that 20 samples will show it is 99.2%.</source>
          <target state="translated">现在有分析器可以对堆栈进行采样，即使是在墙上时钟的时间，但&lt;em&gt;结果&lt;/em&gt;是测量值（或热路径或热点，&amp;ldquo;瓶颈&amp;rdquo;可以从中轻松隐藏）。 他们没有向您显示（并且很容易做到）是实际样本本身。 而且，如果您的目标是&lt;em&gt;找到&lt;/em&gt;瓶颈，那么&lt;em&gt;平均而言&lt;/em&gt; ，您需要查看的瓶颈数量为2除以所需的时间。 因此，如果花费30％的时间，则平均将显示2 / .3 = 6.7个样本，而20个样本将显示99.2％的机会。</target>
        </trans-unit>
        <trans-unit id="7e09a7534638018a7af14062b8d9c917ce457a53" translate="yes" xml:space="preserve">
          <source>There is two different type of profiling</source>
          <target state="translated">有两种不同类型的特征分析</target>
        </trans-unit>
        <trans-unit id="571edc923bc5a6cee4f85a019f1c49bdbbf47a9d" translate="yes" xml:space="preserve">
          <source>These are the two methods I use for speeding up my code:</source>
          <target state="translated">这是我用来加快代码速度的两个方法。</target>
        </trans-unit>
        <trans-unit id="8311f0252f91f1308742d21d20d0472314d898dd" translate="yes" xml:space="preserve">
          <source>These come with classic sampling profilers (&lt;a href=&quot;http://manpages.ubuntu.com/manpages/trusty/man1/perf.1.html&quot;&gt;man-page&lt;/a&gt;) as well as the awesome &lt;a href=&quot;http://web.archive.org/web/20090922171904/http://blog.fenrus.org/?p=5&quot;&gt;timechart&lt;/a&gt;!</source>
          <target state="translated">这些附带经典的采样分析器（ &lt;a href=&quot;http://manpages.ubuntu.com/manpages/trusty/man1/perf.1.html&quot;&gt;手册页&lt;/a&gt; ）以及很棒的&lt;a href=&quot;http://web.archive.org/web/20090922171904/http://blog.fenrus.org/?p=5&quot;&gt;时间表&lt;/a&gt; ！</target>
        </trans-unit>
        <trans-unit id="e0c6e507414a31dbfe361257f83996883b16fb14" translate="yes" xml:space="preserve">
          <source>They will also say it only works on toy programs, when actually it works on any program, and it seems to work better on bigger programs, because they tend to have more problems to find.
They will say it sometimes finds things that aren't problems, but that is only true if you see something &lt;em&gt;once&lt;/em&gt;. If you see a problem on more than one sample, it is real.</source>
          <target state="translated">他们还会说，它实际上仅对玩具程序有效，而实际上对任何程序都有效，并且似乎在较大的程序上效果更好，因为它们往往会发现更多的问题。 他们会说有时发现没有问题的东西，但这只有在您看到&lt;em&gt;一次之后&lt;/em&gt;才是真的。 如果您在多个样本上发现问题，那是真实的。</target>
        </trans-unit>
        <trans-unit id="83b6b75b50c6b06f6bb30d23afad367e7399b38c" translate="yes" xml:space="preserve">
          <source>This added 0.2s to execution, so we are fine time-wise, but I still don't see much of interest, after expanding the &lt;code&gt;common&lt;/code&gt; node with the keyboard right arrow:</source>
          <target state="translated">这增加了0.2s的执行时间，因此我们可以按时使用，但是在使用键盘右箭头扩展 &lt;code&gt;common&lt;/code&gt; 节点之后，我仍然没有看到太多兴趣：</target>
        </trans-unit>
        <trans-unit id="6b876e96a3c3578737d673685356fe1f9dcb0ab6" translate="yes" xml:space="preserve">
          <source>This is a response to &lt;a href=&quot;https://stackoverflow.com/a/375930/321731&quot;&gt;Nazgob's Gprof answer&lt;/a&gt;.</source>
          <target state="translated">这是对&lt;a href=&quot;https://stackoverflow.com/a/375930/321731&quot;&gt;Nazgob的Gprof回答的回应&lt;/a&gt; 。</target>
        </trans-unit>
        <trans-unit id="b830cecda0a3aff113570c5b101b80608993a7f6" translate="yes" xml:space="preserve">
          <source>This turns profiling on. To turn it off and stop whole task we might use:</source>
          <target state="translated">这将开启剖析功能。要关闭它并停止整个任务,我们可以使用:</target>
        </trans-unit>
        <trans-unit id="ae3ef8d5cd3e8ccedaacfb1558e954d08cae24db" translate="yes" xml:space="preserve">
          <source>This was GNU &lt;a href=&quot;https://en.wikipedia.org/wiki/Gprof&quot;&gt;Gprof&lt;/a&gt; (GNU Binutils for Debian) 2.18.0.20080103 running under 64-bit Debian Lenny, if that helps anyone.</source>
          <target state="translated">这是在64位Debian Lenny下运行的GNU &lt;a href=&quot;https://en.wikipedia.org/wiki/Gprof&quot;&gt;Gprof&lt;/a&gt; （用于Debian的GNU Binutils）2.18.0.20080103，如果有帮助的话。</target>
        </trans-unit>
        <trans-unit id="4809190e35f35121b8cef92d7260543e2698ad83" translate="yes" xml:space="preserve">
          <source>To get more info you can look in &lt;a href=&quot;https://sourceware.org/binutils/docs-2.32/gprof/&quot;&gt;https://sourceware.org/binutils/docs-2.32/gprof/&lt;/a&gt;</source>
          <target state="translated">要获取更多信息， &lt;a href=&quot;https://sourceware.org/binutils/docs-2.32/gprof/&quot;&gt;请访问https://sourceware.org/binutils/docs-2.32/gprof/&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9d1e75d21749779f1783a936e0318ab574c9bb06" translate="yes" xml:space="preserve">
          <source>Use &lt;code&gt;-pg&lt;/code&gt; flag when compiling and linking the code and run the executable file. While this program is executed, profiling data is collected in the file a.out.</source>
          <target state="translated">编译和链接代码并运行可执行文件时，请使用 &lt;code&gt;-pg&lt;/code&gt; 标志。 执行此程序时，分析数据收集在文件a.out中。</target>
        </trans-unit>
        <trans-unit id="74fa418da05131589c58de20bc5cd8a74884137e" translate="yes" xml:space="preserve">
          <source>Use a profiler in DEBUG mode to identify questionable parts of your code</source>
          <target state="translated">在 DEBUG 模式下使用剖析器来识别代码中存在问题的部分。</target>
        </trans-unit>
        <trans-unit id="5abe91df474f47b2ce52602a8d2c9a1a1dc9bcdb" translate="yes" xml:space="preserve">
          <source>Use a profiler in RELEASE mode to identify questionable parts of your code.</source>
          <target state="translated">在RELEASE模式下使用剖析器来识别代码中存在问题的部分。</target>
        </trans-unit>
        <trans-unit id="47e2b39318441908a26389043c5f493a08f3da25" translate="yes" xml:space="preserve">
          <source>Uses time sampling, I/O and CPU bottlenecks are revealed.</source>
          <target state="translated">使用时间采样,IO和CPU瓶颈暴露出来。</target>
        </trans-unit>
        <trans-unit id="32ebf48279743cab72d7ff1b43c30c357fa0b5af" translate="yes" xml:space="preserve">
          <source>View gprof output in kcachegrind</source>
          <target state="translated">在kcachegrind中查看gprof的输出。</target>
        </trans-unit>
        <trans-unit id="90e10e752882a92276f4abc5b712de714bd30e0f" translate="yes" xml:space="preserve">
          <source>We can observe that file graphically with &lt;code&gt;gprof2dot&lt;/code&gt; as asked at: &lt;a href=&quot;https://stackoverflow.com/questions/2439060/is-it-possible-to-get-a-graphical-representation-of-gprof-results&quot;&gt;Is it possible to get a graphical representation of gprof results?&lt;/a&gt;</source>
          <target state="translated">我们可以按照 &lt;code&gt;gprof2dot&lt;/code&gt; 的要求以图形方式观察该文件： &lt;a href=&quot;https://stackoverflow.com/questions/2439060/is-it-possible-to-get-a-graphical-representation-of-gprof-results&quot;&gt;是否可以获取gprof结果的图形表示？&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="10445757607ecf8a1d5924c1e702bf14a1ff75d2" translate="yes" xml:space="preserve">
          <source>We observe the following for the &lt;code&gt;-O0&lt;/code&gt; run:</source>
          <target state="translated">对于 &lt;code&gt;-O0&lt;/code&gt; 运行，我们观察到以下内容：</target>
        </trans-unit>
        <trans-unit id="9844986bd80729d071e13db938133404d11ba09f" translate="yes" xml:space="preserve">
          <source>Wherever in you code you can use :</source>
          <target state="translated">在你的代码中,无论你在哪里都可以使用.NET。</target>
        </trans-unit>
        <trans-unit id="674dfbcd4d095f0cd2550aa16069c396575c13ec" translate="yes" xml:space="preserve">
          <source>Which is the best replacement for KProf?</source>
          <target state="translated">KProf的替代品哪个好?</target>
        </trans-unit>
        <trans-unit id="9f5da50867e51ac3a771fd9c2dea086874d61262" translate="yes" xml:space="preserve">
          <source>Yet another way to look at it is called the &lt;a href=&quot;http://en.wikipedia.org/wiki/Rule_of_succession&quot;&gt;Rule Of Succession&lt;/a&gt;.
If you flip a coin 2 times, and it comes up heads both times, what does that tell you about the probable weighting of the coin?
The respected way to answer is to say that it's a Beta distribution, with average value (number of hits + 1) / (number of tries + 2) = (2+1)/(2+2) = 75%.</source>
          <target state="translated">另一种看待它的方法称为&lt;a href=&quot;http://en.wikipedia.org/wiki/Rule_of_succession&quot;&gt;继承规则&lt;/a&gt; 。 如果您掷硬币两次，并且两次都出现正面，那么这可能告诉您硬币的权重是多少？ 尊重的回答方式是说它是Beta分布，平均值（命中数+ 1）/（尝试数+ 2）=（2 + 1）/（2 + 2）= 75％。</target>
        </trans-unit>
        <trans-unit id="e477d44b67435b4f2a51bec584b8c8b5c70e29d8" translate="yes" xml:space="preserve">
          <source>You also define a few functions in &lt;code&gt;toolname.hpp&lt;/code&gt; :</source>
          <target state="translated">您还可以在 &lt;code&gt;toolname.hpp&lt;/code&gt; 中定义一些函数：</target>
        </trans-unit>
        <trans-unit id="d06ead62c5754d4f55d6c94097738323320c25d0" translate="yes" xml:space="preserve">
          <source>You can however use the color map to mitigate those problems a bit. For example, on the previous huge image, I finally managed to find the critical path on the left when I made the brilliant deduction that green comes after red, followed finally by darker and darker blue.</source>
          <target state="translated">不过,你可以用色图来缓解一下这些问题。例如,在上一张巨大的图片上,我终于找到了左边的关键路径,当我做了一个很好的推断,绿色是在红色之后,最后是更深更深的蓝色。</target>
        </trans-unit>
        <trans-unit id="ff319bc3545afd5e52b97cb66321e2f3c78b6c30" translate="yes" xml:space="preserve">
          <source>You can use &lt;a href=&quot;http://en.wikipedia.org/wiki/Valgrind&quot;&gt;Valgrind&lt;/a&gt; with the following options</source>
          <target state="translated">您可以将&lt;a href=&quot;http://en.wikipedia.org/wiki/Valgrind&quot;&gt;Valgrind&lt;/a&gt;与以下选项一起使用</target>
        </trans-unit>
        <trans-unit id="eff4537042a246b1bfc5a7ac9053825ab352cfb3" translate="yes" xml:space="preserve">
          <source>You can use a logging framework like &lt;a href=&quot;https://github.com/emilk/loguru&quot;&gt;&lt;code&gt;loguru&lt;/code&gt;&lt;/a&gt; since it includes timestamps and total uptime which can be used nicely for profiling:</source>
          <target state="translated">您可以使用&lt;a href=&quot;https://github.com/emilk/loguru&quot;&gt; &lt;code&gt;loguru&lt;/code&gt; 之&lt;/a&gt;类的日志记录框架，因为它包含时间戳记和总正常运行时间，可以很好地用于性能分析：</target>
        </trans-unit>
        <trans-unit id="57bb7c327c284cab00d7dbfef1ffa24df1be7086" translate="yes" xml:space="preserve">
          <source>You can use the iprof library:</source>
          <target state="translated">你可以使用iprof库。</target>
        </trans-unit>
        <trans-unit id="09eee30b7f6c8d8c09d32bfa040a796ee7c00fe5" translate="yes" xml:space="preserve">
          <source>You customize the amount of events generated to focus solely on what you desire. It helped us a lot for scheduling issues while consuming the amount of CPU we wanted based on the amount of logged events per second.</source>
          <target state="translated">你可以自定义生成的事件量,只专注于你想要的东西。它对我们的调度问题帮助很大,同时根据每秒钟记录的事件量来消耗我们想要的CPU量。</target>
        </trans-unit>
        <trans-unit id="5885a798125a50436ce410ba3e52f4fbdf4173ce" translate="yes" xml:space="preserve">
          <source>You may have multiple performance problems of different sizes. If you clean out any one of them, the remaining ones will take a larger percentage, and be easier to spot, on subsequent passes.
This &lt;em&gt;magnification effect&lt;/em&gt;, when compounded over multiple problems, can lead to truly massive speedup factors.</source>
          <target state="translated">您可能会遇到多个不同大小的性能问题。 如果您清除其中任何一个，其余的将在以后的传递中占更大的比例，并且更容易发现。 当&lt;em&gt;放大倍数加重&lt;/em&gt;多个问题时，会导致真正巨大的加速因素。</target>
        </trans-unit>
        <trans-unit id="016b297a8bba32365f38f766a39676ea6cbcfd2b" translate="yes" xml:space="preserve">
          <source>You need 3 files :</source>
          <target state="translated">你需要3个文件。</target>
        </trans-unit>
        <trans-unit id="747c3ce81ca5b275ec245943d96de7239bf6b766" translate="yes" xml:space="preserve">
          <source>You retrieve the so-called large buffer with all the data and a small interface parses it and shows events with name (up/down + value) like an oscilloscope does with colors (configured in &lt;code&gt;.hpp&lt;/code&gt; file).</source>
          <target state="translated">您检索具有所有数据的所谓大缓冲区，并通过一个小接口对其进行解析，并显示具有名称（上/下+值）的事件，就像示波器使用颜色（在 &lt;code&gt;.hpp&lt;/code&gt; 文件中配置）一样。</target>
        </trans-unit>
        <trans-unit id="2922e093bcf4e627dba20bdf0f73a9644a37a709" translate="yes" xml:space="preserve">
          <source>and for &lt;code&gt;-O3&lt;/code&gt;:</source>
          <target state="translated">对于 &lt;code&gt;-O3&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="2a5be5a09792a69cac7d084de41932a15ba3728f" translate="yes" xml:space="preserve">
          <source>and for the &lt;code&gt;-O3&lt;/code&gt; run:</source>
          <target state="translated">对于 &lt;code&gt;-O3&lt;/code&gt; 运行：</target>
        </trans-unit>
        <trans-unit id="faa969782037f341d207ee39f51209d6317fc20e" translate="yes" xml:space="preserve">
          <source>and the program does &lt;code&gt;O(n^2)&lt;/code&gt; loops in total. &lt;code&gt;seed&lt;/code&gt; is just to get different output without affecting runtime.</source>
          <target state="translated">并且该程序总共执行 &lt;code&gt;O(n^2)&lt;/code&gt; 循环。 &lt;code&gt;seed&lt;/code&gt; 只是为了获得不同的输出而不会影响运行时间。</target>
        </trans-unit>
        <trans-unit id="ce2455b67d76414f51f9fe06a8ce80817a3d7dbb" translate="yes" xml:space="preserve">
          <source>are there locks that are proving to be bottle necks ?</source>
          <target state="translated">是否有证明是瓶颈的锁?</target>
        </trans-unit>
        <trans-unit id="a6f071b118d3c8c52f2a76d5173897fc74f14d94" translate="yes" xml:space="preserve">
          <source>both &lt;code&gt;fast&lt;/code&gt; and &lt;code&gt;maybe_slow&lt;/code&gt; call &lt;code&gt;common&lt;/code&gt;, which accounts for the bulk of the program execution</source>
          <target state="translated">&lt;code&gt;fast&lt;/code&gt; 调用和 &lt;code&gt;maybe_slow&lt;/code&gt; 调用缓慢调用，都占了程序执行的大部分</target>
        </trans-unit>
        <trans-unit id="9684498d10b2a6f3e91e84082350f59fe31df044" translate="yes" xml:space="preserve">
          <source>but even then, you will be dragging the image around a lot to find what you want, see e.g. this image from a &quot;real&quot; software example taken from &lt;a href=&quot;https://gem5.atlassian.net/browse/GEM5-337&quot;&gt;this ticket&lt;/a&gt;:</source>
          <target state="translated">但是即使那样，您仍然会拖拉图像很多，以找到所需的内容，例如，从&lt;a href=&quot;https://gem5.atlassian.net/browse/GEM5-337&quot;&gt;这张票证中&lt;/a&gt;获取的&amp;ldquo;真实&amp;rdquo;软件示例中的图像：</target>
        </trans-unit>
        <trans-unit id="3af4c4be531f706f742d60135390bd70be4af5f9" translate="yes" xml:space="preserve">
          <source>but in such a simple program the output is not very easy to understand, since we can't easily see neither &lt;code&gt;maybe_slow&lt;/code&gt; nor &lt;code&gt;fast&lt;/code&gt; on that graph:</source>
          <target state="translated">但是在这样一个简单的程序中，输出不是很容易理解，因为我们不能轻易地在该图上看到 &lt;code&gt;maybe_slow&lt;/code&gt; 和 &lt;code&gt;fast&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="a7dd0b976062c449381eb00b3b4071e2c243cb98" translate="yes" xml:space="preserve">
          <source>by running the command &lt;code&gt;gprog --flat-profile a.out&lt;/code&gt; you got the following data</source>
          <target state="translated">通过运行命令 &lt;code&gt;gprog --flat-profile a.out&lt;/code&gt; ,您得到了以下数据</target>
        </trans-unit>
        <trans-unit id="4ad25e6079270f854bc1ab4b0f5c15dda777da63" translate="yes" xml:space="preserve">
          <source>callgrind is the valgrind's tool to profile code and kcachegrind is a KDE program that can visualize cachegrind output.</source>
          <target state="translated">callgrind是valgrind的工具,cachegrind是valgrind对代码进行剖析的工具,kcachegrind是KDE的程序,可以将cachegrind的输出可视化。</target>
        </trans-unit>
        <trans-unit id="fdde301a4277b76ba71eeffc0b565064e8e3eb36" translate="yes" xml:space="preserve">
          <source>centers around the function that is left indented (&lt;code&gt;maybe_flow&lt;/code&gt;). &lt;code&gt;[3]&lt;/code&gt; is the ID of that function. Above the function, are its callers, and below it the callees.</source>
          <target state="translated">以左缩进的函数（ &lt;code&gt;maybe_flow&lt;/code&gt; ）为中心。 &lt;code&gt;[3]&lt;/code&gt; 是该函数的ID。 该函数的上方是其调用方，而其下方是被调用方。</target>
        </trans-unit>
        <trans-unit id="c86da089418268adb10c8acdbbba40db282822e6" translate="yes" xml:space="preserve">
          <source>generates callgrind.out.x. Read it using kcachegrind.</source>
          <target state="translated">生成callgrind.out.x。使用kcachegrind读取它。</target>
        </trans-unit>
        <trans-unit id="b03467560afe04df21ced900b226b5f9dadcb62d" translate="yes" xml:space="preserve">
          <source>gprof is built-into GCC/binutils, so all we have to do is to compile with the &lt;code&gt;-pg&lt;/code&gt; option to enable gprof. We then run the program normally with a size CLI parameter that produces a run of reasonable duration of a few seconds (&lt;code&gt;10000&lt;/code&gt;):</source>
          <target state="translated">gprof内置在GCC / binutils中，因此我们要做的就是使用 &lt;code&gt;-pg&lt;/code&gt; 选项进行编译以启用gprof。 然后，我们通常使用size CLI参数正常运行该程序，该参数会产生合理的持续时间，为几秒钟（ &lt;code&gt;10000&lt;/code&gt; ）：</target>
        </trans-unit>
        <trans-unit id="9cb1273641689838d08c035718b933b581323243" translate="yes" xml:space="preserve">
          <source>gprof requires recompiling the software with instrumentation, and it also uses a sampling approach together with that instrumentation. It therefore strikes a balance between accuracy (sampling is not always fully accurate and can skip functions) and execution slowdown (instrumentation and sampling are relatively fast techniques that don't slow down execution very much).</source>
          <target state="translated">gprof需要用仪器设备重新编译软件,同时它还使用了采样的方法和仪器设备。因此,它在精度(采样不一定完全准确,可能会跳过函数)和执行速度变慢(仪器和采样都是相对较快的技术,不会使执行速度变慢)之间取得了平衡。</target>
        </trans-unit>
        <trans-unit id="6c20dbe559048f4187157de781dccf6f5ba1544e" translate="yes" xml:space="preserve">
          <source>how about IO, handled and optimized ?</source>
          <target state="translated">IO如何,处理和优化?</target>
        </trans-unit>
        <trans-unit id="8ad78a2d8b81d1f48b603777d625c316ec818e1d" translate="yes" xml:space="preserve">
          <source>is my algorithm correct ?</source>
          <target state="translated">我的算法是否正确?</target>
        </trans-unit>
        <trans-unit id="3a13f69d29ec474c66705e3aaadaa6bf50574a2c" translate="yes" xml:space="preserve">
          <source>is there a specific section of code that's proving to be a culprit ?</source>
          <target state="translated">是否有特定的代码部分被证明是罪魁祸首?</target>
        </trans-unit>
        <trans-unit id="406e031b8824ea26ae0bf4d7579a1d89e3fb5906" translate="yes" xml:space="preserve">
          <source>main.c</source>
          <target state="translated">main.c</target>
        </trans-unit>
        <trans-unit id="d72ae9adcbb77a799278937f8a9954bf7d5fee38" translate="yes" xml:space="preserve">
          <source>they don't summarize at the instruction level, and</source>
          <target state="translated">他们不在教学层面进行总结,而且</target>
        </trans-unit>
        <trans-unit id="813d13060cfdfe274f9dedbcdaad7e87361c8245" translate="yes" xml:space="preserve">
          <source>they give confusing summaries in the presence of recursion.</source>
          <target state="translated">他们在递归的情况下给出了令人困惑的总结。</target>
        </trans-unit>
        <trans-unit id="b38e2cc31a913ba13d49dd56d1a341b5650300c3" translate="yes" xml:space="preserve">
          <source>us the command &lt;code&gt;gprof --graph a.out&lt;/code&gt; to get the following data for each function which includes</source>
          <target state="translated">我们使用命令 &lt;code&gt;gprof --graph a.out&lt;/code&gt; 为每个函数获取以下数据，其中包括</target>
        </trans-unit>
        <trans-unit id="5e1acf1cde93e84e5121533952a55ce3fcfa7a9a" translate="yes" xml:space="preserve">
          <source>valgrind runs the program through the valgrind virtual machine. This makes the profiling very accurate, but it also produces a very large slowdown of the program. I have also mentioned kcachegrind previously at: &lt;a href=&quot;https://stackoverflow.com/questions/517589/tools-to-get-a-pictorial-function-call-graph-of-code/31190167#31190167&quot;&gt;Tools to get a pictorial function call graph of code&lt;/a&gt;</source>
          <target state="translated">valgrind通过valgrind虚拟机运行程序。 这样可以使配置文件非常准确，但也会导致程序运行速度大大降低。 我之前在以下地方也提到过kcachegrind： &lt;a href=&quot;https://stackoverflow.com/questions/517589/tools-to-get-a-pictorial-function-call-graph-of-code/31190167#31190167&quot;&gt;用于获取图形函数的代码调用图的工具&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5572540656d11e87dd5bd966d28d6665bd7fe5af" translate="yes" xml:space="preserve">
          <source>which gives as a familiar call graph like other tools, but with the clunky unit of number of samples rather than seconds.</source>
          <target state="translated">这给了一个熟悉的调用图,像其他工具,但与笨拙的单位的样本数而不是秒。</target>
        </trans-unit>
        <trans-unit id="88dc6e662ab6827a52e6b9e9027101485ddfb787" translate="yes" xml:space="preserve">
          <source>which gives:</source>
          <target state="translated">这给了:</target>
        </trans-unit>
        <trans-unit id="6e4de60efe1156a81e465d013cb2669ea775fcd4" translate="yes" xml:space="preserve">
          <source>which shows a GUI that contains data similar to the textual gprof output:</source>
          <target state="translated">它显示了一个GUI,包含了类似于文本gprof输出的数据。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
