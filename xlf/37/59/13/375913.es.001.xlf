<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="es" datatype="htmlbody" original="https://stackoverflow.com/questions/375913">
    <body>
      <group id="375913">
        <trans-unit id="474f686377fbd8012f361b7bd0929092b87eaedd" translate="yes" xml:space="preserve">
          <source>(The key is that we see &lt;code&gt;I&lt;/code&gt; more than once. If we only see it once, that doesn't tell us much except that &lt;code&gt;f&lt;/code&gt; &amp;gt; 0.)</source>
          <target state="translated">(La clave es que vemos &lt;code&gt;I&lt;/code&gt; m&amp;aacute;s de una vez. Si solo lo vemos una vez, eso no nos dice mucho, excepto que &lt;code&gt;f&lt;/code&gt; &amp;gt; 0).</target>
        </trans-unit>
        <trans-unit id="6bddfcbf9effc6af0dc2a4017b8af4d6d8c3b612" translate="yes" xml:space="preserve">
          <source>(not so good for multi-threads, function pointers)</source>
          <target state="translated">(no tan bueno para multi-hilos,punteros de función)</target>
        </trans-unit>
        <trans-unit id="ba000b98349f454ff4d963d272eea56d9076b1f0" translate="yes" xml:space="preserve">
          <source>(wikipedia) Valgrind is in essence a virtual
  machine using just-in-time (JIT)
  compilation techniques, including
  dynamic recompilation. Nothing from
  the original program ever gets run
  directly on the host processor.
  Instead, Valgrind first translates the
  program into a temporary, simpler form
  called Intermediate Representation
  (IR), which is a processor-neutral,
  SSA-based form. After the conversion,
  a tool (see below) is free to do
  whatever transformations it would like
  on the IR, before Valgrind translates
  the IR back into machine code and lets
  the host processor run it.</source>
          <target state="translated">(wikipedia)Valgrind es en esencia una máquina virtual que utiliza técnicas de compilación justo a tiempo (JIT),incluyendo la recompilación dinámica.Nada del programa original se ejecuta directamente en el procesador central.En su lugar,Valgrind primero traduce el programa a una forma temporal y más simple llamada Representación Intermedia (IR),que es una forma neutral del procesador,basada en SSA.Después de la conversión,una herramienta (ver más abajo)es libre de hacer las transformaciones que quiera en el IR,antes de que Valgrind traduzca el IR de nuevo en código de máquina y deje que el procesador anfitrión lo ejecute.</target>
        </trans-unit>
        <trans-unit id="2f52699e83e4b55e1f403f7c6079898edd6b4dff" translate="yes" xml:space="preserve">
          <source>- Above function , there is a list of functions that call the function .</source>
          <target state="translated">-Por encima de la función,hay una lista de funciones que llaman a la función .</target>
        </trans-unit>
        <trans-unit id="97381d92a175352738f31c6b84ed1b50492ba724" translate="yes" xml:space="preserve">
          <source>- Below function , there is a list of functions that are called by the function .</source>
          <target state="translated">-Debajo de la función,hay una lista de funciones que son llamadas por la función .</target>
        </trans-unit>
        <trans-unit id="2a06d6944b31ca287d1197fb91866bf150b9273f" translate="yes" xml:space="preserve">
          <source>- In each section, one function is marked with an index number.</source>
          <target state="translated">-En cada sección,una función está marcada con un número de índice.</target>
        </trans-unit>
        <trans-unit id="579a41d12eaf05b9059712857073fbbb03bff3f1" translate="yes" xml:space="preserve">
          <source>- how many seconds were spent in a function&amp;mdash;including and excluding calls to sub-functions,</source>
          <target state="translated">- cu&amp;aacute;ntos segundos se gastaron en una funci&amp;oacute;n, incluidas y excluidas las llamadas a subfunciones,</target>
        </trans-unit>
        <trans-unit id="2e6d2ff4ea07291f0fe68b6a1592baa107889c9d" translate="yes" xml:space="preserve">
          <source>- the average time per call.</source>
          <target state="translated">-el tiempo medio por llamada.</target>
        </trans-unit>
        <trans-unit id="09049524fc0fe3657417bd54beda8ddfcc7d00b5" translate="yes" xml:space="preserve">
          <source>- the number of calls,</source>
          <target state="translated">-el número de llamadas,</target>
        </trans-unit>
        <trans-unit id="726e946052f1dbb2b5959a244967231f79036c38" translate="yes" xml:space="preserve">
          <source>- what percentage of the overall time was spent for the function,</source>
          <target state="translated">-qué porcentaje del tiempo total se dedicó a la función,</target>
        </trans-unit>
        <trans-unit id="e299449c036492af510a743e9982f023ddc5e7c0" translate="yes" xml:space="preserve">
          <source>1- Flat profiling:</source>
          <target state="translated">1-Perfil plano:</target>
        </trans-unit>
        <trans-unit id="961cb4afe9142078530eb983e36df03834cb40bb" translate="yes" xml:space="preserve">
          <source>2- graph profiling</source>
          <target state="translated">2-perfil gráfico</target>
        </trans-unit>
        <trans-unit id="45d30831218fba6255f04ee5d6cb69901aae44b4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;main&lt;/code&gt; calls &lt;code&gt;fast&lt;/code&gt; and &lt;code&gt;maybe_slow&lt;/code&gt; 3 times, one of the &lt;code&gt;maybe_slow&lt;/code&gt; calls being slow</source>
          <target state="translated">llamadas &lt;code&gt;main&lt;/code&gt; &lt;code&gt;fast&lt;/code&gt; y &lt;code&gt;maybe_slow&lt;/code&gt; vez lentas 3 veces, una de las llamadas tal vez lentas es lenta</target>
        </trans-unit>
        <trans-unit id="2b26d60b77140f5edaf92172d3607731805526d8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;perf&lt;/code&gt; seems to use exclusively Linux kernel sampling mechanisms. This makes it very simple to setup, but also not fully accurate.</source>
          <target state="translated">&lt;code&gt;perf&lt;/code&gt; parece utilizar exclusivamente mecanismos de muestreo de kernel de Linux. Esto lo hace muy simple de configurar, pero tampoco totalmente preciso.</target>
        </trans-unit>
        <trans-unit id="beaa2542bdd5cb48d735e110bbd66e279a970967" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;valgrind&lt;/code&gt; with the combination of &lt;code&gt;callrind&lt;/code&gt; and &lt;code&gt;kcachegrind&lt;/code&gt; should provide a decent estimation on the points above and once it's established that there are issues with some section of code, I'd suggest do a micro bench mark &lt;code&gt;google benchmark&lt;/code&gt; is a good place to start.</source>
          <target state="translated">&lt;code&gt;valgrind&lt;/code&gt; con la combinaci&amp;oacute;n de &lt;code&gt;callrind&lt;/code&gt; y &lt;code&gt;kcachegrind&lt;/code&gt; deber&amp;iacute;a proporcionar una estimaci&amp;oacute;n decente en los puntos anteriores y una vez que se haya establecido que hay problemas con alguna secci&amp;oacute;n del c&amp;oacute;digo, sugerir&amp;iacute;a hacer un micro &lt;code&gt;google benchmark&lt;/code&gt; es un buen lugar para comenzar.</target>
        </trans-unit>
        <trans-unit id="558433641cab85097025b8b72bc823c8bf5e82ad" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;&lt;code&gt;perf&lt;/code&gt; from &lt;code&gt;linux-tools&lt;/code&gt;&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt; &lt;code&gt;perf&lt;/code&gt; de &lt;code&gt;linux-tools&lt;/code&gt; &lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="1e49cc3868d9deb4e8335aa49828e8c34898c8a8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;&lt;em&gt;For CPU bound applications:&lt;/em&gt;&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;&lt;em&gt;Para aplicaciones vinculadas a la CPU:&lt;/em&gt;&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="7196658c799f4043d28d713837910b362f91ab93" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;&lt;em&gt;For I/O bound applications:&lt;/em&gt;&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;&lt;em&gt;Para aplicaciones vinculadas de E / S:&lt;/em&gt;&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="ad2cef4043d111e1ae8c4692b0d3548a6613fcbd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Intel VTune is the best (free for educational purposes).&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;Intel VTune es el mejor (gratis con fines educativos).&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="a2b5b23586b55133642cbd2c4d931e703f33536e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Others:&lt;/strong&gt; AMD Codeanalyst (since replaced with AMD CodeXL), OProfile, 'perf' tools (apt-get install linux-tools)</source>
          <target state="translated">&lt;strong&gt;Otros:&lt;/strong&gt; AMD Codeanalyst (desde que se reemplaz&amp;oacute; con AMD CodeXL), OProfile, herramientas 'perf' (apt-get install linux-tools)</target>
        </trans-unit>
        <trans-unit id="9f805de8a6187ff8c605dc1b87c1dbf750af8806" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Survey of C++ profiling techniques&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;Encuesta de t&amp;eacute;cnicas de perfilado C ++&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="5935609263c49f484c816bb47bd583d8ac724622" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Use Valgrind, callgrind and kcachegrind:&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;Use Valgrind, callgrind y kcachegrind:&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="f76b5fe3b1c9563ed7d1caedae895977e4729ca3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Use google-perftools:&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;Utiliza google-perftools:&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="eaf91a71c594a624723a2785654641c0d49b648f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Use gprof (add -pg):&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;Use gprof (agregar -pg):&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="e2938d732361a260a7c9ca67f5add9a7cbbbdb25" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gperftools&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;gperftools&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="b34e9a3fe97464e4681d8532723054a812e2edcd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gprof&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;gprof&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="a645022e82b11913d78a887f81582e01e0a2ca4c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;valgrind callgrind&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;valgrind callgrind&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="b68ca6f37b8016941f5283f36f3759df87ef2002" translate="yes" xml:space="preserve">
          <source>A few other buzzwords if &lt;code&gt;gprof&lt;/code&gt; does not do the job for you: &lt;a href=&quot;http://en.wikipedia.org/wiki/Valgrind&quot;&gt;Valgrind&lt;/a&gt;, Intel &lt;a href=&quot;http://en.wikipedia.org/wiki/VTune&quot;&gt;VTune&lt;/a&gt;, Sun &lt;a href=&quot;http://en.wikipedia.org/wiki/DTrace&quot;&gt;DTrace&lt;/a&gt;.</source>
          <target state="translated">Algunas otras palabras de moda si &lt;code&gt;gprof&lt;/code&gt; no hace el trabajo por usted: &lt;a href=&quot;http://en.wikipedia.org/wiki/Valgrind&quot;&gt;Valgrind&lt;/a&gt; , Intel &lt;a href=&quot;http://en.wikipedia.org/wiki/VTune&quot;&gt;VTune&lt;/a&gt; , Sun &lt;a href=&quot;http://en.wikipedia.org/wiki/DTrace&quot;&gt;DTrace&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="7c42306a937ace4fd5a87d18a701b72889554283" translate="yes" xml:space="preserve">
          <source>ADDED, to give an intuitive feel for the difference between measuring and random stack sampling:</source>
          <target state="translated">AÑADIDO,para dar una sensación intuitiva de la diferencia entre la medición y el muestreo de pila aleatoria:</target>
        </trans-unit>
        <trans-unit id="9276e16e7cc14ecf9ae00f247b4196166d5d1aeb" translate="yes" xml:space="preserve">
          <source>ADDED: Let me make a Bayesian explanation of how it works.  Suppose there is some instruction &lt;code&gt;I&lt;/code&gt; (call or otherwise) which is on the call stack some fraction &lt;code&gt;f&lt;/code&gt; of the time (and thus costs that much). For simplicity, suppose we don't know what &lt;code&gt;f&lt;/code&gt; is, but assume it is either 0.1, 0.2, 0.3, ... 0.9, 1.0, and the prior probability of each of these possibilities is 0.1, so all of these costs are equally likely a-priori.</source>
          <target state="translated">AGREGADO: Perm&amp;iacute;tanme hacer una explicaci&amp;oacute;n bayesiana de c&amp;oacute;mo funciona. Supongamos que hay alguna instrucci&amp;oacute;n &lt;code&gt;I&lt;/code&gt; (llamada o no) que est&amp;aacute; en la pila de llamadas una fracci&amp;oacute;n &lt;code&gt;f&lt;/code&gt; del tiempo (y por lo tanto cuesta tanto). Para simplificar, supongamos que no sabemos qu&amp;eacute; es &lt;code&gt;f&lt;/code&gt; , pero supongamos que es 0.1, 0.2, 0.3, ... 0.9, 1.0, y la probabilidad previa de cada una de estas posibilidades es 0.1, por lo que todos estos costos son igualmente probablemente a priori.</target>
        </trans-unit>
        <trans-unit id="830b0bd9a847a1c514fcf9b261f15a81a0e87159" translate="yes" xml:space="preserve">
          <source>Actually a bit surprised not many mentioned about &lt;a href=&quot;https://github.com/google/benchmark&quot;&gt;google/benchmark&lt;/a&gt; , while it is a bit cumbersome to pin the specific area of code, specially if the code base is a little big one, however I found this really helpful when used in combination with &lt;code&gt;callgrind&lt;/code&gt;</source>
          <target state="translated">Realmente un poco sorprendido, no muchos mencionaron sobre &lt;a href=&quot;https://github.com/google/benchmark&quot;&gt;google / benchmark&lt;/a&gt; , aunque es un poco engorroso fijar el &amp;aacute;rea espec&amp;iacute;fica del c&amp;oacute;digo, especialmente si la base del c&amp;oacute;digo es un poco grande, sin embargo, encontr&amp;eacute; esto realmente &amp;uacute;til cuando se usa en combinaci&amp;oacute;n con &lt;code&gt;callgrind&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d081153271ba73f6b08b5c733dfc600113ac8050" translate="yes" xml:space="preserve">
          <source>Added: It might not be obvious, but the stack sampling technique works equally well in the presence of recursion. The reason is that the time that would be saved by removal of an instruction is approximated by the fraction of samples containing it, regardless of the number of times it may occur within a sample.</source>
          <target state="translated">Añadido:Puede que no sea obvio,pero la técnica de muestreo de la pila funciona igual de bien en presencia de recursividad.La razón es que el tiempo que se ahorraría al eliminar una instrucción se aproxima a la fracción de muestras que la contienen,independientemente del número de veces que pueda ocurrir dentro de una muestra.</target>
        </trans-unit>
        <trans-unit id="c111026e3becc4657525d1504f2a98b1227fa293" translate="yes" xml:space="preserve">
          <source>After running with either of those methods, we get a &lt;code&gt;prof.out&lt;/code&gt; profile data file as output. We can view that file graphically as an SVG with:</source>
          <target state="translated">Despu&amp;eacute;s de ejecutar cualquiera de esos m&amp;eacute;todos, obtenemos un archivo de datos de perfil &lt;code&gt;prof.out&lt;/code&gt; como salida. Podemos ver ese archivo gr&amp;aacute;ficamente como un SVG con:</target>
        </trans-unit>
        <trans-unit id="938c0204782973ed2cee8235fd6ed9aad9bd19a0" translate="yes" xml:space="preserve">
          <source>Also worth mentioning are</source>
          <target state="translated">También vale la pena mencionar</target>
        </trans-unit>
        <trans-unit id="30c36d00bdcf5c119d2a78569796b3b993fd0320" translate="yes" xml:space="preserve">
          <source>Also, if we go on the bottom right &quot;Call Graph&quot; tab, we see a call graph which we can export by right clicking it to obtain the following image with unreasonable amounts of white border :-)</source>
          <target state="translated">Además,si vamos a la parte inferior derecha de la pestaña &quot;Gráfico de llamada&quot;,vemos un gráfico de llamada que podemos exportar haciendo clic con el botón derecho del ratón para obtener la siguiente imagen con cantidades irrazonables de borde blanco :-)</target>
        </trans-unit>
        <trans-unit id="1ac3bae0296e72365db70cdbd1d168992f9eb4f9" translate="yes" xml:space="preserve">
          <source>Alternatively, we can also get some textual data with:</source>
          <target state="translated">Alternativamente,también podemos obtener algunos datos de texto con:</target>
        </trans-unit>
        <trans-unit id="6fe0b9fdb5d375e1bdddd4970a95aeb96c822b66" translate="yes" xml:space="preserve">
          <source>Alternatively, we can also observe the text output of the &lt;code&gt;gprof&lt;/code&gt; built-in binutils tool which we previously saved at:</source>
          <target state="translated">Alternativamente, tambi&amp;eacute;n podemos observar la salida de texto de la herramienta binutils incorporada &lt;code&gt;gprof&lt;/code&gt; que previamente guardamos en:</target>
        </trans-unit>
        <trans-unit id="d9dc64edf60dc7f5a3736a9279e32f3c03a13aa5" translate="yes" xml:space="preserve">
          <source>Alternatively, we can build the library in at link time, dispensing passing &lt;code&gt;LD_PRELOAD&lt;/code&gt; at runtime:</source>
          <target state="translated">Alternativamente, podemos construir la biblioteca en el momento del enlace, dispensando pasar &lt;code&gt;LD_PRELOAD&lt;/code&gt; en tiempo de ejecuci&amp;oacute;n:</target>
        </trans-unit>
        <trans-unit id="2cf1c27c41cddb303f6c128115968d14017c5f27" translate="yes" xml:space="preserve">
          <source>Another objection I often hear is: &quot;&lt;em&gt;It will stop someplace random, and it will miss the real problem&lt;/em&gt;&quot;.
This comes from having a prior concept of what the real problem is.
A key property of performance problems is that they defy expectations.
Sampling tells you something is a problem, and your first reaction is disbelief.
That is natural, but you can be sure if it finds a problem it is real, and vice-versa.</source>
          <target state="translated">Otra objeci&amp;oacute;n que escucho a menudo es: &quot; &lt;em&gt;Se detendr&amp;aacute; en alg&amp;uacute;n lugar al azar, y perder&amp;aacute; el verdadero problema&lt;/em&gt; &quot;. Esto viene de tener un concepto previo de cu&amp;aacute;l es el verdadero problema. Una propiedad clave de los problemas de rendimiento es que desaf&amp;iacute;an las expectativas. El muestreo te dice que algo es un problema, y ​​tu primera reacci&amp;oacute;n es de incredulidad. Eso es natural, pero puede estar seguro de que si encuentra un problema, es real y viceversa.</target>
        </trans-unit>
        <trans-unit id="fd2b8eb720b92068f67a32658ccfc03d658f34d9" translate="yes" xml:space="preserve">
          <source>Another perf GUI interfaces which might be worth it include:</source>
          <target state="translated">Otra interfase GUI perfecta que podría valer la pena incluir:</target>
        </trans-unit>
        <trans-unit id="48122e4897dd407d27c90182539de52d682b669c" translate="yes" xml:space="preserve">
          <source>Another tool build upon Valgrind is Massif. I use it to profile heap memory usage. It works great. What it does is that it gives you snapshots of memory usage -- detailed information WHAT holds WHAT percentage of memory, and WHO had put it there. Such information is available at different points of time of application run.</source>
          <target state="translated">Otra herramienta construida sobre Valgrind es Massif.Lo uso para hacer un perfil del uso de la memoria del montón.Funciona muy bien.Lo que hace es que te da instantáneas del uso de la memoria...información detallada de QUÉ contiene QUÉ porcentaje de memoria,y QUIÉN lo ha puesto ahí.Esta información está disponible en diferentes momentos de la ejecución de la aplicación.</target>
        </trans-unit>
        <trans-unit id="26cfea3667d4951989c3328a2749151be15581ab" translate="yes" xml:space="preserve">
          <source>Arm MAP is the profiler for parallel, multithreaded or single threaded C, C++, Fortran and F90 codes.  It provides in-depth analysis and bottleneck pinpointing to the source line.  Unlike most profilers, it's designed to be able to profile pthreads, OpenMP or MPI for parallel and threaded code.</source>
          <target state="translated">El brazo MAP es el perfilador para códigos paralelos,multihilo o de un solo hilo C,C++,Fortran y F90.Proporciona un análisis en profundidad y la localización de cuellos de botella en la línea de origen.A diferencia de la mayoría de los perfiladores,está diseñado para ser capaz de perfilar pthreads,OpenMP o MPI para código paralelo y de rosca.</target>
        </trans-unit>
        <trans-unit id="f607a78951293acbad31ccf19fd2a9c3fc0343e3" translate="yes" xml:space="preserve">
          <source>As a very quick summary for each section e.g.:</source>
          <target state="translated">Como un resumen muy rápido para cada sección,por ejemplo:</target>
        </trans-unit>
        <trans-unit id="1361188e9146c170f0554ec525ed827cb7592cc2" translate="yes" xml:space="preserve">
          <source>As no one mentioned Arm MAP, I'd add it as personally I have successfully used Map to profile a C++ scientific program.</source>
          <target state="translated">Como nadie mencionó Arm MAP,lo agregaría como personalmente he usado exitosamente Map para perfilar un programa científico de C++.</target>
        </trans-unit>
        <trans-unit id="33441507f930cc628d62c6420d02d66548f47be3" translate="yes" xml:space="preserve">
          <source>At runtime, we have to pass set the &lt;code&gt;LD_PRELOAD&lt;/code&gt; to point to &lt;code&gt;libprofiler.so&lt;/code&gt;, which you can find with &lt;code&gt;locate libprofiler.so&lt;/code&gt;, e.g. on my system:</source>
          <target state="translated">En el tiempo de ejecuci&amp;oacute;n, tenemos que pasar configurar &lt;code&gt;LD_PRELOAD&lt;/code&gt; para que apunte a &lt;code&gt;libprofiler.so&lt;/code&gt; , que puede encontrar con &lt;code&gt;locate libprofiler.so&lt;/code&gt; , por ejemplo, en mi sistema:</target>
        </trans-unit>
        <trans-unit id="275c32d1f4761fd0f70f1e315294ca1da95e4155" translate="yes" xml:space="preserve">
          <source>At work we have a really nice tool that helps us monitoring what we want in terms of scheduling. This has been useful numerous times.</source>
          <target state="translated">En el trabajo tenemos una herramienta muy buena que nos ayuda a controlar lo que queremos en términos de programación.Esto ha sido útil en numerosas ocasiones.</target>
        </trans-unit>
        <trans-unit id="c762222f2d1b7501653b936c0a39cd0b1aa6a933" translate="yes" xml:space="preserve">
          <source>Be sure to add &lt;code&gt;-pg&lt;/code&gt; to compilation before profiling:</source>
          <target state="translated">Aseg&amp;uacute;rese de agregar &lt;code&gt;-pg&lt;/code&gt; a la compilaci&amp;oacute;n antes de perfilar:</target>
        </trans-unit>
        <trans-unit id="d55e4a545eefcf858705baa9f8981e8dc001934b" translate="yes" xml:space="preserve">
          <source>Because we compiled with &lt;code&gt;-pg&lt;/code&gt;, running the program produces a file &lt;code&gt;gmon.out&lt;/code&gt; file containing the profiling data.</source>
          <target state="translated">Debido a que &lt;code&gt;-pg&lt;/code&gt; con -pg , la ejecuci&amp;oacute;n del programa produce un archivo &lt;code&gt;gmon.out&lt;/code&gt; que contiene los datos de creaci&amp;oacute;n de perfiles.</target>
        </trans-unit>
        <trans-unit id="0e20564692b92dee75c5d2d38aa97448c7b86382" translate="yes" xml:space="preserve">
          <source>But this has the downside that you have to first convert the data to the Common Trace Format, which can be done with &lt;code&gt;perf data --to-ctf&lt;/code&gt;, but it needs to be enabled at build time/have &lt;code&gt;perf&lt;/code&gt; new enough, either of which is not the case for the perf in Ubuntu 18.04</source>
          <target state="translated">Pero esto tiene la desventaja de que primero tiene que convertir los datos al formato de rastreo com&amp;uacute;n, que se puede hacer con &lt;code&gt;perf data --to-ctf&lt;/code&gt; , pero debe habilitarse en el momento de la compilaci&amp;oacute;n / tener un &lt;code&gt;perf&lt;/code&gt; suficientemente nuevo, cualquiera de los cuales no es el caso para el perf en Ubuntu 18.04</target>
        </trans-unit>
        <trans-unit id="7940ac6d6de4ba483d9863a1db60f99151f2c050" translate="yes" xml:space="preserve">
          <source>By default, this produces an extremely verbose output that explains what the output data means. Since I can't explain better than that, I'll let you read it yourself.</source>
          <target state="translated">Por defecto,esto produce una salida extremadamente verbosa que explica lo que significan los datos de salida.Como no puedo explicarlo mejor,dejaré que lo lea usted mismo.</target>
        </trans-unit>
        <trans-unit id="0e030095a99fb3257e11106bd611c56f45c2b3ed" translate="yes" xml:space="preserve">
          <source>Callgrind is a profiler build upon that. Main benefit is that you don't have to run your aplication for hours to get reliable result. Even one second run is sufficient to get rock-solid, reliable results, because Callgrind is a &lt;strong&gt;non-probing&lt;/strong&gt; profiler.</source>
          <target state="translated">Callgrind es un generador de perfiles sobre eso. El principal beneficio es que no tiene que ejecutar su aplicaci&amp;oacute;n durante horas para obtener resultados confiables. Incluso una segunda carrera es suficiente para obtener resultados s&amp;oacute;lidos y confiables, ya que Callgrind es un generador de perfiles &lt;strong&gt;sin sondeo&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="5fea0991e44d8690969eca7425688fa735bb73f4" translate="yes" xml:space="preserve">
          <source>Can you find the most critical call stack easily with all those tiny unsorted spaghetti lines going over one another? There might be better &lt;code&gt;dot&lt;/code&gt; options I'm sure, but I don't want to go there now. What we really need is a proper dedicated viewer for it, but I haven't found one yet:</source>
          <target state="translated">&amp;iquest;Puedes encontrar f&amp;aacute;cilmente la pila de llamadas m&amp;aacute;s cr&amp;iacute;tica con todas esas peque&amp;ntilde;as l&amp;iacute;neas de espagueti sin clasificar que se cruzan entre s&amp;iacute;? Puede haber mejores opciones de &lt;code&gt;dot&lt;/code&gt; , estoy seguro, pero no quiero ir all&amp;iacute; ahora. Lo que realmente necesitamos es un visor dedicado adecuado para ello, pero a&amp;uacute;n no he encontrado uno:</target>
        </trans-unit>
        <trans-unit id="20f2f816399d50a15985c861ae31ff174b730293" translate="yes" xml:space="preserve">
          <source>Caveat: Programmers tend to be skeptical of this technique unless they've used it themselves. They will say that profilers give you this information, but that is only true if they sample the entire call stack, and then let you examine a random set of samples. (The summaries are where the insight is lost.) Call graphs don't give you the same information, because</source>
          <target state="translated">Advertencia:Los programadores tienden a ser escépticos de esta técnica a menos que la hayan usado ellos mismos.Dirán que los perfiladores te dan esta información,pero eso sólo es cierto si toman una muestra de toda la pila de llamadas,y luego te dejan examinar un conjunto de muestras al azar.(En los resúmenes es donde se pierde el conocimiento.)Los gráficos de llamadas no te dan la misma información,porque</target>
        </trans-unit>
        <trans-unit id="099dc36893aed4eafd86bced522e4eb9937e5b98" translate="yes" xml:space="preserve">
          <source>Eclipse Trace Compass plugin: &lt;a href=&quot;https://www.eclipse.org/tracecompass/&quot;&gt;https://www.eclipse.org/tracecompass/&lt;/a&gt;</source>
          <target state="translated">Complemento Eclipse Trace Compass: &lt;a href=&quot;https://www.eclipse.org/tracecompass/&quot;&gt;https://www.eclipse.org/tracecompass/&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b0500648de7f8cd809c18dfe1a12b0af1850fcf1" translate="yes" xml:space="preserve">
          <source>First install gperftools with:</source>
          <target state="translated">Primero instala las herramientas Gperftools con:</target>
        </trans-unit>
        <trans-unit id="f718a4c61c9b86b8efe3e4a514c5978b1cf993ee" translate="yes" xml:space="preserve">
          <source>First we have to remove the &lt;code&gt;-pg&lt;/code&gt; flag to go back to normal compilation, otherwise the run actually fails with &lt;a href=&quot;https://stackoverflow.com/questions/2146082/valgrind-profiling-timer-expired&quot;&gt;&lt;code&gt;Profiling timer expired&lt;/code&gt;&lt;/a&gt;, and yes, this is so common that I did and there was a Stack Overflow question for it.</source>
          <target state="translated">Primero tenemos que eliminar el indicador &lt;code&gt;-pg&lt;/code&gt; para volver a la compilaci&amp;oacute;n normal, de lo contrario, la ejecuci&amp;oacute;n realmente falla con el &lt;a href=&quot;https://stackoverflow.com/questions/2146082/valgrind-profiling-timer-expired&quot;&gt; &lt;code&gt;Profiling timer expired&lt;/code&gt; &lt;/a&gt; , y s&amp;iacute;, esto es tan com&amp;uacute;n que lo hice y hab&amp;iacute;a una pregunta de desbordamiento de pila.</target>
        </trans-unit>
        <trans-unit id="731b082498e3f526227d4ecd0e76236c0bf482bd" translate="yes" xml:space="preserve">
          <source>First, &lt;code&gt;time&lt;/code&gt; tells us that the execution time with and without &lt;code&gt;-pg&lt;/code&gt; were the same, which is great: no slowdown! I have however seen accounts of 2x - 3x slowdowns on complex software, e.g. as &lt;a href=&quot;https://gem5.atlassian.net/browse/GEM5-337&quot;&gt;shown in this ticket&lt;/a&gt;.</source>
          <target state="translated">Primero, el &lt;code&gt;time&lt;/code&gt; nos dice que el tiempo de ejecuci&amp;oacute;n con y sin &lt;code&gt;-pg&lt;/code&gt; fue el mismo, lo cual es genial: &amp;iexcl;no hay desaceleraci&amp;oacute;n! Sin embargo, he visto cuentas de ralentizaciones 2x - 3x en software complejo, por ejemplo, como se &lt;a href=&quot;https://gem5.atlassian.net/browse/GEM5-337&quot;&gt;muestra en este ticket&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="bcac6f94f0587cb573e61450977a4e3d79d79994" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;-O3&lt;/code&gt;, see here like in the graphical output that &lt;code&gt;maybe_slow&lt;/code&gt; and &lt;code&gt;fast&lt;/code&gt; don't have a known parent, which is what the documentation says that &lt;code&gt;&amp;lt;spontaneous&amp;gt;&lt;/code&gt; means.</source>
          <target state="translated">Para &lt;code&gt;-O3&lt;/code&gt; , vea aqu&amp;iacute;, como en la salida gr&amp;aacute;fica, que &lt;code&gt;maybe_slow&lt;/code&gt; vez lento y &lt;code&gt;fast&lt;/code&gt; no tienen un padre conocido, que es lo que dice la documentaci&amp;oacute;n que significa &lt;code&gt;&amp;lt;spontaneous&amp;gt;&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="06d456a661f5b9ba3d3687e35852bf5858fab879" translate="yes" xml:space="preserve">
          <source>For CPU, the reason for profiling in &lt;strong&gt;DEBUG&lt;/strong&gt; mode is because if your tried profiling in &lt;strong&gt;RELEASE&lt;/strong&gt; mode, the compiler is going to reduce math, vectorize loops, and inline functions which tends to glob your code into an un-mappable mess when it's assembled. &lt;strong&gt;An un-mappable mess means your profiler will not be able to clearly identify what is taking so long because the assembly may not correspond to the source code under optimization&lt;/strong&gt;. If you need the performance (e.g. timing sensitive) of &lt;strong&gt;RELEASE&lt;/strong&gt; mode, disable debugger features as needed to keep a usable performance.</source>
          <target state="translated">Para la CPU, la raz&amp;oacute;n para crear un perfil en el modo &lt;strong&gt;DEBUG&lt;/strong&gt; es que si intentaste crear un perfil en el modo &lt;strong&gt;RELEASE&lt;/strong&gt; , el compilador reducir&amp;aacute; las matem&amp;aacute;ticas, vectorizar&amp;aacute; los bucles y las funciones en l&amp;iacute;nea que tienden a engrosar tu c&amp;oacute;digo en un desastre no asignable cuando se ensambla. &lt;strong&gt;Un desorden no asignable significa que su generador de perfiles no podr&amp;aacute; identificar claramente lo que est&amp;aacute; tardando tanto porque el ensamblado puede no corresponder con el c&amp;oacute;digo fuente bajo optimizaci&amp;oacute;n&lt;/strong&gt; . Si necesita el rendimiento (p. Ej., Sensible al tiempo) del modo &lt;strong&gt;RELEASE&lt;/strong&gt; , desactive las funciones del depurador seg&amp;uacute;n sea necesario para mantener un rendimiento utilizable.</target>
        </trans-unit>
        <trans-unit id="1fd1a4e3732d0165baa94d16658f7291d1e77cb2" translate="yes" xml:space="preserve">
          <source>For I/O-bound, the profiler can still identify I/O operations in &lt;strong&gt;RELEASE&lt;/strong&gt; mode because I/O operations are either externally linked to a shared library (most of the time) or in the worst case, will result in a sys-call interrupt vector (which is also easily identifiable by the profiler).</source>
          <target state="translated">Para el enlace de E / S, el generador de perfiles a&amp;uacute;n puede identificar operaciones de E / S en modo &lt;strong&gt;LIBERACI&amp;Oacute;N&lt;/strong&gt; porque las operaciones de E / S est&amp;aacute;n vinculadas externamente a una biblioteca compartida (la mayor&amp;iacute;a de las veces) o, en el peor de los casos, dar&amp;aacute;n como resultado un sistema. vector de interrupci&amp;oacute;n de llamada (que tambi&amp;eacute;n es f&amp;aacute;cilmente identificable por el generador de perfiles).</target>
        </trans-unit>
        <trans-unit id="da7a5db11d6d72fa7a5c6be30597dedf122892f3" translate="yes" xml:space="preserve">
          <source>For educational reasons, we will also do a run without optimizations enabled. Note that this is useless in practice, as you normally only care about optimizing the performance of the optimized program:</source>
          <target state="translated">Por razones educativas,también haremos una corrida sin optimizaciones habilitadas.Tenga en cuenta que esto es inútil en la práctica,ya que normalmente sólo se preocupa de optimizar el rendimiento del programa optimizado:</target>
        </trans-unit>
        <trans-unit id="59ad9936a7b0f4788c6ad3b0ea79e48e4a42d309" translate="yes" xml:space="preserve">
          <source>For single-threaded programs you can use &lt;strong&gt;igprof&lt;/strong&gt;, The Ignominous Profiler: &lt;a href=&quot;https://igprof.org/&quot;&gt;https://igprof.org/&lt;/a&gt; .</source>
          <target state="translated">Para programas de subproceso &amp;uacute;nico, puede usar &lt;strong&gt;igprof&lt;/strong&gt; , The Ignominous Profiler: &lt;a href=&quot;https://igprof.org/&quot;&gt;https://igprof.org/&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="e32962da9446269770cb3283319d83603bd59de6" translate="yes" xml:space="preserve">
          <source>HPCToolkit (&lt;a href=&quot;http://hpctoolkit.org/&quot;&gt;http://hpctoolkit.org/&lt;/a&gt;) - Open-source, works for parallel programs and has a GUI with which to look at the results multiple ways</source>
          <target state="translated">HPCToolkit ( &lt;a href=&quot;http://hpctoolkit.org/&quot;&gt;http://hpctoolkit.org/&lt;/a&gt; ): de c&amp;oacute;digo abierto, funciona para programas paralelos y tiene una GUI con la que puede ver los resultados de varias maneras</target>
        </trans-unit>
        <trans-unit id="9717437adb9e366c40c178b8b430e79730bf5d20" translate="yes" xml:space="preserve">
          <source>Here is an off-the-cuff illustration of the difference between examining measurements and examining stack samples.
The bottleneck could be one big blob like this, or numerous small ones, it makes no difference.</source>
          <target state="translated">Aquí hay una ilustración de la diferencia entre examinar las mediciones y examinar las muestras de la pila.El cuello de botella podría ser una gran mancha como esta,o numerosas pequeñas,no hay diferencia.</target>
        </trans-unit>
        <trans-unit id="84e40b82ddf20b2158fe8ae0544325f38d246cfd" translate="yes" xml:space="preserve">
          <source>Here, the &lt;code&gt;gprof&lt;/code&gt; tool reads the &lt;code&gt;gmon.out&lt;/code&gt; trace information, and generates a human readable report in &lt;code&gt;main.gprof&lt;/code&gt;, which &lt;code&gt;gprof2dot&lt;/code&gt; then reads to generate a graph.</source>
          <target state="translated">Aqu&amp;iacute;, la herramienta &lt;code&gt;gprof&lt;/code&gt; lee la informaci&amp;oacute;n de rastreo de &lt;code&gt;gmon.out&lt;/code&gt; y genera un informe legible por humanos en &lt;code&gt;main.gprof&lt;/code&gt; , que luego lee &lt;code&gt;gprof2dot&lt;/code&gt; para generar un gr&amp;aacute;fico.</target>
        </trans-unit>
        <trans-unit id="966dcaa64447ac43f2c9e4440202155cac9e9802" translate="yes" xml:space="preserve">
          <source>Hope the idea is not obfuscated by the lack of sample code.</source>
          <target state="translated">Espero que la idea no se vea ofuscada por la falta de código de muestra.</target>
        </trans-unit>
        <trans-unit id="b3431131b2f01fa3ab4147cebf53065a9fed2f0e" translate="yes" xml:space="preserve">
          <source>How can I profile C++ code running on Linux</source>
          <target state="translated">¿Cómo puedo hacer un perfil del código C++que se ejecuta en Linux</target>
        </trans-unit>
        <trans-unit id="e8d9679c58e452301713e88a029c43b1b26959f7" translate="yes" xml:space="preserve">
          <source>However, if you're in a hurry and you can manually interrupt your program under the debugger while it's being subjectively slow, there's a simple way to find performance problems.</source>
          <target state="translated">Sin embargo,si tienes prisa y puedes interrumpir manualmente tu programa bajo el depurador mientras está siendo subjetivamente lento,hay una forma sencilla de encontrar problemas de rendimiento.</target>
        </trans-unit>
        <trans-unit id="469ae7f984de3b3fd05c657ff55c2e05a4bb68f3" translate="yes" xml:space="preserve">
          <source>I assume you're using GCC. The standard solution would be to profile with &lt;a href=&quot;http://www.math.utah.edu/docs/info/gprof_toc.html&quot;&gt;gprof&lt;/a&gt;.</source>
          <target state="translated">Supongo que est&amp;aacute;s usando GCC. La soluci&amp;oacute;n est&amp;aacute;ndar ser&amp;iacute;a perfilar con &lt;a href=&quot;http://www.math.utah.edu/docs/info/gprof_toc.html&quot;&gt;gprof&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2626ed11413be172901c236c0ce46711b1e4e409" translate="yes" xml:space="preserve">
          <source>I choose SVG output instead of PNG because the SVG is searchable with Ctrl + F and the file size can be about 10x smaller. Also, the width and height of the generated image can be humoungous with tens of thousands of pixels for complex software, and GNOME &lt;code&gt;eog&lt;/code&gt; 3.28.1 bugs out in that case for PNGs, while SVGs get opened by my browser automatically. gimp 2.8 worked well though, see also:</source>
          <target state="translated">Elijo la salida SVG en lugar de PNG porque la SVG se puede buscar con Ctrl + F y el tama&amp;ntilde;o del archivo puede ser aproximadamente 10 veces m&amp;aacute;s peque&amp;ntilde;o. Adem&amp;aacute;s, el ancho y la altura de la imagen generada pueden ser enormes con decenas de miles de p&amp;iacute;xeles para software complejo, y GNOME &lt;code&gt;eog&lt;/code&gt; 3.28.1 produce errores en ese caso para PNG, mientras que mi navegador abre autom&amp;aacute;ticamente los SVG. Sin embargo, gimp 2.8 funcion&amp;oacute; bien, ver tambi&amp;eacute;n:</target>
        </trans-unit>
        <trans-unit id="219d1034d60f6bcc0c754cc1eb67709e077e407e" translate="yes" xml:space="preserve">
          <source>I enable &lt;code&gt;--dump-instr=yes --collect-jumps=yes&lt;/code&gt; because this also dumps information that enables us to view a per assembly line breakdown of performance, at a relatively small added overhead cost.</source>
          <target state="translated">&lt;code&gt;--dump-instr=yes --collect-jumps=yes&lt;/code&gt; porque esto tambi&amp;eacute;n voltea informaci&amp;oacute;n que nos permite ver un desglose del rendimiento por l&amp;iacute;nea de ensamblaje, a un costo adicional relativamente peque&amp;ntilde;o.</target>
        </trans-unit>
        <trans-unit id="161bfcf11ea394e1742c7e0a58a6044adc301c18" translate="yes" xml:space="preserve">
          <source>I have a C++ application, running on Linux, which I'm in the process of optimizing. How can I pinpoint which areas of my code are running slowly?</source>
          <target state="translated">Tengo una aplicación C++,corriendo en Linux,que estoy en proceso de optimizar.¿Cómo puedo señalar qué áreas de mi código están funcionando lentamente?</target>
        </trans-unit>
        <trans-unit id="3fa235219dffe6626130e2fa7a49a904b35f5be5" translate="yes" xml:space="preserve">
          <source>I have used HPCToolkit and VTune and they are very effective at finding the long pole in the tent and do not need your code to be recompiled (except that you have to use -g -O or RelWithDebInfo type build in CMake to get meaningful output). I have heard TAU is similar in capabilities.</source>
          <target state="translated">He usado HPCToolkit y VTune y son muy efectivos para encontrar el palo largo de la tienda y no necesitan que tu código sea recompilado (excepto que tienes que usar -g -O o RelWithDebInfo tipo build in CMake para obtener un resultado significativo).He oído que TAU es similar en capacidades.</target>
        </trans-unit>
        <trans-unit id="d652bdc8e2d715973adba2c14a8aeac6ea5b5520" translate="yes" xml:space="preserve">
          <source>I haven't tried it yet but I've heard good things about &lt;a href=&quot;https://github.com/gperftools/gperftools&quot;&gt;google-perftools&lt;/a&gt;. It is definitely worth a try.</source>
          <target state="translated">Todav&amp;iacute;a no lo he probado, pero he o&amp;iacute;do cosas buenas sobre &lt;a href=&quot;https://github.com/gperftools/gperftools&quot;&gt;google-perftools&lt;/a&gt; . Definitivamente vale la pena intentarlo.</target>
        </trans-unit>
        <trans-unit id="9adb36d7693cd006e9f81ed5125c898080cee016" translate="yes" xml:space="preserve">
          <source>I recommend in next window to click on &quot;Self&quot; column header, otherwise it shows that &quot;main()&quot; is most time consuming task. &quot;Self&quot; shows how much each function itself took time, not together with dependents.</source>
          <target state="translated">Recomiendo en la siguiente ventana hacer clic en el encabezado de la columna &quot;Self&quot;,de lo contrario muestra que &quot;main()&quot; es la tarea que más tiempo consume.&quot;Self&quot; muestra cuánto tiempo le llevó a cada función en sí,no junto con los dependientes.</target>
        </trans-unit>
        <trans-unit id="0f482aa371b33a0d0d5638abd01afc2e298063e0" translate="yes" xml:space="preserve">
          <source>I think &lt;code&gt;fast&lt;/code&gt; is not showing on that graph because kcachegrind must have simplified the visualization because that call takes up too little time, this will likely be the behavior you want on a real program. The right click menu has some settings to control when to cull such nodes, but I couldn't get it to show such a short call after a quick attempt. If I click on &lt;code&gt;fast&lt;/code&gt; on the left window, it does show a call graph with &lt;code&gt;fast&lt;/code&gt;, so that stack was actually captured. No one had yet found a way to show the complete graph call graph: &lt;a href=&quot;https://stackoverflow.com/questions/33769323/make-callgrind-show-all-function-calls-in-the-kcachegrind-callgraph&quot;&gt;Make callgrind show all function calls in the kcachegrind callgraph&lt;/a&gt;</source>
          <target state="translated">Creo que &lt;code&gt;fast&lt;/code&gt; no se muestra en ese gr&amp;aacute;fico porque kcachegrind debe haber simplificado la visualizaci&amp;oacute;n porque esa llamada toma muy poco tiempo, este ser&amp;aacute; probablemente el comportamiento que desea en un programa real. El men&amp;uacute; del bot&amp;oacute;n derecho tiene algunas configuraciones para controlar cu&amp;aacute;ndo eliminar dichos nodos, pero no pude hacer que mostrara una llamada tan corta despu&amp;eacute;s de un intento r&amp;aacute;pido. Si hago clic en &lt;code&gt;fast&lt;/code&gt; en la ventana izquierda, muestra un gr&amp;aacute;fico de llamadas con &lt;code&gt;fast&lt;/code&gt; , por lo que esa pila fue capturada. Nadie hab&amp;iacute;a encontrado todav&amp;iacute;a una manera de mostrar el gr&amp;aacute;fico de llamada de gr&amp;aacute;fico completo: &lt;a href=&quot;https://stackoverflow.com/questions/33769323/make-callgrind-show-all-function-calls-in-the-kcachegrind-callgraph&quot;&gt;hacer que callgrind muestre todas las llamadas de funci&amp;oacute;n en el gr&amp;aacute;fico de llamada de kcachegrind&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9b793d633b0f9e951abdde18b8e2544cd6ea68b3" translate="yes" xml:space="preserve">
          <source>I would use Valgrind and Callgrind as a base for my profiling tool suite. What is important to know is that Valgrind is basically a Virtual Machine:</source>
          <target state="translated">Usaría a Valgrind y Callgrind como base para mi conjunto de herramientas de perfiles.Lo que es importante saber es que Valgrind es básicamente una máquina virtual:</target>
        </trans-unit>
        <trans-unit id="6263011d671a27f31d355d03e88575c38e353dc8" translate="yes" xml:space="preserve">
          <source>I'm not sure if there is a nice way to do line-by-line profiling with gprof: &lt;a href=&quot;https://stackoverflow.com/questions/9608949/gprof-time-spent-in-particular-lines-of-code&quot;&gt;`gprof` time spent in particular lines of code&lt;/a&gt;</source>
          <target state="translated">No estoy seguro de si hay una buena manera de hacer perfiles l&amp;iacute;nea por l&amp;iacute;nea con gprof: el &lt;a href=&quot;https://stackoverflow.com/questions/9608949/gprof-time-spent-in-particular-lines-of-code&quot;&gt;tiempo `gprof` dedicado a l&amp;iacute;neas de c&amp;oacute;digo particulares&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bf065948e194a11f5b37f951703925e3fcde2b09" translate="yes" xml:space="preserve">
          <source>I've been using Gprof the last couple of days and have already found three significant limitations, one of which I've not seen documented anywhere else (yet):</source>
          <target state="translated">He estado usando Gprof el último par de días y ya he encontrado tres limitaciones significativas,una de las cuales no he visto documentada en ningún otro lugar (todavía):</target>
        </trans-unit>
        <trans-unit id="9d8fb54a20b1fdf3a9f7cf18def00021f5a8cb9f" translate="yes" xml:space="preserve">
          <source>IMHO identifying the piece that is causing bottleneck is the key here. I'd however try and answer the following questions first and choose tool based on that</source>
          <target state="translated">La clave está en la identificación de la pieza que está causando el cuello de botella.Sin embargo,intentaría responder a las siguientes preguntas primero y elegiría una herramienta basada en eso</target>
        </trans-unit>
        <trans-unit id="dc4e7b05ff6973f04bea57e88cf21f888a6f86ea" translate="yes" xml:space="preserve">
          <source>If you don't have a profiler, use the poor man's profiler. Hit pause while debugging your application. Most developer suites will break into assembly with commented line numbers. You're statistically likely to land in a region that is eating most of your CPU cycles.</source>
          <target state="translated">Si no tienes un perfil,usa el perfil del pobre hombre.Presiona pausa mientras depuras tu aplicación.La mayoría de las suites de desarrolladores entrarán en ensamblado con números de línea comentados.Es estadísticamente probable que aterrices en una región que se está comiendo la mayoría de tus ciclos de CPU.</target>
        </trans-unit>
        <trans-unit id="41540dddcb3ba901435fd0caa48e769fcf817d8a" translate="yes" xml:space="preserve">
          <source>If your goal is to use a profiler, use one of the suggested ones.</source>
          <target state="translated">Si su objetivo es usar un perfilador,use uno de los sugeridos.</target>
        </trans-unit>
        <trans-unit id="be4b7a0ea0d89ea3adbdc8138fbc2d0f2de5e625" translate="yes" xml:space="preserve">
          <source>In our example, outputs were for &lt;code&gt;-O0&lt;/code&gt;:</source>
          <target state="translated">En nuestro ejemplo, las salidas fueron para &lt;code&gt;-O0&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="431412748fb4a5baa6b65245373a7cfce7f17b4e" translate="yes" xml:space="preserve">
          <source>In this answer, I will use several different tools to a analyze a few very simple test programs, in order to concretely compare how those tools work.</source>
          <target state="translated">En esta respuesta,usaré varias herramientas diferentes para analizar unos pocos programas de prueba muy simples,con el fin de comparar concretamente cómo funcionan esas herramientas.</target>
        </trans-unit>
        <trans-unit id="635f12c9ec4169045833041b2619bfd5f883a755" translate="yes" xml:space="preserve">
          <source>Intel VTune (&lt;a href=&quot;https://software.intel.com/en-us/vtune&quot;&gt;https://software.intel.com/en-us/vtune&lt;/a&gt;) - If you have intel compilers this is very good</source>
          <target state="translated">Intel VTune ( &lt;a href=&quot;https://software.intel.com/en-us/vtune&quot;&gt;https://software.intel.com/en-us/vtune&lt;/a&gt; ): si tiene compiladores de inteligencia, esto es muy bueno</target>
        </trans-unit>
        <trans-unit id="4642763219e0fa4ea52daaf7bd5c245926adf1ae" translate="yes" xml:space="preserve">
          <source>It doesn't work properly on multi-threaded code, unless you use a &lt;a href=&quot;http://sam.zoy.org/writings/programming/gprof.html&quot;&gt;workaround&lt;/a&gt;</source>
          <target state="translated">No funciona correctamente en c&amp;oacute;digo de subprocesos m&amp;uacute;ltiples, a menos que use una &lt;a href=&quot;http://sam.zoy.org/writings/programming/gprof.html&quot;&gt;soluci&amp;oacute;n alternativa&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0a9760c557903dfa3fa680464b187e1b7fe3f2d6" translate="yes" xml:space="preserve">
          <source>It is a sampling profiler, along the lines of the... long... answer by Mike Dunlavey, which will gift wrap the results in a browsable call stack tree, annotated with the time or memory spent in each function, either cumulative or per-function.</source>
          <target state="translated">Es un perfilador de muestras,en la línea de la...larga...respuesta de Mike Dunlavey,que envolverá de regalo los resultados en un árbol de pila de llamadas navegable,anotado con el tiempo o la memoria empleados en cada función,ya sea acumulativo o por función.</target>
        </trans-unit>
        <trans-unit id="2ba19127395eb53bf0dd24f844c9a45fd905b0fe" translate="yes" xml:space="preserve">
          <source>It says &lt;a href=&quot;http://www.cs.utah.edu/dept/old/texinfo/as/gprof.html&quot;&gt;here&lt;/a&gt; that &quot;... the number-of-calls figures are derived by counting, not sampling. They are completely accurate...&quot;. Yet I find my call graph giving me 5345859132+784984078 as call stats to my most-called function, where the first number is supposed to be direct calls, and the second recursive calls (which are all from itself). Since this implied I had a bug, I put in long (64-bit) counters into the code and did the same run again. My counts: 5345859132 direct, and 78094395406 self-recursive calls.  There are a lot of digits there, so I'll point out the recursive calls I measure are 78bn, versus 784m from Gprof: a factor of 100 different. Both runs were single threaded and unoptimised code, one compiled &lt;code&gt;-g&lt;/code&gt; and the other &lt;code&gt;-pg&lt;/code&gt;.</source>
          <target state="translated">&lt;a href=&quot;http://www.cs.utah.edu/dept/old/texinfo/as/gprof.html&quot;&gt;Aqu&amp;iacute;&lt;/a&gt; dice que &quot;... las cifras del n&amp;uacute;mero de llamadas se obtienen contando, no muestreando. Son completamente precisas ...&quot;. Sin embargo, encuentro que mi gr&amp;aacute;fico de llamadas me da 5345859132 + 784984078 como estad&amp;iacute;sticas de llamadas a mi funci&amp;oacute;n m&amp;aacute;s llamada, donde se supone que el primer n&amp;uacute;mero son llamadas directas y las segundas llamadas recursivas (que son todas de s&amp;iacute; mismo). Como esto implicaba que ten&amp;iacute;a un error, puse contadores largos (64 bits) en el c&amp;oacute;digo y volv&amp;iacute; a hacer lo mismo. Mis recuentos: 5345859132 directo y 78094395406 llamadas autorrecurrentes. Hay muchos d&amp;iacute;gitos all&amp;iacute;, as&amp;iacute; que se&amp;ntilde;alar&amp;eacute; que las llamadas recursivas que mido son 78 mil millones, frente a 784 millones de Gprof: un factor de 100 diferentes. Ambas ejecuciones eran c&amp;oacute;digo de subproceso &amp;uacute;nico y no optimizado, una compilada &lt;code&gt;-g&lt;/code&gt; y la otra &lt;code&gt;-pg&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7cb7e9d2c15a7ea8f0115ba5bd906e6333b013da" translate="yes" xml:space="preserve">
          <source>It will generate a file called &lt;code&gt;callgrind.out.x&lt;/code&gt;. You can then use &lt;code&gt;kcachegrind&lt;/code&gt; tool to read this file. It will give you a graphical analysis of things with results like which lines cost how much.</source>
          <target state="translated">&lt;code&gt;callgrind.out.x&lt;/code&gt; un archivo llamado callgrind.out.x . Luego puede usar la herramienta &lt;code&gt;kcachegrind&lt;/code&gt; para leer este archivo. Le dar&amp;aacute; un an&amp;aacute;lisis gr&amp;aacute;fico de las cosas con resultados como qu&amp;eacute; l&amp;iacute;neas cuestan cu&amp;aacute;nto.</target>
        </trans-unit>
        <trans-unit id="76e28acade3cc2a27154df90bec362b0a631dfad" translate="yes" xml:space="preserve">
          <source>It's cross-platform and allows you not to measure performance of your application also in real-time. You can even couple it with a live graph.
Full disclaimer: I am the author.</source>
          <target state="translated">Es multiplataforma y permite no medir el rendimiento de su aplicación también en tiempo real.Incluso puedes acoplarlo con un gráfico en vivo.Descargo de responsabilidad completo:Soy el autor.</target>
        </trans-unit>
        <trans-unit id="4b1a94bb5d0f0a36818c07bf5e21ac7db7f6e83a" translate="yes" xml:space="preserve">
          <source>It's in C++ and must be customized to your needs. Unfortunately I can't share code, just concepts.
You use a &quot;large&quot; &lt;code&gt;volatile&lt;/code&gt; buffer containing timestamps and event ID that you can dump post mortem or after stopping the logging system (and dump this into a file for example).</source>
          <target state="translated">Est&amp;aacute; en C ++ y debe personalizarse seg&amp;uacute;n sus necesidades. Desafortunadamente no puedo compartir c&amp;oacute;digo, solo conceptos. Utiliza un b&amp;uacute;fer &lt;code&gt;volatile&lt;/code&gt; &quot;grande&quot; que contiene marcas de tiempo e ID de evento que puede volcar post mortem o despu&amp;eacute;s de detener el sistema de registro (y volcar esto en un archivo, por ejemplo).</target>
        </trans-unit>
        <trans-unit id="6866c589c53df9bdbafd58f6594aa3b9926e0556" translate="yes" xml:space="preserve">
          <source>Just halt it several times, and each time look at the call stack. If there is some code that is wasting some percentage of the time, 20% or 50% or whatever, that is the probability that you will catch it in the act on each sample. So that is roughly the percentage of samples on which you will see it. There is no educated guesswork required.
If you do have a guess as to what the problem is, this will prove or disprove it.</source>
          <target state="translated">Sólo deténgalo varias veces,y cada vez mire la pila de llamadas.Si hay algún código que está perdiendo algún porcentaje del tiempo,20% o 50% o lo que sea,esa es la probabilidad de que lo atrapen en el acto en cada muestra.Así que ese es aproximadamente el porcentaje de muestras en el que lo verás.No se requieren conjeturas educadas.Si tienes una suposición de cuál es el problema,esto lo probará o refutará.</target>
        </trans-unit>
        <trans-unit id="c54056bc2b49a6ba4b6c533b7638afae9a9d85da" translate="yes" xml:space="preserve">
          <source>MAP is commercial software.</source>
          <target state="translated">MAP es un software comercial.</target>
        </trans-unit>
        <trans-unit id="7f9d5d60edf957a9efb30c849d25e99af6c8e0ff" translate="yes" xml:space="preserve">
          <source>Measurement is horizontal; it tells you what fraction of time specific routines take.
Sampling is vertical.
If there is any way to avoid what the whole program is doing at that moment, &lt;em&gt;and if you see it on a second sample&lt;/em&gt;, you've found the bottleneck.
That's what makes the difference - seeing the whole reason for the time being spent, not just how much.</source>
          <target state="translated">La medida es horizontal; le dice qu&amp;eacute; fracci&amp;oacute;n de tiempo toman rutinas espec&amp;iacute;ficas. El muestreo es vertical. Si hay alguna forma de evitar lo que est&amp;aacute; haciendo todo el programa en ese momento, &lt;em&gt;y si lo ve en una segunda muestra&lt;/em&gt; , ha encontrado el cuello de botella. Eso es lo que marca la diferencia: ver el motivo completo del tiempo que se dedica, no solo cu&amp;aacute;nto.</target>
        </trans-unit>
        <trans-unit id="477fe7fb42d087f23d433845410b477d61ad6d95" translate="yes" xml:space="preserve">
          <source>N.B.</source>
          <target state="translated">N.B.</target>
        </trans-unit>
        <trans-unit id="5f635737b21ddea700a64d9ba1681ee8be0ba798" translate="yes" xml:space="preserve">
          <source>Newer kernels (e.g. the latest Ubuntu kernels) come with the new 'perf' tools (&lt;code&gt;apt-get install linux-tools&lt;/code&gt;) AKA &lt;a href=&quot;https://en.wikipedia.org/wiki/Perf_(Linux)&quot;&gt;perf_events&lt;/a&gt;.</source>
          <target state="translated">Los nuevos n&amp;uacute;cleos (por ejemplo, los &amp;uacute;ltimos n&amp;uacute;cleos de Ubuntu) vienen con las nuevas herramientas 'perf' ( &lt;code&gt;apt-get install linux-tools&lt;/code&gt; ) AKA &lt;a href=&quot;https://en.wikipedia.org/wiki/Perf_(Linux)&quot;&gt;perf_events&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="130b95199b8439c21a930c61f8c68941d88450a0" translate="yes" xml:space="preserve">
          <source>Now it says P(f &amp;gt;= 0.5) is 26%, up from the prior assumption of 0.6%. So Bayes allows us to update our estimate of the probable cost of &lt;code&gt;I&lt;/code&gt;. If the amount of data is small, it doesn't tell us accurately what the cost is, only that it is big enough to be worth fixing.</source>
          <target state="translated">Ahora dice que P (f&amp;gt; = 0.5) es 26%, por encima del supuesto anterior de 0.6%. Entonces Bayes nos permite actualizar nuestra estimaci&amp;oacute;n del costo probable de &lt;code&gt;I&lt;/code&gt; . Si la cantidad de datos es peque&amp;ntilde;a, no nos dice con precisi&amp;oacute;n cu&amp;aacute;l es el costo, solo que es lo suficientemente grande como para que valga la pena arreglarla.</target>
        </trans-unit>
        <trans-unit id="1cc1ff9a05639f9a0179486bc64f7be0e48efc59" translate="yes" xml:space="preserve">
          <source>Now we have some files named callgrind.out.* in current directory. To see profiling results use:</source>
          <target state="translated">Ahora tenemos algunos archivos llamados callgrind.out.*en el directorio actual.Para ver los resultados del perfil use:</target>
        </trans-unit>
        <trans-unit id="3440a6fe929867ee09b71eda8a51b92ff532c181" translate="yes" xml:space="preserve">
          <source>Now when it works and we want to start profiling we should run in another window:</source>
          <target state="translated">Ahora,cuando funcione y queramos empezar a hacer perfiles,deberíamos correr en otra ventana:</target>
        </trans-unit>
        <trans-unit id="d3cb823c58e17cf865b583e5a20e385601ec48b1" translate="yes" xml:space="preserve">
          <source>Off the bat, &lt;code&gt;time&lt;/code&gt; tells us that the program took 29.5 seconds to execute, so we had a slowdown of about 15x on this example. Clearly, this slowdown is going to be a serious limitation for larger workloads. On the &quot;real world software example&quot; &lt;a href=&quot;https://gem5.atlassian.net/browse/GEM5-337&quot;&gt;mentioned here&lt;/a&gt;, I observed a slowdown of 80x.</source>
          <target state="translated">De buenas a primeras, el &lt;code&gt;time&lt;/code&gt; nos dice que el programa tard&amp;oacute; 29,5 segundos en ejecutarse, por lo que tuvimos una desaceleraci&amp;oacute;n de aproximadamente 15x en este ejemplo. Claramente, esta desaceleraci&amp;oacute;n ser&amp;aacute; una limitaci&amp;oacute;n seria para cargas de trabajo m&amp;aacute;s grandes. En el &quot;ejemplo de software del mundo real&quot; &lt;a href=&quot;https://gem5.atlassian.net/browse/GEM5-337&quot;&gt;mencionado aqu&amp;iacute;&lt;/a&gt; , observ&amp;eacute; una desaceleraci&amp;oacute;n de 80x.</target>
        </trans-unit>
        <trans-unit id="a2a01fb750150dbf6d32038f978233485a2955b5" translate="yes" xml:space="preserve">
          <source>On the a more complex example it becomes clear what the graph means:</source>
          <target state="translated">En el ejemplo más complejo se hace evidente lo que significa el gráfico:</target>
        </trans-unit>
        <trans-unit id="7ea07b1121db66a159bd732b96f60b6003126f27" translate="yes" xml:space="preserve">
          <source>Once you have understood the data output format, you can reduce verbosity to show just the data without the tutorial with the &lt;code&gt;-b&lt;/code&gt; option:</source>
          <target state="translated">Una vez que haya entendido el formato de salida de datos, puede reducir la verbosidad para mostrar solo los datos sin el tutorial con la opci&amp;oacute;n &lt;code&gt;-b&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="dbba8950c83f558b0a5b23878c05f60878612272" translate="yes" xml:space="preserve">
          <source>One cool thing about &lt;code&gt;perf&lt;/code&gt; is the FlameGraph tool from Brendan Gregg which displays the call stack timings in a very neat way that allows you to quickly see the big calls. The tool is available at: &lt;a href=&quot;https://github.com/brendangregg/FlameGraph&quot;&gt;https://github.com/brendangregg/FlameGraph&lt;/a&gt; and is also mentioned on his perf tutorial at: &lt;a href=&quot;http://www.brendangregg.com/perf.html#FlameGraphs&quot;&gt;http://www.brendangregg.com/perf.html#FlameGraphs&lt;/a&gt; When I ran &lt;code&gt;perf&lt;/code&gt; without &lt;code&gt;sudo&lt;/code&gt; I got &lt;a href=&quot;https://github.com/brendangregg/FlameGraph/issues/132&quot;&gt;&lt;code&gt;ERROR: No stack counts found&lt;/code&gt;&lt;/a&gt; so for now I'll be doing it with &lt;code&gt;sudo&lt;/code&gt;:</source>
          <target state="translated">Una cosa genial sobre &lt;code&gt;perf&lt;/code&gt; es la herramienta FlameGraph de Brendan Gregg, que muestra los tiempos de la pila de llamadas de una manera muy ordenada que le permite ver r&amp;aacute;pidamente las grandes llamadas. La herramienta est&amp;aacute; disponible en: &lt;a href=&quot;https://github.com/brendangregg/FlameGraph&quot;&gt;https://github.com/brendangregg/FlameGraph&lt;/a&gt; y tambi&amp;eacute;n se menciona en su tutorial de perf en: &lt;a href=&quot;http://www.brendangregg.com/perf.html#FlameGraphs&quot;&gt;http://www.brendangregg.com/perf.html#FlameGraphs&lt;/a&gt; Cuando ejecut&amp;eacute; &lt;code&gt;perf&lt;/code&gt; sin &lt;code&gt;sudo&lt;/code&gt; obtuve &lt;a href=&quot;https://github.com/brendangregg/FlameGraph/issues/132&quot;&gt; &lt;code&gt;ERROR: No stack counts found&lt;/code&gt; &lt;/a&gt; as&amp;iacute; que por ahora lo har&amp;eacute; con &lt;code&gt;sudo&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="e1e2d5fff1404430f80a9893a77239e1b369f4c6" translate="yes" xml:space="preserve">
          <source>P.P.S As a rough generality, the more layers of abstraction you have in your software, the more likely you are to find that that is the cause of performance problems (and the opportunity to get speedup).</source>
          <target state="translated">P.P.S Como generalidad aproximada,cuantas más capas de abstracción tengas en tu software,más probable es que encuentres que esa es la causa de los problemas de rendimiento (y la oportunidad de conseguir velocidad).</target>
        </trans-unit>
        <trans-unit id="bbd10fe9dd107bcf29e061a1dbca691e2b9849ec" translate="yes" xml:space="preserve">
          <source>P.S. This can also be done on multi-thread programs if there is a way to collect call-stack samples of the thread pool at a point in time, as there is in Java.</source>
          <target state="translated">P.D.Esto también se puede hacer en programas multi-hilo si hay una manera de recoger muestras de la pila de llamadas del conjunto de hilos en un momento dado,como hay en Java.</target>
        </trans-unit>
        <trans-unit id="4d12df151686c7a7b5c9580f37caebf54fd9c01a" translate="yes" xml:space="preserve">
          <source>Previously called &quot;Google Performance Tools&quot;, source: &lt;a href=&quot;https://github.com/gperftools/gperftools&quot;&gt;https://github.com/gperftools/gperftools&lt;/a&gt; Sample based.</source>
          <target state="translated">Anteriormente llamado &quot;Google Performance Tools&quot;, fuente: &lt;a href=&quot;https://github.com/gperftools/gperftools&quot;&gt;https://github.com/gperftools/gperftools&lt;/a&gt; Basado en muestras.</target>
        </trans-unit>
        <trans-unit id="4830d80b5bfe85f29931b456dde61e5a468406a7" translate="yes" xml:space="preserve">
          <source>Related question &lt;a href=&quot;https://stackoverflow.com/questions/56672/how-do-you-profile-your-code&quot;&gt;here&lt;/a&gt;.</source>
          <target state="translated">Pregunta relacionada &lt;a href=&quot;https://stackoverflow.com/questions/56672/how-do-you-profile-your-code&quot;&gt;aqu&amp;iacute;&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2577e776d1182d8b8493c8c47c6882960c233679" translate="yes" xml:space="preserve">
          <source>See also: &lt;a href=&quot;https://stackoverflow.com/questions/10874308/how-to-use-google-perf-tools&quot;&gt;How to use google perf tools&lt;/a&gt;</source>
          <target state="translated">Ver tambi&amp;eacute;n: &lt;a href=&quot;https://stackoverflow.com/questions/10874308/how-to-use-google-perf-tools&quot;&gt;C&amp;oacute;mo usar las herramientas de Google Perf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d6bdcd52d3599e74ca39aae086a3b346358703f1" translate="yes" xml:space="preserve">
          <source>See also: &lt;a href=&quot;https://stackoverflow.com/questions/46949407/gperftools-profile-file-not-dumped&quot;&gt;gperftools - profile file not dumped&lt;/a&gt;</source>
          <target state="translated">Ver tambi&amp;eacute;n: &lt;a href=&quot;https://stackoverflow.com/questions/46949407/gperftools-profile-file-not-dumped&quot;&gt;gperftools - archivo de perfil no volcado&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6298bf00768899707674b3da8584b8c3f21f0334" translate="yes" xml:space="preserve">
          <source>So then I try to benchmark the &lt;code&gt;-O0&lt;/code&gt; program to see if that shows anything, and only now, at last, do I see a call graph:</source>
          <target state="translated">Entonces trato de comparar el programa &lt;code&gt;-O0&lt;/code&gt; para ver si eso muestra algo, y solo ahora, por fin, veo un gr&amp;aacute;fico de llamadas:</target>
        </trans-unit>
        <trans-unit id="72cc02d3d1195fc59d61046450268e28e14c7f19" translate="yes" xml:space="preserve">
          <source>So this is what I recommend. Run program first:</source>
          <target state="translated">Así que esto es lo que recomiendo.Ejecute el programa primero:</target>
        </trans-unit>
        <trans-unit id="f6772b5f69ff63328b7702c6bb60c75468582ac8" translate="yes" xml:space="preserve">
          <source>So we compile and run as:</source>
          <target state="translated">Así que compilamos y ejecutamos como:</target>
        </trans-unit>
        <trans-unit id="cd0d4cbba9f07fc22442ddaca962fad827eac83d" translate="yes" xml:space="preserve">
          <source>So, even a very small number of samples can tell us a lot about the cost of instructions that it sees. (And it will see them with a frequency, on average, proportional to their cost. If &lt;code&gt;n&lt;/code&gt; samples are taken, and &lt;code&gt;f&lt;/code&gt; is the cost, then &lt;code&gt;I&lt;/code&gt; will appear on &lt;code&gt;nf+/-sqrt(nf(1-f))&lt;/code&gt; samples. Example, &lt;code&gt;n=10&lt;/code&gt;, &lt;code&gt;f=0.3&lt;/code&gt;, that is &lt;code&gt;3+/-1.4&lt;/code&gt; samples.)</source>
          <target state="translated">Entonces, incluso un n&amp;uacute;mero muy peque&amp;ntilde;o de muestras puede decirnos mucho sobre el costo de las instrucciones que ve. (Y los ver&amp;aacute; con una frecuencia, en promedio, proporcional a su costo. Si se toman &lt;code&gt;n&lt;/code&gt; muestras &lt;code&gt;f&lt;/code&gt; es el costo, entonces aparecer&amp;eacute; en &lt;code&gt;nf+/-sqrt(nf(1-f))&lt;/code&gt; muestras. Ejemplo , &lt;code&gt;n=10&lt;/code&gt; , &lt;code&gt;f=0.3&lt;/code&gt; , es decir &lt;code&gt;3+/-1.4&lt;/code&gt; muestras.)</target>
        </trans-unit>
        <trans-unit id="08239f177013e0b069b1f213aca8eed8d3e8fd03" translate="yes" xml:space="preserve">
          <source>Suppose the prior assumptions are different. Suppose we assume P(f=0.1) is .991 (nearly certain), and all the other possibilities are almost impossible (0.001). In other words, our prior certainty is that &lt;code&gt;I&lt;/code&gt; is cheap. Then we get:</source>
          <target state="translated">Supongamos que los supuestos anteriores son diferentes. Supongamos que suponemos que P (f = 0.1) es .991 (casi seguro), y todas las dem&amp;aacute;s posibilidades son casi imposibles (0.001). En otras palabras, nuestra certeza previa es que &lt;code&gt;I&lt;/code&gt; barato. Entonces obtenemos:</target>
        </trans-unit>
        <trans-unit id="d3b210cd62d3d15648f1f008cbd6a6ff99536cd2" translate="yes" xml:space="preserve">
          <source>TAU (&lt;a href=&quot;http://www.cs.uoregon.edu/research/tau/home.php&quot;&gt;http://www.cs.uoregon.edu/research/tau/home.php&lt;/a&gt;)</source>
          <target state="translated">TAU ( &lt;a href=&quot;http://www.cs.uoregon.edu/research/tau/home.php&quot;&gt;http://www.cs.uoregon.edu/research/tau/home.php&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="ed5f5a3f654a364aa389b4161a87b5a87e7b8ec1" translate="yes" xml:space="preserve">
          <source>TODO on complex C++ software, I see some entries of type &lt;code&gt;&amp;lt;cycle N&amp;gt;&lt;/code&gt;, e.g. &lt;code&gt;&amp;lt;cycle 11&amp;gt;&lt;/code&gt; where I'd expect function names, what does that mean? I noticed there is a &quot;Cycle Detection&quot; button to toggle that on and off, but what does it mean?</source>
          <target state="translated">TODO en el complejo software de C ++, veo algunas entradas de tipo &lt;code&gt;&amp;lt;cycle N&amp;gt;&lt;/code&gt; , por ejemplo, &lt;code&gt;&amp;lt;cycle 11&amp;gt;&lt;/code&gt; donde esperar&amp;iacute;a nombres de funciones, &amp;iquest;qu&amp;eacute; significa eso? Not&amp;eacute; que hay un bot&amp;oacute;n de &quot;Detecci&amp;oacute;n de ciclos&quot; para activar y desactivar eso, pero &amp;iquest;qu&amp;eacute; significa?</target>
        </trans-unit>
        <trans-unit id="b13a96071f0d7fdcbc2465e3e45593db4c6ebbfd" translate="yes" xml:space="preserve">
          <source>TODO there are a log of &lt;code&gt;[unknown]&lt;/code&gt; functions in that example, why is that?</source>
          <target state="translated">TODO hay un registro de funciones &lt;code&gt;[unknown]&lt;/code&gt; en ese ejemplo, &amp;iquest;por qu&amp;eacute; es eso?</target>
        </trans-unit>
        <trans-unit id="a19af3eb40fbc53ad472e0220f8b098385c16862" translate="yes" xml:space="preserve">
          <source>TODO: what happened on the &lt;code&gt;-O3&lt;/code&gt; execution? Is it simply that &lt;code&gt;maybe_slow&lt;/code&gt; and &lt;code&gt;fast&lt;/code&gt; were too fast and did not get any samples? Does it work well with &lt;code&gt;-O3&lt;/code&gt; on larger programs that take longer to execute? Did I miss some CLI option? I found out about &lt;code&gt;-F&lt;/code&gt; to control the sample frequency in Hertz, but I turned it up to the max allowed by default of &lt;code&gt;-F 39500&lt;/code&gt; (could be increased with &lt;code&gt;sudo&lt;/code&gt;) and I still don't see clear calls.</source>
          <target state="translated">TODO: &amp;iquest;qu&amp;eacute; pas&amp;oacute; con la ejecuci&amp;oacute;n de &lt;code&gt;-O3&lt;/code&gt; ? &amp;iquest;Es simplemente que &lt;code&gt;maybe_slow&lt;/code&gt; y &lt;code&gt;fast&lt;/code&gt; fueron demasiado r&amp;aacute;pidos y no obtuvieron ninguna muestra? &amp;iquest; &lt;code&gt;-O3&lt;/code&gt; bien con -O3 en programas m&amp;aacute;s grandes que tardan m&amp;aacute;s en ejecutarse? &amp;iquest;Me perd&amp;iacute; alguna opci&amp;oacute;n de CLI? Descubr&amp;iacute; que &lt;code&gt;-F&lt;/code&gt; controla la frecuencia de muestreo en Hertz, pero lo &lt;code&gt;-F 39500&lt;/code&gt; al m&amp;aacute;ximo permitido por defecto de -F 39500 (podr&amp;iacute;a incrementarse con &lt;code&gt;sudo&lt;/code&gt; ) y todav&amp;iacute;a no veo llamadas claras.</target>
        </trans-unit>
        <trans-unit id="6dc4e6a8f235d6662619c2b13f904dd1fd3359cb" translate="yes" xml:space="preserve">
          <source>TODO: why is &lt;code&gt;main&lt;/code&gt; missing from the &lt;code&gt;-O3&lt;/code&gt; output, even though I can see it on a &lt;code&gt;bt&lt;/code&gt; in GDB? &lt;a href=&quot;https://stackoverflow.com/questions/39041871/missing-function-from-gprof-output&quot;&gt;Missing function from GProf output&lt;/a&gt; I think it is because gprof is also sampling based in addition to its compiled instrumentation, and the &lt;code&gt;-O3&lt;/code&gt;&lt;code&gt;main&lt;/code&gt; is just too fast and got no samples.</source>
          <target state="translated">TODO: &amp;iquest;por qu&amp;eacute; falta la salida &lt;code&gt;main&lt;/code&gt; &lt;code&gt;-O3&lt;/code&gt; , aunque puedo verlo en un &lt;code&gt;bt&lt;/code&gt; en GDB? &lt;a href=&quot;https://stackoverflow.com/questions/39041871/missing-function-from-gprof-output&quot;&gt;Falta la funci&amp;oacute;n de la salida de GProf&lt;/a&gt; Creo que es porque gprof tambi&amp;eacute;n est&amp;aacute; basado en muestreo adem&amp;aacute;s de su instrumentaci&amp;oacute;n compilada, y el &lt;code&gt;main&lt;/code&gt; &lt;code&gt;-O3&lt;/code&gt; es demasiado r&amp;aacute;pido y no obtuvo muestras.</target>
        </trans-unit>
        <trans-unit id="6e53b60f1e0dfa855a4eef9b666d7279dabbee1a" translate="yes" xml:space="preserve">
          <source>Tested in Ubuntu 18.04, gprof2dot 2019.11.30, valgrind 3.13.0, perf 4.15.18, Linux kernel 4.15.0, FLameGraph 1a0dc6985aad06e76857cf2a354bd5ba0c9ce96b, gperftools 2.5-2.</source>
          <target state="translated">Probado en Ubuntu 18.04,gprof2dot 2019.11.30,valgrind 3.13.0,perf 4.15.18,Linux kernel 4.15.0,FLameGraph 1a0dc6985aad06e76857cf2a354bd5ba0c9ce96b,gperftools 2.5-2.</target>
        </trans-unit>
        <trans-unit id="3a80a1a4fbc3469221b7d5afd5713915cd326c8f" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;-O0&lt;/code&gt; output is pretty much self-explanatory. For example, it shows that the 3 &lt;code&gt;maybe_slow&lt;/code&gt; calls and their child calls take up 97.56% of the total runtime, although execution of &lt;code&gt;maybe_slow&lt;/code&gt; itself without children accounts for 0.00% of the total execution time, i.e. almost all the time spent in that function was spent on child calls.</source>
          <target state="translated">La salida &lt;code&gt;-O0&lt;/code&gt; se explica por s&amp;iacute; misma. Por ejemplo, muestra que las 3 llamadas &lt;code&gt;maybe_slow&lt;/code&gt; y sus llamadas secundarias ocupan el 97.56% del tiempo de ejecuci&amp;oacute;n total, aunque la ejecuci&amp;oacute;n de &lt;code&gt;maybe_slow&lt;/code&gt; sin hijos representa el 0.00% del tiempo total de ejecuci&amp;oacute;n, es decir, casi todo el tiempo dedicado a esa funci&amp;oacute;n fue gastado en llamadas de ni&amp;ntilde;os.</target>
        </trans-unit>
        <trans-unit id="e42365e1a6bff7fc2be05d355cadfefbdf5ea497" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;probe&lt;/code&gt; function uses a few assembly lines to retrieve the clock timestamp ASAP and then sets an entry in the buffer. We also have an atomic increment to safely find an index where to store the log event.
Of course buffer is circular.</source>
          <target state="translated">La funci&amp;oacute;n de &lt;code&gt;probe&lt;/code&gt; utiliza algunas l&amp;iacute;neas de ensamblaje para recuperar la marca de tiempo del reloj lo antes posible y luego establece una entrada en el b&amp;uacute;fer. Tambi&amp;eacute;n tenemos un incremento at&amp;oacute;mico para encontrar de forma segura un &amp;iacute;ndice donde almacenar el evento de registro. Por supuesto, el buffer es circular.</target>
        </trans-unit>
        <trans-unit id="f1066d2d94872d1ccbce2036cf5aac073098024e" translate="yes" xml:space="preserve">
          <source>The answer to run &lt;code&gt;valgrind --tool=callgrind&lt;/code&gt; is not quite complete without some options. We usually do not want to profile 10 minutes of slow startup time under Valgrind and want to profile our program when it is doing some task.</source>
          <target state="translated">La respuesta para ejecutar &lt;code&gt;valgrind --tool=callgrind&lt;/code&gt; no est&amp;aacute; completa sin algunas opciones. Por lo general, no queremos perfilar 10 minutos de tiempo de inicio lento en Valgrind y queremos perfilar nuestro programa cuando est&amp;aacute; realizando alguna tarea.</target>
        </trans-unit>
        <trans-unit id="36529a459a19fd1c86e51be05eccbd6859fb938f" translate="yes" xml:space="preserve">
          <source>The call graph gets confused by function pointers. Example: I have a function called &lt;code&gt;multithread()&lt;/code&gt; which enables me to multi-thread a specified function over a specified array (both passed as arguments). Gprof however, views all calls to &lt;code&gt;multithread()&lt;/code&gt; as equivalent for the purposes of computing time spent in children. Since some functions I pass to &lt;code&gt;multithread()&lt;/code&gt; take much longer than others my call graphs are mostly useless. (To those wondering if threading is the issue here: no, &lt;code&gt;multithread()&lt;/code&gt; can optionally, and did in this case, run everything sequentially on the calling thread only).</source>
          <target state="translated">El gr&amp;aacute;fico de llamadas se confunde con los punteros de funci&amp;oacute;n. Ejemplo: Tengo una funci&amp;oacute;n llamada &lt;code&gt;multithread()&lt;/code&gt; que me permite subprocesar una funci&amp;oacute;n espec&amp;iacute;fica sobre una matriz espec&amp;iacute;fica (ambas pasadas como argumentos). Sin embargo, Gprof considera todas las llamadas a &lt;code&gt;multithread()&lt;/code&gt; como equivalentes para calcular el tiempo que pasan en los ni&amp;ntilde;os. Como algunas funciones que paso a &lt;code&gt;multithread()&lt;/code&gt; tardan mucho m&amp;aacute;s que otras, mis gr&amp;aacute;ficos de llamadas son en su mayor&amp;iacute;a in&amp;uacute;tiles. (Para aquellos que se preguntan si el tema es el subproceso: no, &lt;code&gt;multithread()&lt;/code&gt; puede opcionalmente, y en este caso, ejecutar todo secuencialmente solo en el subproceso de llamada).</target>
        </trans-unit>
        <trans-unit id="a13b1906e442733e77eedc379dcc29d514b536bc" translate="yes" xml:space="preserve">
          <source>The concept is to define events in &lt;code&gt;tool_events_id.hpp&lt;/code&gt; like that :</source>
          <target state="translated">El concepto es definir eventos en &lt;code&gt;tool_events_id.hpp&lt;/code&gt; as&amp;iacute;:</target>
        </trans-unit>
        <trans-unit id="cdbc70168923d9d12dc32d25f0bf32b6568e5382" translate="yes" xml:space="preserve">
          <source>The downside of this is that there seems to be no Ubuntu package, and building it requires Qt 5.10 while Ubuntu 18.04 is at Qt 5.9.</source>
          <target state="translated">La desventaja de esto es que parece que no hay ningún paquete de Ubuntu,y construirlo requiere Qt 5.10 mientras que Ubuntu 18.04 está en Qt 5.9.</target>
        </trans-unit>
        <trans-unit id="9b0a146d516aab93931f86859127170f3626293f" translate="yes" xml:space="preserve">
          <source>The following test program is very simple and does the following:</source>
          <target state="translated">El siguiente programa de pruebas es muy simple y hace lo siguiente:</target>
        </trans-unit>
        <trans-unit id="c15b8d0e65310a3d3e9195caeac933a4e6e84c5b" translate="yes" xml:space="preserve">
          <source>The important thing is that these tools can be &lt;strong&gt;system profiling&lt;/strong&gt; and not just process profiling - they can show the interaction between threads, processes and the kernel and let you understand the scheduling and I/O dependencies between processes.</source>
          <target state="translated">Lo importante es que estas herramientas pueden ser la &lt;strong&gt;creaci&amp;oacute;n de perfiles del sistema&lt;/strong&gt; y no solo la creaci&amp;oacute;n de perfiles de procesos: pueden mostrar la interacci&amp;oacute;n entre subprocesos, procesos y el n&amp;uacute;cleo y le permiten comprender la programaci&amp;oacute;n y las dependencias de E / S entre procesos.</target>
        </trans-unit>
        <trans-unit id="b0d289675b93f88ef384de46c84952610710e353" translate="yes" xml:space="preserve">
          <source>The last column says that, for example, the probability that &lt;code&gt;f&lt;/code&gt; &amp;gt;= 0.5 is 92%, up from the prior assumption of 60%.</source>
          <target state="translated">La &amp;uacute;ltima columna dice que, por ejemplo, la probabilidad de que &lt;code&gt;f&lt;/code&gt; &amp;gt; = 0.5 sea del 92%, por encima del supuesto anterior del 60%.</target>
        </trans-unit>
        <trans-unit id="627640c346afb672f5025becd9584112a15ea55e" translate="yes" xml:space="preserve">
          <source>The nicest way to view this data I've found so far is to make pprof output the same format that kcachegrind takes as input (yes, the Valgrind-project-viewer-tool) and use kcachegrind to view that:</source>
          <target state="translated">La mejor manera de ver estos datos que he encontrado hasta ahora es hacer que la salida de pprof tenga el mismo formato que la entrada de kcachegrind (sí,la herramienta de visualización de proyectos de Valgrind)y usar kcachegrind para ver eso:</target>
        </trans-unit>
        <trans-unit id="23eff0579ea840d749f7dacd94ef9acf84a98ca9" translate="yes" xml:space="preserve">
          <source>The program interface is:</source>
          <target state="translated">La interfaz del programa es:</target>
        </trans-unit>
        <trans-unit id="19c4249e44db6d866c57af334d73685ec61d153c" translate="yes" xml:space="preserve">
          <source>The run generates a profile data file named &lt;code&gt;callgrind.out.&amp;lt;pid&amp;gt;&lt;/code&gt; e.g. &lt;code&gt;callgrind.out.8554&lt;/code&gt; in my case. We view that file with:</source>
          <target state="translated">La ejecuci&amp;oacute;n genera un archivo de datos de perfil llamado &lt;code&gt;callgrind.out.&amp;lt;pid&amp;gt;&lt;/code&gt; por ejemplo, &lt;code&gt;callgrind.out.8554&lt;/code&gt; en mi caso. Vemos ese archivo con:</target>
        </trans-unit>
        <trans-unit id="ed9bf7107eccd71620c4c6d6bab4692580d8a7db" translate="yes" xml:space="preserve">
          <source>The slow call of &lt;code&gt;maybe_slow&lt;/code&gt; is 10x longer, and dominates runtime if we consider calls to the child function &lt;code&gt;common&lt;/code&gt;. Ideally, the profiling tool will be able to point us to the specific slow call.</source>
          <target state="translated">La llamada lenta de &lt;code&gt;maybe_slow&lt;/code&gt; es 10 veces m&amp;aacute;s larga y domina el tiempo de ejecuci&amp;oacute;n si consideramos que las llamadas a la funci&amp;oacute;n secundaria son &lt;code&gt;common&lt;/code&gt; . Idealmente, la herramienta de creaci&amp;oacute;n de perfiles podr&amp;aacute; indicarnos la llamada lenta espec&amp;iacute;fica.</target>
        </trans-unit>
        <trans-unit id="b6c8bf98ecf41059e3237cb533da10773d9fb4c5" translate="yes" xml:space="preserve">
          <source>The source for gprof2dot is at: &lt;a href=&quot;https://github.com/jrfonseca/gprof2dot&quot;&gt;https://github.com/jrfonseca/gprof2dot&lt;/a&gt;</source>
          <target state="translated">La fuente de gprof2dot est&amp;aacute; en: &lt;a href=&quot;https://github.com/jrfonseca/gprof2dot&quot;&gt;https://github.com/jrfonseca/gprof2dot&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="769adfccf68fa21aab6b3394b205656f5dab0061" translate="yes" xml:space="preserve">
          <source>Then suppose we take just 2 stack samples, and we see instruction &lt;code&gt;I&lt;/code&gt; on both samples, designated observation &lt;code&gt;o=2/2&lt;/code&gt;. This gives us new estimates of the frequency &lt;code&gt;f&lt;/code&gt; of &lt;code&gt;I&lt;/code&gt;, according to this:</source>
          <target state="translated">Luego, supongamos que tomamos solo 2 muestras de pila, y vemos la instrucci&amp;oacute;n &lt;code&gt;I&lt;/code&gt; en ambas muestras, designada observaci&amp;oacute;n &lt;code&gt;o=2/2&lt;/code&gt; . Esto nos da nuevas estimaciones de la frecuencia &lt;code&gt;f&lt;/code&gt; de &lt;code&gt;I&lt;/code&gt; , de acuerdo con esto:</target>
        </trans-unit>
        <trans-unit id="7291eec49875f5d9df200c27efdf4f591116af3f" translate="yes" xml:space="preserve">
          <source>Then switch to RELEASE mode and comment out the questionable sections of your code (stub it with nothing) until you see changes in performance.</source>
          <target state="translated">Luego cambia al modo LIBERACIÓN y comenta las secciones dudosas de tu código (no le des nada)hasta que veas cambios en el rendimiento.</target>
        </trans-unit>
        <trans-unit id="2e742db948961b9c3cb6ead916f107d3e8dda15c" translate="yes" xml:space="preserve">
          <source>Then, we can enable the gperftools CPU profiler in two ways: at runtime, or at build time.</source>
          <target state="translated">Entonces,podemos habilitar el perfil de la CPU de gperftools de dos maneras:en tiempo de ejecución,o en tiempo de construcción.</target>
        </trans-unit>
        <trans-unit id="924edd37a29a71ab9fe4c8baa007eb8497e44c49" translate="yes" xml:space="preserve">
          <source>There are profilers now that sample the stack, even on wall-clock time, but &lt;em&gt;what comes out&lt;/em&gt; is measurements (or hot path, or hot spot, from which a &quot;bottleneck&quot; can easily hide). What they don't show you (and they easily could) is the actual samples themselves. And if your goal is to &lt;em&gt;find&lt;/em&gt; the bottleneck, the number of them you need to see is, &lt;em&gt;on average&lt;/em&gt;, 2 divided by the fraction of time it takes.
So if it takes 30% of time, 2/.3 = 6.7 samples, on average, will show it, and the chance that 20 samples will show it is 99.2%.</source>
          <target state="translated">Ahora hay perfiladores que muestrean la pila, incluso en el tiempo del reloj de pared, pero &lt;em&gt;lo que sale&lt;/em&gt; son las mediciones (o ruta caliente, o punto caliente, de la cual se puede ocultar f&amp;aacute;cilmente un &quot;cuello de botella&quot;). Lo que no le muestran (y podr&amp;iacute;an f&amp;aacute;cilmente) son las muestras en s&amp;iacute; mismas. Y si su objetivo es &lt;em&gt;encontrar&lt;/em&gt; el cuello de botella, el n&amp;uacute;mero de ellos que necesita ver es, &lt;em&gt;en promedio&lt;/em&gt; , 2 dividido por la fracci&amp;oacute;n de tiempo que lleva. Entonces, si toma el 30% del tiempo, 2 / .3 = 6.7 muestras, en promedio, lo mostrar&amp;aacute;n, y la probabilidad de que 20 muestras lo muestren es del 99.2%.</target>
        </trans-unit>
        <trans-unit id="7e09a7534638018a7af14062b8d9c917ce457a53" translate="yes" xml:space="preserve">
          <source>There is two different type of profiling</source>
          <target state="translated">Hay dos tipos diferentes de perfiles</target>
        </trans-unit>
        <trans-unit id="571edc923bc5a6cee4f85a019f1c49bdbbf47a9d" translate="yes" xml:space="preserve">
          <source>These are the two methods I use for speeding up my code:</source>
          <target state="translated">Estos son los dos métodos que uso para acelerar mi código:</target>
        </trans-unit>
        <trans-unit id="8311f0252f91f1308742d21d20d0472314d898dd" translate="yes" xml:space="preserve">
          <source>These come with classic sampling profilers (&lt;a href=&quot;http://manpages.ubuntu.com/manpages/trusty/man1/perf.1.html&quot;&gt;man-page&lt;/a&gt;) as well as the awesome &lt;a href=&quot;http://web.archive.org/web/20090922171904/http://blog.fenrus.org/?p=5&quot;&gt;timechart&lt;/a&gt;!</source>
          <target state="translated">&amp;iexcl;Estos vienen con perfiladores de muestreo cl&amp;aacute;sicos ( &lt;a href=&quot;http://manpages.ubuntu.com/manpages/trusty/man1/perf.1.html&quot;&gt;p&amp;aacute;gina de manual&lt;/a&gt; ), as&amp;iacute; como el impresionante &lt;a href=&quot;http://web.archive.org/web/20090922171904/http://blog.fenrus.org/?p=5&quot;&gt;diagrama de tiempo&lt;/a&gt; !</target>
        </trans-unit>
        <trans-unit id="e0c6e507414a31dbfe361257f83996883b16fb14" translate="yes" xml:space="preserve">
          <source>They will also say it only works on toy programs, when actually it works on any program, and it seems to work better on bigger programs, because they tend to have more problems to find.
They will say it sometimes finds things that aren't problems, but that is only true if you see something &lt;em&gt;once&lt;/em&gt;. If you see a problem on more than one sample, it is real.</source>
          <target state="translated">Tambi&amp;eacute;n dir&amp;aacute;n que solo funciona en programas de juguetes, cuando en realidad funciona en cualquier programa, y ​​parece funcionar mejor en programas m&amp;aacute;s grandes, porque tienden a tener m&amp;aacute;s problemas para encontrar. Dir&amp;aacute;n que a veces encuentra cosas que no son problemas, pero eso solo es cierto si ves algo &lt;em&gt;una vez&lt;/em&gt; . Si ve un problema en m&amp;aacute;s de una muestra, es real.</target>
        </trans-unit>
        <trans-unit id="83b6b75b50c6b06f6bb30d23afad367e7399b38c" translate="yes" xml:space="preserve">
          <source>This added 0.2s to execution, so we are fine time-wise, but I still don't see much of interest, after expanding the &lt;code&gt;common&lt;/code&gt; node with the keyboard right arrow:</source>
          <target state="translated">Esto agreg&amp;oacute; 0.2s a la ejecuci&amp;oacute;n, por lo que estamos bien en cuanto al tiempo, pero a&amp;uacute;n no veo mucho inter&amp;eacute;s, despu&amp;eacute;s de expandir el nodo &lt;code&gt;common&lt;/code&gt; con la flecha derecha del teclado:</target>
        </trans-unit>
        <trans-unit id="6b876e96a3c3578737d673685356fe1f9dcb0ab6" translate="yes" xml:space="preserve">
          <source>This is a response to &lt;a href=&quot;https://stackoverflow.com/a/375930/321731&quot;&gt;Nazgob's Gprof answer&lt;/a&gt;.</source>
          <target state="translated">Esta es una respuesta a &lt;a href=&quot;https://stackoverflow.com/a/375930/321731&quot;&gt;la respuesta Gprof de Nazgob&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="b830cecda0a3aff113570c5b101b80608993a7f6" translate="yes" xml:space="preserve">
          <source>This turns profiling on. To turn it off and stop whole task we might use:</source>
          <target state="translated">Esto hace que el perfil se active.Para apagarlo y detener toda la tarea que podríamos usar:</target>
        </trans-unit>
        <trans-unit id="ae3ef8d5cd3e8ccedaacfb1558e954d08cae24db" translate="yes" xml:space="preserve">
          <source>This was GNU &lt;a href=&quot;https://en.wikipedia.org/wiki/Gprof&quot;&gt;Gprof&lt;/a&gt; (GNU Binutils for Debian) 2.18.0.20080103 running under 64-bit Debian Lenny, if that helps anyone.</source>
          <target state="translated">Este fue GNU &lt;a href=&quot;https://en.wikipedia.org/wiki/Gprof&quot;&gt;Gprof&lt;/a&gt; (GNU Binutils para Debian) 2.18.0.20080103 ejecut&amp;aacute;ndose bajo Debian Lenny de 64 bits, si eso ayuda a alguien.</target>
        </trans-unit>
        <trans-unit id="4809190e35f35121b8cef92d7260543e2698ad83" translate="yes" xml:space="preserve">
          <source>To get more info you can look in &lt;a href=&quot;https://sourceware.org/binutils/docs-2.32/gprof/&quot;&gt;https://sourceware.org/binutils/docs-2.32/gprof/&lt;/a&gt;</source>
          <target state="translated">Para obtener m&amp;aacute;s informaci&amp;oacute;n, puede consultar &lt;a href=&quot;https://sourceware.org/binutils/docs-2.32/gprof/&quot;&gt;https://sourceware.org/binutils/docs-2.32/gprof/&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9d1e75d21749779f1783a936e0318ab574c9bb06" translate="yes" xml:space="preserve">
          <source>Use &lt;code&gt;-pg&lt;/code&gt; flag when compiling and linking the code and run the executable file. While this program is executed, profiling data is collected in the file a.out.</source>
          <target state="translated">Use el indicador &lt;code&gt;-pg&lt;/code&gt; cuando compile y vincule el c&amp;oacute;digo y ejecute el archivo ejecutable. Mientras se ejecuta este programa, los datos de creaci&amp;oacute;n de perfiles se recopilan en el archivo a.out.</target>
        </trans-unit>
        <trans-unit id="74fa418da05131589c58de20bc5cd8a74884137e" translate="yes" xml:space="preserve">
          <source>Use a profiler in DEBUG mode to identify questionable parts of your code</source>
          <target state="translated">Use un perfil en modo DEBUG para identificar partes cuestionables de su código</target>
        </trans-unit>
        <trans-unit id="5abe91df474f47b2ce52602a8d2c9a1a1dc9bcdb" translate="yes" xml:space="preserve">
          <source>Use a profiler in RELEASE mode to identify questionable parts of your code.</source>
          <target state="translated">Use un perfil en modo LIBERACIÓN para identificar partes cuestionables de su código.</target>
        </trans-unit>
        <trans-unit id="47e2b39318441908a26389043c5f493a08f3da25" translate="yes" xml:space="preserve">
          <source>Uses time sampling, I/O and CPU bottlenecks are revealed.</source>
          <target state="translated">Utiliza el muestreo de tiempo,se revelan los cuellos de botella de IO y CPU.</target>
        </trans-unit>
        <trans-unit id="32ebf48279743cab72d7ff1b43c30c357fa0b5af" translate="yes" xml:space="preserve">
          <source>View gprof output in kcachegrind</source>
          <target state="translated">Ver la salida de gprof en kcachegrind</target>
        </trans-unit>
        <trans-unit id="90e10e752882a92276f4abc5b712de714bd30e0f" translate="yes" xml:space="preserve">
          <source>We can observe that file graphically with &lt;code&gt;gprof2dot&lt;/code&gt; as asked at: &lt;a href=&quot;https://stackoverflow.com/questions/2439060/is-it-possible-to-get-a-graphical-representation-of-gprof-results&quot;&gt;Is it possible to get a graphical representation of gprof results?&lt;/a&gt;</source>
          <target state="translated">Podemos observar ese archivo gr&amp;aacute;ficamente con &lt;code&gt;gprof2dot&lt;/code&gt; como se pregunta en: &lt;a href=&quot;https://stackoverflow.com/questions/2439060/is-it-possible-to-get-a-graphical-representation-of-gprof-results&quot;&gt;&amp;iquest;Es posible obtener una representaci&amp;oacute;n gr&amp;aacute;fica de los resultados de gprof?&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="10445757607ecf8a1d5924c1e702bf14a1ff75d2" translate="yes" xml:space="preserve">
          <source>We observe the following for the &lt;code&gt;-O0&lt;/code&gt; run:</source>
          <target state="translated">Observamos lo siguiente para la ejecuci&amp;oacute;n &lt;code&gt;-O0&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="9844986bd80729d071e13db938133404d11ba09f" translate="yes" xml:space="preserve">
          <source>Wherever in you code you can use :</source>
          <target state="translated">En cualquier parte de tu código puedes usar..:</target>
        </trans-unit>
        <trans-unit id="674dfbcd4d095f0cd2550aa16069c396575c13ec" translate="yes" xml:space="preserve">
          <source>Which is the best replacement for KProf?</source>
          <target state="translated">¿Cuál es el mejor reemplazo para KProf?</target>
        </trans-unit>
        <trans-unit id="9f5da50867e51ac3a771fd9c2dea086874d61262" translate="yes" xml:space="preserve">
          <source>Yet another way to look at it is called the &lt;a href=&quot;http://en.wikipedia.org/wiki/Rule_of_succession&quot;&gt;Rule Of Succession&lt;/a&gt;.
If you flip a coin 2 times, and it comes up heads both times, what does that tell you about the probable weighting of the coin?
The respected way to answer is to say that it's a Beta distribution, with average value (number of hits + 1) / (number of tries + 2) = (2+1)/(2+2) = 75%.</source>
          <target state="translated">Otra forma de verlo se llama la &lt;a href=&quot;http://en.wikipedia.org/wiki/Rule_of_succession&quot;&gt;Regla de Sucesi&amp;oacute;n&lt;/a&gt; . Si lanzas una moneda 2 veces, y sale cara en ambas ocasiones, &amp;iquest;qu&amp;eacute; te dice eso sobre el probable peso de la moneda? La forma respetada de responder es decir que es una distribuci&amp;oacute;n Beta, con un valor promedio (n&amp;uacute;mero de aciertos + 1) / (n&amp;uacute;mero de intentos + 2) = (2 + 1) / (2 + 2) = 75%.</target>
        </trans-unit>
        <trans-unit id="e477d44b67435b4f2a51bec584b8c8b5c70e29d8" translate="yes" xml:space="preserve">
          <source>You also define a few functions in &lt;code&gt;toolname.hpp&lt;/code&gt; :</source>
          <target state="translated">Tambi&amp;eacute;n define algunas funciones en &lt;code&gt;toolname.hpp&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="d06ead62c5754d4f55d6c94097738323320c25d0" translate="yes" xml:space="preserve">
          <source>You can however use the color map to mitigate those problems a bit. For example, on the previous huge image, I finally managed to find the critical path on the left when I made the brilliant deduction that green comes after red, followed finally by darker and darker blue.</source>
          <target state="translated">Sin embargo,puedes usar el mapa de colores para mitigar un poco esos problemas.Por ejemplo,en la enorme imagen anterior,finalmente logré encontrar el camino crítico a la izquierda cuando hice la brillante deducción de que el verde viene después del rojo,seguido finalmente por el azul más y más oscuro.</target>
        </trans-unit>
        <trans-unit id="ff319bc3545afd5e52b97cb66321e2f3c78b6c30" translate="yes" xml:space="preserve">
          <source>You can use &lt;a href=&quot;http://en.wikipedia.org/wiki/Valgrind&quot;&gt;Valgrind&lt;/a&gt; with the following options</source>
          <target state="translated">Puedes usar &lt;a href=&quot;http://en.wikipedia.org/wiki/Valgrind&quot;&gt;Valgrind&lt;/a&gt; con las siguientes opciones</target>
        </trans-unit>
        <trans-unit id="eff4537042a246b1bfc5a7ac9053825ab352cfb3" translate="yes" xml:space="preserve">
          <source>You can use a logging framework like &lt;a href=&quot;https://github.com/emilk/loguru&quot;&gt;&lt;code&gt;loguru&lt;/code&gt;&lt;/a&gt; since it includes timestamps and total uptime which can be used nicely for profiling:</source>
          <target state="translated">Puede usar un marco de registro como &lt;a href=&quot;https://github.com/emilk/loguru&quot;&gt; &lt;code&gt;loguru&lt;/code&gt; &lt;/a&gt; ya que incluye marcas de tiempo y tiempo de actividad total que se puede usar muy bien para crear perfiles:</target>
        </trans-unit>
        <trans-unit id="57bb7c327c284cab00d7dbfef1ffa24df1be7086" translate="yes" xml:space="preserve">
          <source>You can use the iprof library:</source>
          <target state="translated">Puedes usar la biblioteca iprof:</target>
        </trans-unit>
        <trans-unit id="09eee30b7f6c8d8c09d32bfa040a796ee7c00fe5" translate="yes" xml:space="preserve">
          <source>You customize the amount of events generated to focus solely on what you desire. It helped us a lot for scheduling issues while consuming the amount of CPU we wanted based on the amount of logged events per second.</source>
          <target state="translated">Personaliza la cantidad de eventos generados para centrarse únicamente en lo que desea.Nos ayudó mucho para los problemas de programación mientras consumíamos la cantidad de CPU que queríamos en base a la cantidad de eventos registrados por segundo.</target>
        </trans-unit>
        <trans-unit id="5885a798125a50436ce410ba3e52f4fbdf4173ce" translate="yes" xml:space="preserve">
          <source>You may have multiple performance problems of different sizes. If you clean out any one of them, the remaining ones will take a larger percentage, and be easier to spot, on subsequent passes.
This &lt;em&gt;magnification effect&lt;/em&gt;, when compounded over multiple problems, can lead to truly massive speedup factors.</source>
          <target state="translated">Puede tener m&amp;uacute;ltiples problemas de rendimiento de diferentes tama&amp;ntilde;os. Si limpia alguno de ellos, los restantes tomar&amp;aacute;n un porcentaje mayor y ser&amp;aacute;n m&amp;aacute;s f&amp;aacute;ciles de detectar en los pases posteriores. Este &lt;em&gt;efecto de aumento&lt;/em&gt; , cuando se combina con m&amp;uacute;ltiples problemas, puede conducir a factores de aceleraci&amp;oacute;n realmente masivos.</target>
        </trans-unit>
        <trans-unit id="016b297a8bba32365f38f766a39676ea6cbcfd2b" translate="yes" xml:space="preserve">
          <source>You need 3 files :</source>
          <target state="translated">Necesitas 3 archivos:</target>
        </trans-unit>
        <trans-unit id="747c3ce81ca5b275ec245943d96de7239bf6b766" translate="yes" xml:space="preserve">
          <source>You retrieve the so-called large buffer with all the data and a small interface parses it and shows events with name (up/down + value) like an oscilloscope does with colors (configured in &lt;code&gt;.hpp&lt;/code&gt; file).</source>
          <target state="translated">Recupera el llamado b&amp;uacute;fer grande con todos los datos y una peque&amp;ntilde;a interfaz lo analiza y muestra eventos con nombre (arriba / abajo + valor) como lo hace un osciloscopio con colores (configurado en &lt;code&gt;.hpp&lt;/code&gt; archivo .hpp ).</target>
        </trans-unit>
        <trans-unit id="2922e093bcf4e627dba20bdf0f73a9644a37a709" translate="yes" xml:space="preserve">
          <source>and for &lt;code&gt;-O3&lt;/code&gt;:</source>
          <target state="translated">y para &lt;code&gt;-O3&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="2a5be5a09792a69cac7d084de41932a15ba3728f" translate="yes" xml:space="preserve">
          <source>and for the &lt;code&gt;-O3&lt;/code&gt; run:</source>
          <target state="translated">y para la ejecuci&amp;oacute;n &lt;code&gt;-O3&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="faa969782037f341d207ee39f51209d6317fc20e" translate="yes" xml:space="preserve">
          <source>and the program does &lt;code&gt;O(n^2)&lt;/code&gt; loops in total. &lt;code&gt;seed&lt;/code&gt; is just to get different output without affecting runtime.</source>
          <target state="translated">y el programa realiza &lt;code&gt;O(n^2)&lt;/code&gt; bucles en total. &lt;code&gt;seed&lt;/code&gt; es solo para obtener una salida diferente sin afectar el tiempo de ejecuci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="ce2455b67d76414f51f9fe06a8ce80817a3d7dbb" translate="yes" xml:space="preserve">
          <source>are there locks that are proving to be bottle necks ?</source>
          <target state="translated">¿hay cerraduras que están demostrando ser cuellos de botella?</target>
        </trans-unit>
        <trans-unit id="a6f071b118d3c8c52f2a76d5173897fc74f14d94" translate="yes" xml:space="preserve">
          <source>both &lt;code&gt;fast&lt;/code&gt; and &lt;code&gt;maybe_slow&lt;/code&gt; call &lt;code&gt;common&lt;/code&gt;, which accounts for the bulk of the program execution</source>
          <target state="translated">llamada &lt;code&gt;fast&lt;/code&gt; y &lt;code&gt;maybe_slow&lt;/code&gt; vez lenta lenta &lt;code&gt;common&lt;/code&gt; , que representa la mayor parte de la ejecuci&amp;oacute;n del programa</target>
        </trans-unit>
        <trans-unit id="9684498d10b2a6f3e91e84082350f59fe31df044" translate="yes" xml:space="preserve">
          <source>but even then, you will be dragging the image around a lot to find what you want, see e.g. this image from a &quot;real&quot; software example taken from &lt;a href=&quot;https://gem5.atlassian.net/browse/GEM5-337&quot;&gt;this ticket&lt;/a&gt;:</source>
          <target state="translated">pero incluso as&amp;iacute;, arrastrar&amp;aacute; la imagen mucho para encontrar lo que desea, vea, por ejemplo, esta imagen de un ejemplo de software &quot;real&quot; tomado de &lt;a href=&quot;https://gem5.atlassian.net/browse/GEM5-337&quot;&gt;este ticket&lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="3af4c4be531f706f742d60135390bd70be4af5f9" translate="yes" xml:space="preserve">
          <source>but in such a simple program the output is not very easy to understand, since we can't easily see neither &lt;code&gt;maybe_slow&lt;/code&gt; nor &lt;code&gt;fast&lt;/code&gt; on that graph:</source>
          <target state="translated">pero en un programa tan simple, la salida no es muy f&amp;aacute;cil de entender, ya que no podemos ver ni &lt;code&gt;maybe_slow&lt;/code&gt; ni &lt;code&gt;fast&lt;/code&gt; en ese gr&amp;aacute;fico:</target>
        </trans-unit>
        <trans-unit id="a7dd0b976062c449381eb00b3b4071e2c243cb98" translate="yes" xml:space="preserve">
          <source>by running the command &lt;code&gt;gprog --flat-profile a.out&lt;/code&gt; you got the following data</source>
          <target state="translated">ejecutando el comando &lt;code&gt;gprog --flat-profile a.out&lt;/code&gt; obtuviste los siguientes datos</target>
        </trans-unit>
        <trans-unit id="4ad25e6079270f854bc1ab4b0f5c15dda777da63" translate="yes" xml:space="preserve">
          <source>callgrind is the valgrind's tool to profile code and kcachegrind is a KDE program that can visualize cachegrind output.</source>
          <target state="translated">callgrind es la herramienta de valgrind para perfilar el código y kcachegrind es un programa de KDE que puede visualizar la salida de cachegrind.</target>
        </trans-unit>
        <trans-unit id="fdde301a4277b76ba71eeffc0b565064e8e3eb36" translate="yes" xml:space="preserve">
          <source>centers around the function that is left indented (&lt;code&gt;maybe_flow&lt;/code&gt;). &lt;code&gt;[3]&lt;/code&gt; is the ID of that function. Above the function, are its callers, and below it the callees.</source>
          <target state="translated">se centra alrededor de la funci&amp;oacute;n que queda sangrada ( &lt;code&gt;maybe_flow&lt;/code&gt; ). &lt;code&gt;[3]&lt;/code&gt; es la ID de esa funci&amp;oacute;n. Encima de la funci&amp;oacute;n, est&amp;aacute;n sus llamadores, y debajo de ella los callees.</target>
        </trans-unit>
        <trans-unit id="c86da089418268adb10c8acdbbba40db282822e6" translate="yes" xml:space="preserve">
          <source>generates callgrind.out.x. Read it using kcachegrind.</source>
          <target state="translated">genera callgrind.out.x.Léelo usando kcachegrind.</target>
        </trans-unit>
        <trans-unit id="b03467560afe04df21ced900b226b5f9dadcb62d" translate="yes" xml:space="preserve">
          <source>gprof is built-into GCC/binutils, so all we have to do is to compile with the &lt;code&gt;-pg&lt;/code&gt; option to enable gprof. We then run the program normally with a size CLI parameter that produces a run of reasonable duration of a few seconds (&lt;code&gt;10000&lt;/code&gt;):</source>
          <target state="translated">gprof est&amp;aacute; integrado en GCC / binutils, por lo que todo lo que tenemos que hacer es compilar con la opci&amp;oacute;n &lt;code&gt;-pg&lt;/code&gt; para habilitar gprof. Luego ejecutamos el programa normalmente con un par&amp;aacute;metro CLI de tama&amp;ntilde;o que produce una ejecuci&amp;oacute;n de duraci&amp;oacute;n razonable de unos pocos segundos ( &lt;code&gt;10000&lt;/code&gt; ):</target>
        </trans-unit>
        <trans-unit id="9cb1273641689838d08c035718b933b581323243" translate="yes" xml:space="preserve">
          <source>gprof requires recompiling the software with instrumentation, and it also uses a sampling approach together with that instrumentation. It therefore strikes a balance between accuracy (sampling is not always fully accurate and can skip functions) and execution slowdown (instrumentation and sampling are relatively fast techniques that don't slow down execution very much).</source>
          <target state="translated">El gprof requiere la recompilación del software con instrumentación,y también utiliza un enfoque de muestreo junto con esa instrumentación.Por lo tanto,logra un equilibrio entre la exactitud (el muestreo no siempre es totalmente exacto y puede omitir funciones)y la ralentización de la ejecución (la instrumentación y el muestreo son técnicas relativamente rápidas que no ralentizan mucho la ejecución).</target>
        </trans-unit>
        <trans-unit id="6c20dbe559048f4187157de781dccf6f5ba1544e" translate="yes" xml:space="preserve">
          <source>how about IO, handled and optimized ?</source>
          <target state="translated">¿Qué tal IO,manejado y optimizado?</target>
        </trans-unit>
        <trans-unit id="8ad78a2d8b81d1f48b603777d625c316ec818e1d" translate="yes" xml:space="preserve">
          <source>is my algorithm correct ?</source>
          <target state="translated">¿Es mi algoritmo correcto?</target>
        </trans-unit>
        <trans-unit id="3a13f69d29ec474c66705e3aaadaa6bf50574a2c" translate="yes" xml:space="preserve">
          <source>is there a specific section of code that's proving to be a culprit ?</source>
          <target state="translated">¿hay una sección específica del código que está resultando ser un culpable?</target>
        </trans-unit>
        <trans-unit id="406e031b8824ea26ae0bf4d7579a1d89e3fb5906" translate="yes" xml:space="preserve">
          <source>main.c</source>
          <target state="translated">main.c</target>
        </trans-unit>
        <trans-unit id="d72ae9adcbb77a799278937f8a9954bf7d5fee38" translate="yes" xml:space="preserve">
          <source>they don't summarize at the instruction level, and</source>
          <target state="translated">no se resumen en el nivel de instrucción,y</target>
        </trans-unit>
        <trans-unit id="813d13060cfdfe274f9dedbcdaad7e87361c8245" translate="yes" xml:space="preserve">
          <source>they give confusing summaries in the presence of recursion.</source>
          <target state="translated">dan resúmenes confusos en presencia de recurrencia.</target>
        </trans-unit>
        <trans-unit id="b38e2cc31a913ba13d49dd56d1a341b5650300c3" translate="yes" xml:space="preserve">
          <source>us the command &lt;code&gt;gprof --graph a.out&lt;/code&gt; to get the following data for each function which includes</source>
          <target state="translated">&lt;code&gt;gprof --graph a.out&lt;/code&gt; el comando gprof --graph a.out para obtener los siguientes datos para cada funci&amp;oacute;n que incluye</target>
        </trans-unit>
        <trans-unit id="5e1acf1cde93e84e5121533952a55ce3fcfa7a9a" translate="yes" xml:space="preserve">
          <source>valgrind runs the program through the valgrind virtual machine. This makes the profiling very accurate, but it also produces a very large slowdown of the program. I have also mentioned kcachegrind previously at: &lt;a href=&quot;https://stackoverflow.com/questions/517589/tools-to-get-a-pictorial-function-call-graph-of-code/31190167#31190167&quot;&gt;Tools to get a pictorial function call graph of code&lt;/a&gt;</source>
          <target state="translated">valgrind ejecuta el programa a trav&amp;eacute;s de la m&amp;aacute;quina virtual valgrind. Esto hace que el perfil sea muy preciso, pero tambi&amp;eacute;n produce una gran desaceleraci&amp;oacute;n del programa. Tambi&amp;eacute;n he mencionado kcachegrind anteriormente en: &lt;a href=&quot;https://stackoverflow.com/questions/517589/tools-to-get-a-pictorial-function-call-graph-of-code/31190167#31190167&quot;&gt;Herramientas para obtener una funci&amp;oacute;n gr&amp;aacute;fica llamada gr&amp;aacute;fico de c&amp;oacute;digo&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5572540656d11e87dd5bd966d28d6665bd7fe5af" translate="yes" xml:space="preserve">
          <source>which gives as a familiar call graph like other tools, but with the clunky unit of number of samples rather than seconds.</source>
          <target state="translated">que da como una gráfica de llamada familiar como otras herramientas,pero con la unidad torpe de número de muestras en lugar de segundos.</target>
        </trans-unit>
        <trans-unit id="88dc6e662ab6827a52e6b9e9027101485ddfb787" translate="yes" xml:space="preserve">
          <source>which gives:</source>
          <target state="translated">que da:</target>
        </trans-unit>
        <trans-unit id="6e4de60efe1156a81e465d013cb2669ea775fcd4" translate="yes" xml:space="preserve">
          <source>which shows a GUI that contains data similar to the textual gprof output:</source>
          <target state="translated">que muestra una interfaz gráfica de usuario que contiene datos similares a la salida textual de gprof:</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
