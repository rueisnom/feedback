<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="https://stackoverflow.com/questions/1711631">
    <body>
      <group id="1711631">
        <trans-unit id="cdefc7d8d0f502ac2649cb2130333beb8e549c0c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;SQLITE_STATIC&lt;/code&gt; tells it that the memory address you gave it will be valid until the query has been performed (which in this loop is always the case). This will save you several allocate, copy and deallocate operations per loop. Possibly a large improvement.</source>
          <target state="translated">&lt;code&gt;SQLITE_STATIC&lt;/code&gt; 告诉您，您提供给它的内存地址在执行查询之前将一直有效（在此循环中始终如此）。 这样可以为每个循环节省几个分配，复制和取消分配操作。 可能会有很大的改善。</target>
        </trans-unit>
        <trans-unit id="dc841631c7f8778b9620c5a6aebf79da39172daf" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;SQLITE_TRANSIENT&lt;/code&gt; will cause SQLite to copy the string data before returning.</source>
          <target state="translated">&lt;code&gt;SQLITE_TRANSIENT&lt;/code&gt; 将导致SQLite在返回之前复制字符串数据。</target>
        </trans-unit>
        <trans-unit id="8e0bb93e0bdc0ef4869a544f1ec90a67170531be" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;I hope you're still with me!&lt;/em&gt; The reason we started down this road is that bulk-insert performance varies so wildly with SQLite, and it's not always obvious what changes need to be made to speed-up our operation. Using the same compiler (and compiler options), the same version of SQLite and the same data we've optimized our code and our usage of SQLite to go &lt;strong&gt;from a worst-case scenario of 85 inserts per second to over 96,000 inserts per second!&lt;/strong&gt;</source>
          <target state="translated">&lt;em&gt;我希望你仍然和我在一起！&lt;/em&gt; 我们开始这条路的原因是，使用SQLite进行大容量插入的性能变化如此之大，并不一定总是需要进行哪些更改以加快操作速度。 使用相同的编译器（和编译器选项），相同版本的SQLite和相同数据，我们优化了代码，并优化了SQLite的使用，使其&lt;strong&gt;从最坏的情况下每秒85次插入变为每秒超过96,000次插入！&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="651a2b8d1ad8078419cf895a15b4c0df27a8c401" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;Let's write some code!&lt;/em&gt;</source>
          <target state="translated">&lt;em&gt;让我们写一些代码！&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="b76b98300a521ccc66355329ebd7994a2475558c" translate="yes" xml:space="preserve">
          <source>&lt;s&gt;&lt;a href=&quot;https://github.com/rdpoor/CreateOrUpdate&quot;&gt;https://github.com/rdpoor/CreateOrUpdate&lt;/a&gt;&lt;/s&gt;</source>
          <target state="translated">&lt;s&gt;&lt;a href=&quot;https://github.com/rdpoor/CreateOrUpdate&quot;&gt;https://github.com/rdpoor/CreateOrUpdate&lt;/a&gt;&lt;/s&gt;</target>
        </trans-unit>
        <trans-unit id="7ff38676434e7065905a1e026ab3bdabb754fafc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Avoid &lt;a href=&quot;https://www.sqlite.org/c3ref/clear_bindings.html&quot;&gt;&lt;code&gt;sqlite3_clear_bindings(stmt)&lt;/code&gt;&lt;/a&gt;.&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;避免使用&lt;a href=&quot;https://www.sqlite.org/c3ref/clear_bindings.html&quot;&gt; &lt;code&gt;sqlite3_clear_bindings(stmt)&lt;/code&gt; &lt;/a&gt; 。&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="b52454c570dc6b7585dff89d6bafeb4102012edb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Background:&lt;/strong&gt; We are using SQLite as part of a desktop application. We have large amounts of configuration data stored in XML files that are parsed and loaded into an SQLite database for further processing when the application is initialized. SQLite is ideal for this situation because it's fast, it requires no specialized configuration, and the database is stored on disk as a single file.</source>
          <target state="translated">&lt;strong&gt;背景：&lt;/strong&gt;我们正在将SQLite用作桌面应用程序的一部分。 我们将大量配置数据存储在XML文件中，这些文件会被解析并加载到SQLite数据库中，以便在初始化应用程序时进行进一步处理。 SQLite非常适合这种情况，因为它速度快，不需要专门的配置，并且数据库作为单个文件存储在磁盘上。</target>
        </trans-unit>
        <trans-unit id="270000b21d15ce5fe27f22341bb7e2a04b3d7f99" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Create Index then Insert Data&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;创建索引然后插入数据&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="c801cbf9bc118950e73a5431424c376f22274ca3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Insert Data then Create Index&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;插入数据然后创建索引&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="dbe1b95758ace93ba6c358fe7b1aa18db2be2a7d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Note: We are back to using a real database file. In-memory databases are fast, but not necessarily practical&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;注意：我们将回到使用真实的数据库文件。&lt;/strong&gt; &lt;strong&gt;内存数据库速度很快，但不一定实用&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="26ea07b172d510db473fb482ce4002bc7313046a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Rationale:&lt;/strong&gt;&lt;em&gt;Initially I was disappointed with the performance I was seeing.&lt;/em&gt; It turns-out that the performance of SQLite can vary significantly (both for bulk-inserts and selects) depending on how the database is configured and how you're using the API. It was not a trivial matter to figure out what all of the options and techniques were, so I thought it prudent to create this community wiki entry to share the results with Stack&amp;nbsp;Overflow readers in order to save others the trouble of the same investigations.</source>
          <target state="translated">&lt;strong&gt;基本原理：&lt;/strong&gt; &lt;em&gt;最初，我对看到的性能感到失望。&lt;/em&gt; 事实证明，取决于数据库的配置方式和使用API​​的方式，SQLite的性能可能会发生很大的变化（对于批量插入和选择）。 弄清楚所有选项和技术是什么都不是一件容易的事，因此，我认为创建此社区Wiki条目与Stack Overflow阅读器共享结果以节省其他人的麻烦是审慎的做法。</target>
        </trans-unit>
        <trans-unit id="a76bacf9fc05e4ab73ba39928377f27b7df97668" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;The Code:&lt;/strong&gt; A simple C program that reads the text file line-by-line, splits the string into values and then inserts the data into an SQLite database. In this &quot;baseline&quot; version of the code, the database is created, but we won't actually insert data:</source>
          <target state="translated">&lt;strong&gt;代码：&lt;/strong&gt;一个简单的C程序，它逐行读取文本文件，将字符串拆分为值，然后将数据插入SQLite数据库。 在此&amp;ldquo;基准&amp;rdquo;版本的代码中，创建了数据库，但实际上不会插入数据：</target>
        </trans-unit>
        <trans-unit id="2dfc837caac3acc0839eec2ba05612c43327300d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;The Experiment:&lt;/strong&gt; Rather than simply talking about performance tips in the general sense (i.e. &lt;em&gt;&quot;Use a transaction!&quot;&lt;/em&gt;), I thought it best to write some C code and &lt;em&gt;actually measure&lt;/em&gt; the impact of various options. We're going to start with some simple data:</source>
          <target state="translated">&lt;strong&gt;实验：&lt;/strong&gt;我认为，最好是编写一些C代码并&lt;em&gt;实际衡量&lt;/em&gt;各种选择的影响，而不是简单地谈论一般意义上的性能提示（即&lt;em&gt;&amp;ldquo;使用事务！&amp;rdquo;&lt;/em&gt; ）。 我们将从一些简单的数据开始：</target>
        </trans-unit>
        <trans-unit id="28ff78fa5b07af5d0e3d7492a98d06fe91ed622f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;UPDATE&lt;/strong&gt;:</source>
          <target state="translated">&lt;strong&gt;UPDATE&lt;/strong&gt;:</target>
        </trans-unit>
        <trans-unit id="d11a8e1d536d373238c3435a955657ece24fd305" translate="yes" xml:space="preserve">
          <source>A 28 MB TAB-delimited text file (approximately 865,000 records) of the &lt;a href=&quot;http://www.toronto.ca/open/datasets/ttc-routes&quot;&gt;complete transit schedule for the city of Toronto&lt;/a&gt;</source>
          <target state="translated">28 MB TAB分隔的文本文件（约865,000条记录）， &lt;a href=&quot;http://www.toronto.ca/open/datasets/ttc-routes&quot;&gt;用于多伦多市&lt;/a&gt;的完整运输时间表</target>
        </trans-unit>
        <trans-unit id="7a43bd3a7ef7eab400ead152ddcab810fb34cbd0" translate="yes" xml:space="preserve">
          <source>A little slower than the previous optimization at &lt;strong&gt;64,000 inserts per second.&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;每秒64,000次插入的&lt;/strong&gt;速度比之前的优化稍慢&lt;strong&gt;。&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="9f763d7016e134b7f26a749db3a5f6f6a28b8a9b" translate="yes" xml:space="preserve">
          <source>A slight refactoring to the string processing code used in our parameter binding has allowed us to perform &lt;strong&gt;96,700 inserts per second.&lt;/strong&gt; I think it's safe to say that this is &lt;em&gt;plenty fast&lt;/em&gt;. As we start to tweak other variables (i.e. page size, index creation, etc.) this will be our benchmark.</source>
          <target state="translated">稍微重构参数绑定中使用的字符串处理代码，就可以&lt;strong&gt;每秒&lt;/strong&gt;执行&lt;strong&gt;96,700次插入。&lt;/strong&gt; 我认为可以肯定地说这非常&lt;em&gt;快&lt;/em&gt; 。 随着我们开始调整其他变量（例如页面大小，索引创建等），这将成为我们的基准。</target>
        </trans-unit>
        <trans-unit id="8df6f713ac0e07c6021d93f5e0c15401d3336b1d" translate="yes" xml:space="preserve">
          <source>After reading this tutorial, I tried to implement it to my program.</source>
          <target state="translated">看完这个教程后,我试着把它实施到我的程序中。</target>
        </trans-unit>
        <trans-unit id="ea6e032fcc1739cdfb3de8cd3a6c6dfc44b90167" translate="yes" xml:space="preserve">
          <source>After two weeks of research and checking multiple resources: Hard Drive, Ram, Cache, I found out that some settings on your hard drive can affect the I/O rate. By clicking properties on your desired output drive you can see two options in the general tab. Opt1: Compress this drive, Opt2: Allow files of this drive to have contents indexed.</source>
          <target state="translated">经过两个星期的研究和检查多个资源,我发现。硬盘、内存、缓存,我发现你的硬盘上的一些设置会影响到IO率。通过点击你想要的输出硬盘的属性,可以在常规选项卡中看到两个选项。选项1:压缩这个硬盘,选项2:压缩这个硬盘。压缩这个驱动器,Opt2:允许该硬盘的文件内容被索引。</target>
        </trans-unit>
        <trans-unit id="123802917e1e1b5959658891a69eb20a10c75487" translate="yes" xml:space="preserve">
          <source>Also for us, SHAREDCACHE made the performance slower, so I manually put PRIVATECACHE (cause it was enabled globally for us)</source>
          <target state="translated">另外,对我们来说,SHAREDCACHE使性能变慢,所以我手动把PRIVATECACHE(因为它对我们来说是全局启用的)。</target>
        </trans-unit>
        <trans-unit id="4583ff4050220eaa4631bc21a580cf8fe41b8f3b" translate="yes" xml:space="preserve">
          <source>Although not specifically an SQLite improvement, I don't like the extra &lt;code&gt;char*&lt;/code&gt; assignment operations in the &lt;code&gt;while&lt;/code&gt; loop. Let's quickly refactor that code to pass the output of &lt;code&gt;strtok()&lt;/code&gt; directly into &lt;code&gt;sqlite3_bind_text()&lt;/code&gt;, and let the compiler try to speed things up for us:</source>
          <target state="translated">尽管没有特别改进SQLite，但我不喜欢 &lt;code&gt;while&lt;/code&gt; 循环中额外的 &lt;code&gt;char*&lt;/code&gt; 赋值操作。 让我们快速重构该代码，以将 &lt;code&gt;strtok()&lt;/code&gt; 的输出直接传递到 &lt;code&gt;sqlite3_bind_text()&lt;/code&gt; ，让编译器尝试为我们加快速度：</target>
        </trans-unit>
        <trans-unit id="26e259678078857d966992a8f6abac77d4d0cd86" translate="yes" xml:space="preserve">
          <source>As expected, bulk-inserts are slower if one column is indexed, but it does make a difference if the index is created after the data is inserted. Our no-index baseline is 96,000 inserts per second. &lt;strong&gt;Creating the index first then inserting data gives us 47,700 inserts per second, whereas inserting the data first then creating the index gives us 63,300 inserts per second.&lt;/strong&gt;</source>
          <target state="translated">不出所料，如果对一列进行索引，则大容量插入会变慢，但是如果在插入数据后创建索引，则确实会有所不同。 我们的无索引基准是每秒96,000次插入。 &lt;strong&gt;首先创建索引，然后插入数据，每秒可提供47,700次插入，而先创建数据，然后创建索引，则可提供每秒63,300次插入。&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="087e0edf98958c039e2818a7fb1853f6115bf21c" translate="yes" xml:space="preserve">
          <source>Before we start measuring &lt;code&gt;SELECT&lt;/code&gt; performance, we know that we'll be creating indices. It's been suggested in one of the answers below that when doing bulk inserts, it is faster to create the index after the data has been inserted (as opposed to creating the index first then inserting the data). Let's try:</source>
          <target state="translated">在开始评估 &lt;code&gt;SELECT&lt;/code&gt; 性能之前，我们知道我们将创建索引。 在以下答案之一中，建议进行批量插入时，插入数据后创建索引的速度更快（与先创建索引然后插入数据相反）。 咱们试试吧：</target>
        </trans-unit>
        <trans-unit id="872e67d1abe340cdbf86668e654d983f7d5c9e25" translate="yes" xml:space="preserve">
          <source>Bulk imports seems to perform best if you can chunk your &lt;strong&gt;INSERT/UPDATE&lt;/strong&gt; statements.  A value of 10,000 or so has worked well for me on a table with only a few rows, YMMV...</source>
          <target state="translated">如果可以对&lt;strong&gt;INSERT / UPDATE&lt;/strong&gt;语句进行分块，则批量导入似乎表现最佳。 在只有几行的表YMMV上，大约10,000的值对我来说效果很好。</target>
        </trans-unit>
        <trans-unit id="671341a42a7c965624dd8daf9888d2a9ef366288" translate="yes" xml:space="preserve">
          <source>By default, SQLite will evaluate every INSERT / UPDATE statement within a unique transaction. If performing a large number of inserts, it's advisable to wrap your operation in a transaction:</source>
          <target state="translated">默认情况下,SQLite会在一个唯一的事务中评估每个INSERT UPDATE语句。如果执行大量的插入操作,建议在事务中封装你的操作。</target>
        </trans-unit>
        <trans-unit id="bbd325acdd91b4ca873182b6a019ce434225ae43" translate="yes" xml:space="preserve">
          <source>By default, SQLite will pause after issuing a OS-level write command. This guarantees that the data is written to the disk. By setting &lt;code&gt;synchronous = OFF&lt;/code&gt;, we are instructing SQLite to simply hand-off the data to the OS for writing and then continue. There's a chance that the database file may become corrupted if the computer suffers a catastrophic crash (or power failure) before the data is written to the platter:</source>
          <target state="translated">默认情况下，SQLite将在发出OS级写命令后暂停。 这样可以确保将数据写入磁盘。 通过设置 &lt;code&gt;synchronous = OFF&lt;/code&gt; ，我们指示SQLite只需将数据移交给OS进行写入，然后继续。 如果计算机在将数据写入磁盘之前遭受灾难性崩溃（或电源故障），则数据库文件可能会损坏：</target>
        </trans-unit>
        <trans-unit id="9021172ebb7b79e513f34761501efc24dc16188c" translate="yes" xml:space="preserve">
          <source>By disabling these two options all 3 PCs now take approximately the same time to finish (1hr and 20 to 40min). If you encounter slow inserts check whether your hard drive is configured with these options. It will save you lots of time and headaches trying to find the solution</source>
          <target state="translated">通过禁用这两个选项,现在所有3台电脑的完成时间大致相同(1小时和20到40分钟)。如果你遇到插入速度慢,请检查你的硬盘是否配置了这些选项。这将为您节省大量的时间和寻找解决方案的麻烦。</target>
        </trans-unit>
        <trans-unit id="b433a2b1f636acc88cf161d1b849771010a891d6" translate="yes" xml:space="preserve">
          <source>CREATE INDEX then INSERT vs. INSERT then CREATE INDEX</source>
          <target state="translated">CREATE INDEX then INSERT vs.INSERT then CREATE INDEX</target>
        </trans-unit>
        <trans-unit id="3a324f600d2cc2caa3c71c2e0bb04baf34e8a893" translate="yes" xml:space="preserve">
          <source>Call bulkInsert method :</source>
          <target state="translated">调用 bulkInsert方法。</target>
        </trans-unit>
        <trans-unit id="bcdd9e61da1685f26f0ee4233e52764e50d2bfe3" translate="yes" xml:space="preserve">
          <source>Consider storing the rollback journal in memory by evaluating &lt;code&gt;PRAGMA journal_mode = MEMORY&lt;/code&gt;. Your transaction will be faster, but if you lose power or your program crashes during a transaction you database could be left in a corrupt state with a partially-completed transaction:</source>
          <target state="translated">考虑通过评估 &lt;code&gt;PRAGMA journal_mode = MEMORY&lt;/code&gt; 将回滚日志存储在内存中。 您的事务将更快，但是如果在事务期间断电或程序崩溃，则数据库可能会因部分完成的事务而处于损坏状态：</target>
        </trans-unit>
        <trans-unit id="9f72f2c529b0b1804a0aaf2ee6c8d2facf67621b" translate="yes" xml:space="preserve">
          <source>Don't use &lt;code&gt;!feof(file)&lt;/code&gt;!</source>
          <target state="translated">不要使用 &lt;code&gt;!feof(file)&lt;/code&gt; ！</target>
        </trans-unit>
        <trans-unit id="4326aa174e0a3d69e5993b7a70921bb5ff0f2f6e" translate="yes" xml:space="preserve">
          <source>Fantastic! We're able to do &lt;strong&gt;72,000 inserts per second.&lt;/strong&gt;</source>
          <target state="translated">太棒了！ 我们&lt;strong&gt;每秒&lt;/strong&gt;能够完成&lt;strong&gt;72,000次插入。&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="bfc83a3f5364bbc68fa36baf227468de56749928" translate="yes" xml:space="preserve">
          <source>First find the items, in the table:</source>
          <target state="translated">先找项目,在表格中。</target>
        </trans-unit>
        <trans-unit id="090aa308f4299a2ee89598663268aa093622fa6a" translate="yes" xml:space="preserve">
          <source>For older versions of SQLite - Consider a less paranoid journal mode (&lt;code&gt;pragma journal_mode&lt;/code&gt;). There is &lt;code&gt;NORMAL&lt;/code&gt;, and then there is &lt;code&gt;OFF&lt;/code&gt;, which can significantly increase insert speed if you're not too worried about the database possibly getting corrupted if the OS crashes. If your application crashes the data should be fine. Note that in newer versions, the &lt;code&gt;OFF/MEMORY&lt;/code&gt; settings are not safe for application level crashes.</source>
          <target state="translated">对于较旧版本的SQLite-考虑较少的偏执日志模式（ &lt;code&gt;pragma journal_mode&lt;/code&gt; ）。 有 &lt;code&gt;NORMAL&lt;/code&gt; ，然后有 &lt;code&gt;OFF&lt;/code&gt; ，如果您不太担心数据库可能因操作系统崩溃而损坏，则可以显着提高插入速度。 如果您的应用程序崩溃，数据应该没问题。 请注意，在较新的版本中， &lt;code&gt;OFF/MEMORY&lt;/code&gt; 设置对于应用程序级崩溃并不安全。</target>
        </trans-unit>
        <trans-unit id="2195eed65de850d03a01f0bdc090231f2d372a07" translate="yes" xml:space="preserve">
          <source>For our small (200mb) db this made 50-75% speed-up (3.8.0.2 64-bit on Windows 7). Our tables are heavily non-normalized (1000-1500 columns, roughly 100,000 or more rows).</source>
          <target state="translated">对于我们的小数据库(200MB)来说,这使得速度提高了50-75%(Windows 7上的3.8.0.2 64位)。我们的表是严重的非正常化(1000-1500列,大约10万或更多的行)。</target>
        </trans-unit>
        <trans-unit id="76b075177903579178f27a07d252a978c4cb462b" translate="yes" xml:space="preserve">
          <source>Great! We can do 920,000 inserts per second, provided we don't actually do any inserts :-)</source>
          <target state="translated">很好!我们可以做920,000次/秒的插件。我们可以做到92万次/秒插入,只要我们不做任何插入,就能做到92万次/秒。)</target>
        </trans-unit>
        <trans-unit id="6f22b5c109f71a3ddf56af02c88cbacffcd2a204" translate="yes" xml:space="preserve">
          <source>Here is where your suggestion fails. You use a single transaction for all the records and a single insert with no errors/fails. Let's say that you are splitting each record into multiple inserts on different tables. What happens if the record is broken?</source>
          <target state="translated">这里是你的建议失败的地方。你对所有的记录都使用了一个单一的事务,并且在没有错误的情况下进行单一的插入。假设你在不同的表上将每条记录拆分成多个插入。如果记录被破坏了,会发生什么?</target>
        </trans-unit>
        <trans-unit id="026af1bc2703948250f4b6f33ec6faec09054f5b" translate="yes" xml:space="preserve">
          <source>I coudn't get any gain from transactions until I raised cache_size to a higher value i.e.  &lt;code&gt;PRAGMA cache_size=10000;&lt;/code&gt;</source>
          <target state="translated">在将cache_size提高到更高的值之前，我无法从交易中获得任何收益，即 &lt;code&gt;PRAGMA cache_size=10000;&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="8eecbe52b4dec4bfb03e25ab1108fd0f15070e96" translate="yes" xml:space="preserve">
          <source>I have 4-5 files that contain addresses. Each file has approx 30 million records. I am using the same configuration that you are suggesting but my number of INSERTs per second is way low (~10.000 records per sec).</source>
          <target state="translated">我有4-5个包含地址的文件。每个文件大约有3000万条记录。我使用的配置和你建议的一样,但我的INSERT的数量是很低的(约10.000条记录/秒)。</target>
        </trans-unit>
        <trans-unit id="11917b0c226929497af8ba620faa1e4b102c1e64" translate="yes" xml:space="preserve">
          <source>I'd gladly take suggestions for other scenarios to try... And will be compiling similar data for SELECT queries soon.</source>
          <target state="translated">我很乐意接受其他方案的建议.....。并将在不久后编译类似的数据进行SELECT查询。</target>
        </trans-unit>
        <trans-unit id="5172e4b2cd07b7be64b179445d83376ec1008f29" translate="yes" xml:space="preserve">
          <source>I'm using it in production code where I frequently need to import large datasets, and I'm pretty happy with it.</source>
          <target state="translated">我在生产代码中使用它,经常需要导入大数据集,我对它很满意。</target>
        </trans-unit>
        <trans-unit id="3ac5e5ce6fc4a6695514c6dae964ca288c928cef" translate="yes" xml:space="preserve">
          <source>I'm using the SQLite &quot;Amalgamation&quot;, compiled directly into my test application. The SQLite version I happen to have is a bit older (3.6.7), but I suspect these results will be comparable to the latest release (please leave a comment if you think otherwise).</source>
          <target state="translated">我使用的是SQLite &quot;Amalgamation&quot;,直接编译到我的测试程序中。我所使用的SQLite版本有点旧(3.6.7),但我猜测这些结果与最新的版本相当(如果你不这么认为,请留下评论)。</target>
        </trans-unit>
        <trans-unit id="46f70793f4a5cfd620882059cb0f77361cfd03c5" translate="yes" xml:space="preserve">
          <source>I've also asked similar questions &lt;a href=&quot;https://stackoverflow.com/questions/784173/what-are-the-performance-characteristics-of-sqlite-with-very-large-database-files&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://stackoverflow.com/questions/768708/are-there-known-issues-with-using-sqlite-and-file-locking-on-different-platforms&quot;&gt;here&lt;/a&gt;.</source>
          <target state="translated">我也在&lt;a href=&quot;https://stackoverflow.com/questions/784173/what-are-the-performance-characteristics-of-sqlite-with-very-large-database-files&quot;&gt;这里&lt;/a&gt;和&lt;a href=&quot;https://stackoverflow.com/questions/768708/are-there-known-issues-with-using-sqlite-and-file-locking-on-different-platforms&quot;&gt;这里&lt;/a&gt;问过类似的问题。</target>
        </trans-unit>
        <trans-unit id="dfe5fd5527d0cb852bc797c4c3ee406f746ecc8c" translate="yes" xml:space="preserve">
          <source>If anyone has any other ideas on how to speed it up, I am open to suggestions.</source>
          <target state="translated">如果大家有什么其他的想法,我很乐意接受建议。</target>
        </trans-unit>
        <trans-unit id="7d973398e85df8aa77d57e79abb3b81143a022d7" translate="yes" xml:space="preserve">
          <source>If you are using multiple threads, you can try using the &lt;a href=&quot;http://sqlite.org/c3ref/enable_shared_cache.html&quot;&gt;shared page cache&lt;/a&gt;, which will allow loaded pages to be shared between threads, which can avoid expensive I/O calls.</source>
          <target state="translated">如果使用多个线程，则可以尝试使用&lt;a href=&quot;http://sqlite.org/c3ref/enable_shared_cache.html&quot;&gt;共享页面缓存&lt;/a&gt; ，这将允许在线程之间共享已加载的页面，从而避免了昂贵的I / O调用。</target>
        </trans-unit>
        <trans-unit id="9909caa1776cc754ea391a5613c40fbd59f9912b" translate="yes" xml:space="preserve">
          <source>If you care only about reading, somewhat faster (but might read stale data) version is to read from multiple connections from multiple threads (connection per-thread).</source>
          <target state="translated">如果你只关心读取,稍微快一点的版本是从多个线程的多个连接(每个线程的连接)读取数据。</target>
        </trans-unit>
        <trans-unit id="0b5e37f7ae64ee540105a2d6879bb33438066595" translate="yes" xml:space="preserve">
          <source>If you have indices, consider calling &lt;code&gt;CREATE INDEX&lt;/code&gt; after doing all your inserts. This is significantly faster than creating the index and then doing your inserts.</source>
          <target state="translated">如果有索引，请在完成所有插入操作后考虑调用 &lt;code&gt;CREATE INDEX&lt;/code&gt; 。 这比创建索引然后进行插入要快得多。</target>
        </trans-unit>
        <trans-unit id="050d6cf06387f361bcb1b0a00b8117a50e73b6db" translate="yes" xml:space="preserve">
          <source>Imported 864913 records in 0.94
  seconds</source>
          <target state="translated">在0.94秒内导入864913条记录</target>
        </trans-unit>
        <trans-unit id="ffb4f30cad268801a7084f86177635a7739c17eb" translate="yes" xml:space="preserve">
          <source>Imported 864913 records in 10.94
  seconds</source>
          <target state="translated">在10.94秒内导入864913条记录。</target>
        </trans-unit>
        <trans-unit id="ad66121de714d2631e9f96a8c355fd562e17c3d9" translate="yes" xml:space="preserve">
          <source>Imported 864913 records in 12.00
  seconds</source>
          <target state="translated">在12.00秒内导入864913条记录。</target>
        </trans-unit>
        <trans-unit id="7581475071859f0e2df5c5baa78a47d7b29eb343" translate="yes" xml:space="preserve">
          <source>Imported 864913 records in 12.41
  seconds</source>
          <target state="translated">输入864913条记录,用时12.41秒。</target>
        </trans-unit>
        <trans-unit id="1fb6a8f5771eb996d9c96dec9c9972b1f7a9e674" translate="yes" xml:space="preserve">
          <source>Imported 864913 records in 13.50
  seconds</source>
          <target state="translated">13.50秒内导入864913条记录。</target>
        </trans-unit>
        <trans-unit id="d736e88540c433cf8f1e9c878c95b8f56e1ea201" translate="yes" xml:space="preserve">
          <source>Imported 864913 records in 13.66
  seconds</source>
          <target state="translated">13.66秒内导入864913条记录。</target>
        </trans-unit>
        <trans-unit id="9cf20009ccbdebfe12b3df7ef0636a31ba57d3da" translate="yes" xml:space="preserve">
          <source>Imported 864913 records in 16.27
  seconds</source>
          <target state="translated">16.27秒内导入864913条记录。</target>
        </trans-unit>
        <trans-unit id="8df99d63ed560901211f5c08656f730de84694b0" translate="yes" xml:space="preserve">
          <source>Imported 864913 records in 18.13
  seconds</source>
          <target state="translated">18.13秒内导入864913条记录。</target>
        </trans-unit>
        <trans-unit id="1c548ea621ed72950e828c5b68b12bf11f6ff451" translate="yes" xml:space="preserve">
          <source>Imported 864913 records in 38.03
  seconds</source>
          <target state="translated">38.03秒内导入864913条记录。</target>
        </trans-unit>
        <trans-unit id="154309e441b9898bacf87ed35cc90b6b605fec43" translate="yes" xml:space="preserve">
          <source>Imported 864913 records in 8.94
  seconds</source>
          <target state="translated">8.94秒内导入864913条记录。</target>
        </trans-unit>
        <trans-unit id="e912b781943a231d89776110872a3ee052f22c58" translate="yes" xml:space="preserve">
          <source>Imported 864913 records in 9933.61
  seconds</source>
          <target state="translated">在9933.61秒内导入864913条记录。</target>
        </trans-unit>
        <trans-unit id="5c0542021a1c71e3d1128b2a31c3b68cde8950f7" translate="yes" xml:space="preserve">
          <source>Improve INSERT-per-second performance of SQLite</source>
          <target state="translated">提高SQLite的INSERT-per-second性能</target>
        </trans-unit>
        <trans-unit id="437377a502bface456863ef3ef3ca4aa7c7b5e37" translate="yes" xml:space="preserve">
          <source>In Addition to my answer above, you should keep in mind that inserts per second depending on the hard drive you are using too. I tested it on 3 different PCs with different hard drives and got massive differences in times. PC1 (1hr 30m), PC2 (6hrs) PC3 (14hrs), so I started wondering why would that be.</source>
          <target state="translated">除了上面我的回答之外,你应该记住,每秒钟插入的时间取决于你使用的硬盘。我在3台不同的电脑上测试了不同的硬盘,得到的时间差异很大。PC1(1小时30米),PC2(6小时),PC3(14小时),所以我开始怀疑为什么会这样。</target>
        </trans-unit>
        <trans-unit id="084c1b49fd936281f7874c2b66922cccc3363366" translate="yes" xml:space="preserve">
          <source>Inspired by this post and by the Stack Overflow question that led me here -- &lt;a href=&quot;https://stackoverflow.com/questions/1609637/is-it-possible-to-insert-multiple-rows-at-a-time-in-an-sqlite-database&quot;&gt;Is it possible to insert multiple rows at a time in an SQLite database?&lt;/a&gt; -- I've posted my first &lt;a href=&quot;http://en.wikipedia.org/wiki/Git_%28software%29&quot;&gt;Git&lt;/a&gt; repository:</source>
          <target state="translated">受到这篇文章以及导致我在此处出现的堆栈溢出问题的启发- &lt;a href=&quot;https://stackoverflow.com/questions/1609637/is-it-possible-to-insert-multiple-rows-at-a-time-in-an-sqlite-database&quot;&gt;是否可以一次在SQLite数据库中插入多行？&lt;/a&gt; -我发布了我的第一个&lt;a href=&quot;http://en.wikipedia.org/wiki/Git_%28software%29&quot;&gt;Git&lt;/a&gt;存储库：</target>
        </trans-unit>
        <trans-unit id="c680b9c6bf7b7bee16783907b9286d2edf9090e1" translate="yes" xml:space="preserve">
          <source>It's not super-practical to store our database in RAM, but it's impressive that we can perform &lt;strong&gt;79,000 inserts per second.&lt;/strong&gt;</source>
          <target state="translated">将我们的数据库存储在RAM中并不是很实际，但是令人印象深刻的是，我们&lt;strong&gt;每秒&lt;/strong&gt;可以执行&lt;strong&gt;79,000次插入。&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="a48a72fa6f2a189900ebabff54aef564cc57e5d1" translate="yes" xml:space="preserve">
          <source>Just for kicks, let's build upon all of the previous optimizations and redefine the database filename so we're working entirely in RAM:</source>
          <target state="translated">只是为了好玩,让我们在之前所有优化的基础上,重新定义数据库文件名,这样我们就完全在RAM中工作了。</target>
        </trans-unit>
        <trans-unit id="def03dbbcc838f12adc51dd0a6831295811e8a51" translate="yes" xml:space="preserve">
          <source>Let's combine the previous two optimizations. It's a little more risky (in case of a crash), but we're just importing data (not running a bank):</source>
          <target state="translated">让我们把前面的两个优化结合起来。这样做风险比较大一点(万一崩溃了),但我们只是导入数据(不是运行银行)。</target>
        </trans-unit>
        <trans-unit id="fff50fdf32382d53297912d58830059c40882d48" translate="yes" xml:space="preserve">
          <source>Link: &lt;a href=&quot;https://www.vogella.com/tutorials/AndroidSQLite/article.html&quot;&gt;https://www.vogella.com/tutorials/AndroidSQLite/article.html&lt;/a&gt;
check Using ContentProvider Section for more details</source>
          <target state="translated">链接： &lt;a href=&quot;https://www.vogella.com/tutorials/AndroidSQLite/article.html&quot;&gt;https&lt;/a&gt; : //www.vogella.com/tutorials/AndroidSQLite/article.html检查&amp;ldquo;使用ContentProvider&amp;rdquo;部分以了解更多详细信息</target>
        </trans-unit>
        <trans-unit id="cef93c612411b122f72e46695577441dc0b9538a" translate="yes" xml:space="preserve">
          <source>More detail: &lt;a href=&quot;http://www.hoogli.com/blogs/micro/index.html#Avoid_sqlite3_clear_bindings%28%29&quot;&gt;Avoid_sqlite3_clear_bindings()&lt;/a&gt;</source>
          <target state="translated">详细信息： &lt;a href=&quot;http://www.hoogli.com/blogs/micro/index.html#Avoid_sqlite3_clear_bindings%28%29&quot;&gt;避免使用_sqlite3_clear_bindings（）&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fd6066b18d7c77401d867d16f3862c393664eb4f" translate="yes" xml:space="preserve">
          <source>My solution was to use &lt;strong&gt;multiple&lt;/strong&gt; transactions. I begin and end a transaction every 10.000 records (Don't ask why that number, it was the fastest one I tested). I created an array sized 10.000 and insert the successful records there. When the error occurs, I do a rollback, begin a transaction, insert the records from my array, commit and then begin a new transaction after the broken record.</source>
          <target state="translated">我的解决方案是使用&lt;strong&gt;多个&lt;/strong&gt;事务。 我每10.000条记录开始和结束一笔交易（不要问为什么这个数字，它是我测试过的最快的）。 我创建了一个大小为10.000的数组，并在其中插入成功的记录。 当发生错误时，我进行回滚，开始事务，从数组中插入记录，提交，然后在损坏的记录之后开始新的事务。</target>
        </trans-unit>
        <trans-unit id="6b739c10e74293364ba0d24439f65294b7e5a99a" translate="yes" xml:space="preserve">
          <source>My test machine is a 3.60 GHz P4 running Windows XP.</source>
          <target state="translated">我的测试机是一台运行Windows XP的3.60 GHz P4。</target>
        </trans-unit>
        <trans-unit id="725b0e5796b8baedf9846345cfd466c118fc41d9" translate="yes" xml:space="preserve">
          <source>Nice! There's a little bit more code (don't forget to call &lt;code&gt;sqlite3_clear_bindings&lt;/code&gt; and &lt;code&gt;sqlite3_reset&lt;/code&gt;), but we've more than doubled our performance to &lt;strong&gt;53,000 inserts per second.&lt;/strong&gt;</source>
          <target state="translated">真好！ 还有更多代码（不要忘记调用 &lt;code&gt;sqlite3_clear_bindings&lt;/code&gt; 和 &lt;code&gt;sqlite3_reset&lt;/code&gt; ），但是我们的性能提高了一倍以上，达到&lt;strong&gt;每秒53,000次插入。&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="71cb481da4769935c7cb6729efb5b6f47e162d66" translate="yes" xml:space="preserve">
          <source>On bulk inserts</source>
          <target state="translated">关于散装刀片</target>
        </trans-unit>
        <trans-unit id="23976e24ccb5c88d260af95f9f2b0d95e8f2c905" translate="yes" xml:space="preserve">
          <source>Optimizing SQLite is tricky. Bulk-insert performance of a C application can vary from 85 inserts per second to over 96,000 inserts per second!</source>
          <target state="translated">SQLite的优化是很棘手的。一个C应用程序的批量插入性能从每秒85次插入到每秒超过96,000次插入都有不同的表现!</target>
        </trans-unit>
        <trans-unit id="b15b2b66aeaf59f9d4af467b7dce9e1e809af568" translate="yes" xml:space="preserve">
          <source>PRAGMA journal_mode = MEMORY</source>
          <target state="translated">PRAGMA log_mode=MEMORY</target>
        </trans-unit>
        <trans-unit id="03a9d7f69c55c2b9bb13076c851bb0c196528d38" translate="yes" xml:space="preserve">
          <source>PRAGMA synchronous = OFF</source>
          <target state="translated">PRAGMA synchronous=OFF</target>
        </trans-unit>
        <trans-unit id="e1c75641cd03f3af313aee27467012f92b04b104" translate="yes" xml:space="preserve">
          <source>PRAGMA synchronous = OFF &lt;em&gt;and&lt;/em&gt; PRAGMA journal_mode = MEMORY</source>
          <target state="translated">PRAGMA同步= OFF &lt;em&gt;和&lt;/em&gt; PRAGMA journal_mode = MEMORY</target>
        </trans-unit>
        <trans-unit id="847bfcdea40fa982b77f7b8713d9d1c6b544bfb7" translate="yes" xml:space="preserve">
          <source>Playing with page sizes makes a difference as well (&lt;code&gt;PRAGMA page_size&lt;/code&gt;). Having larger page sizes can make reads and writes go a bit faster as larger pages are held in memory. Note that more memory will be used for your database.</source>
          <target state="translated">使用页面大小也有所不同（ &lt;code&gt;PRAGMA page_size&lt;/code&gt; ）。 由于较大的页面保留在内存中，因此具有较大的页面大小可使读取和写入的速度更快。 请注意，更多的内存将用于您的数据库。</target>
        </trans-unit>
        <trans-unit id="e22067ebafc73b20c18e9861a705da211f5109b2" translate="yes" xml:space="preserve">
          <source>Prior to calling &lt;a href=&quot;https://www.sqlite.org/c3ref/step.html&quot;&gt;sqlite3_step()&lt;/a&gt; for the first time or immediately
  after &lt;a href=&quot;https://www.sqlite.org/c3ref/reset.html&quot;&gt;sqlite3_reset()&lt;/a&gt;, the application can invoke the
  &lt;a href=&quot;https://www.sqlite.org/c3ref/bind_blob.html&quot;&gt;sqlite3_bind()&lt;/a&gt; interfaces to attach values to the parameters. Each
  call to &lt;a href=&quot;https://www.sqlite.org/c3ref/bind_blob.html&quot;&gt;sqlite3_bind()&lt;/a&gt; overrides prior bindings on the same parameter</source>
          <target state="translated">在第一次调用&lt;a href=&quot;https://www.sqlite.org/c3ref/step.html&quot;&gt;sqlite3_step（）&lt;/a&gt;之前或在&lt;a href=&quot;https://www.sqlite.org/c3ref/reset.html&quot;&gt;sqlite3_reset（）&lt;/a&gt;之后立即调用，该应用程序可以调用&lt;a href=&quot;https://www.sqlite.org/c3ref/bind_blob.html&quot;&gt;sqlite3_bind（）&lt;/a&gt;接口将值附加到参数。 每次调用&lt;a href=&quot;https://www.sqlite.org/c3ref/bind_blob.html&quot;&gt;sqlite3_bind（）都会&lt;/a&gt;覆盖先前对同一参数的绑定</target>
        </trans-unit>
        <trans-unit id="d1dc6289bf46af7ecb10109083938ec233b12f44" translate="yes" xml:space="preserve">
          <source>Put inserts/updates in a transaction.</source>
          <target state="translated">把insertsupdates放在事务中。</target>
        </trans-unit>
        <trans-unit id="9d00bcd205db77802eec1f76d0bfc2a4e82e6fe2" translate="yes" xml:space="preserve">
          <source>Refactoring C Code</source>
          <target state="translated">重构C代码</target>
        </trans-unit>
        <trans-unit id="b25e791003c46490e0166b007c32a101c17271c5" translate="yes" xml:space="preserve">
          <source>Running the code as-is doesn't actually perform any database operations, but it will give us an idea of how fast the raw C file I/O and string processing operations are.</source>
          <target state="translated">按原样运行这段代码实际上不会执行任何数据库操作,但它可以让我们了解到原始C文件IO和字符串处理操作的速度。</target>
        </trans-unit>
        <trans-unit id="7801bb5ecd32d8d6426f3434d9102bfe5f65316d" translate="yes" xml:space="preserve">
          <source>Several tips:</source>
          <target state="translated">几条提示。</target>
        </trans-unit>
        <trans-unit id="bf198eba07fea151a8fb44062f5cf968092033e2" translate="yes" xml:space="preserve">
          <source>So here is where the rollback comes. The only issue with the rollback is that you lose all your inserts and start from the top. How can you solve this?</source>
          <target state="translated">所以这里就是回滚的地方了。回滚的唯一问题是,你失去了所有的插入物,要从头开始。如何解决这个问题呢?</target>
        </trans-unit>
        <trans-unit id="db1afd310be73362aa7fb7a0a2c83f7cc9be9db1" translate="yes" xml:space="preserve">
          <source>Summary (so far)</source>
          <target state="translated">摘要(迄今为止)</target>
        </trans-unit>
        <trans-unit id="fda992cd350165c2f1d88bde9d7d98e92fe66347" translate="yes" xml:space="preserve">
          <source>Take advantage of saving space...smaller databases go faster. For instance, if you have key value pairs, try making the key an &lt;code&gt;INTEGER PRIMARY KEY&lt;/code&gt; if possible, which will replace the implied unique row number column in the table.</source>
          <target state="translated">利用节省空间的优势...较小的数据库运行速度更快。 例如，如果您有键值对，请尝试尽可能将键设置为 &lt;code&gt;INTEGER PRIMARY KEY&lt;/code&gt; ，它将替换表中隐含的唯一行号列。</target>
        </trans-unit>
        <trans-unit id="c6c60bb2b9aa2a9dcf08905210c5336ed8ba1be0" translate="yes" xml:space="preserve">
          <source>That's better. Simply wrapping all of our inserts in a single transaction improved our performance to &lt;strong&gt;23,000 inserts per second.&lt;/strong&gt;</source>
          <target state="translated">这样更好 只需将所有插入物包装在一个事务中，就可以将我们的性能提高到&lt;strong&gt;每秒23,000个插入物。&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="017b6041e5e97ed016693e8cbc3c4c24484ab010" translate="yes" xml:space="preserve">
          <source>The &quot;Control&quot;</source>
          <target state="translated">&quot;控制&quot;</target>
        </trans-unit>
        <trans-unit id="88cbff81bd98a7e52e52c7d398a52f2faba9c9c8" translate="yes" xml:space="preserve">
          <source>The &quot;Worst-Case-Scenario&quot;</source>
          <target state="translated">&quot;最坏的情况下&quot;</target>
        </trans-unit>
        <trans-unit id="508178fc7a6c1685a9ce3f25f29c50b86c817345" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;https://www.sqlite.org/cintro.html&quot;&gt;C API intro&lt;/a&gt; from the SQLite docs says:</source>
          <target state="translated">SQLite文档中的&lt;a href=&quot;https://www.sqlite.org/cintro.html&quot;&gt;C API简介&lt;/a&gt;说：</target>
        </trans-unit>
        <trans-unit id="38372a1525b1bf61a2f5f0cc1ddbbf17915e0784" translate="yes" xml:space="preserve">
          <source>The ON CONFLICT command does not apply, cause if you have 10 elements in a record and you need each element inserted to a different table, if element 5 gets a CONSTRAINT error, then all previous 4 inserts need to go too.</source>
          <target state="translated">ON CONFLICT命令并不适用,因为如果你有10个元素在一条记录中,而你需要将每个元素都插入到不同的表中,如果元素5得到一个CONSTRAINT错误,那么之前的4个元素也需要去掉。</target>
        </trans-unit>
        <trans-unit id="8ea1fabbe4633c8d4db54d4c842da90c9441b97d" translate="yes" xml:space="preserve">
          <source>The algorithm I created helped me reduce my process by 2 hours. Final loading process of file 1hr 30m which is still slow but not compared to the 4hrs that it initially took. I managed to speed the inserts from 10.000/s to ~14.000/s</source>
          <target state="translated">我创建的算法帮助我缩短了2个小时。文件的最终加载过程为1小时30分钟,虽然还是很慢,但比起最初的4个小时来,还是慢了不少。我成功地将插入速度从10.000s提高到了14.000s。</target>
        </trans-unit>
        <trans-unit id="9f26c58828cd1c833c02f45b4f92100c4d5ab702" translate="yes" xml:space="preserve">
          <source>The answer to your question is that the newer SQLite&amp;nbsp;3 has improved performance, use that.</source>
          <target state="translated">您的问题的答案是，较新的SQLite 3具有改进的性能，请使用该性能。</target>
        </trans-unit>
        <trans-unit id="3987d3dbe6e66d288d9494f0c4a889acd57d2876" translate="yes" xml:space="preserve">
          <source>The code in the test sets the bindings every time through which should be enough.</source>
          <target state="translated">测试中的代码每次都会设置绑定,这应该足够了。</target>
        </trans-unit>
        <trans-unit id="03189b3c8c0eb7bf3b9cf95a8e187e585d96d60f" translate="yes" xml:space="preserve">
          <source>The code is compiled with &lt;a href=&quot;http://en.wikipedia.org/wiki/Visual_C%2B%2B#32-bit_versions&quot;&gt;Visual C++&lt;/a&gt; 2005 as &quot;Release&quot; with &quot;Full Optimization&quot; (/Ox) and Favor Fast Code (/Ot).</source>
          <target state="translated">使用&lt;a href=&quot;http://en.wikipedia.org/wiki/Visual_C%2B%2B#32-bit_versions&quot;&gt;Visual C ++&lt;/a&gt; 2005将代码编译为&amp;ldquo;发布&amp;rdquo;，&amp;ldquo;完全优化&amp;rdquo;（/ Ox）和&amp;ldquo;快捷&amp;rdquo;代码（/ Ot）。</target>
        </trans-unit>
        <trans-unit id="0ea5d284cecee101861f152c707c71f8fd74f317" translate="yes" xml:space="preserve">
          <source>The improvements are now smaller, but we're up to &lt;strong&gt;69,600 inserts per second.&lt;/strong&gt;</source>
          <target state="translated">现在，改进的幅度较小，但&lt;strong&gt;每秒&lt;/strong&gt;最多可&lt;strong&gt;插入69,600次。&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="be2689c857db56817bdb470b056a287c52a2e0c0" translate="yes" xml:space="preserve">
          <source>There is nothing in the docs for &lt;a href=&quot;https://www.sqlite.org/c3ref/clear_bindings.html&quot;&gt;&lt;code&gt;sqlite3_clear_bindings&lt;/code&gt;&lt;/a&gt; saying you must call it in addition to simply setting the bindings.</source>
          <target state="translated">在&lt;a href=&quot;https://www.sqlite.org/c3ref/clear_bindings.html&quot;&gt; &lt;code&gt;sqlite3_clear_bindings&lt;/code&gt; &lt;/a&gt;的文档中没有任何内容说除了简单地设置绑定之外，您还必须调用它。</target>
        </trans-unit>
        <trans-unit id="bf20f7e886b3170f117100780131fac01a688cee" translate="yes" xml:space="preserve">
          <source>This answer &lt;em&gt;&lt;a href=&quot;https://stackoverflow.com/questions/11769366/why-is-sqlalchemy-insert-with-sqlite-25-times-slower-than-using-sqlite3-directly/11769768#11769768&quot;&gt;Why is SQLAlchemy insert with sqlite 25 times slower than using sqlite3 directly?&lt;/a&gt;&lt;/em&gt; by SqlAlchemy Orm Author has 100k inserts in 0.5 sec, and I have seen similar results with python-sqlite and SqlAlchemy. Which leads me to believe that performance has improved with SQLite&amp;nbsp;3.</source>
          <target state="translated">这个答案&lt;em&gt;&lt;a href=&quot;https://stackoverflow.com/questions/11769366/why-is-sqlalchemy-insert-with-sqlite-25-times-slower-than-using-sqlite3-directly/11769768#11769768&quot;&gt;为什么SQLAlchemy用sqlite插入比直接使用sqlite3慢25倍？&lt;/a&gt;&lt;/em&gt; 作者：SqlAlchemy Orm作者在0.5秒内插入了10万次插入，我在python-sqlite和SqlAlchemy中也看到了类似的结果。 这使我相信，使用SQLite 3可以提高性能。</target>
        </trans-unit>
        <trans-unit id="c6adca7440b2ae1858c40b313a78614c3cd32177" translate="yes" xml:space="preserve">
          <source>This is going to be slow because the SQL will be compiled into VDBE code for every insert and every insert will happen in its own transaction. &lt;em&gt;How slow?&lt;/em&gt;</source>
          <target state="translated">这将很慢，因为对于每个插入，SQL都将被编译成VDBE代码，并且每个插入都将在其自己的事务中进行。 &lt;em&gt;有多慢&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="db63715f64683425fd01f5904246eab4178b65d8" translate="yes" xml:space="preserve">
          <source>This solution helped me bypass the issues I have when dealing with files containing bad/duplicate records (I had almost 4% bad records).</source>
          <target state="translated">这个解决方案帮助我绕过了我在处理包含不良重复记录的文件时遇到的问题(我有近4%的不良记录)。</target>
        </trans-unit>
        <trans-unit id="ed7f88668556c34e922129040d0b4cb1fff93abd" translate="yes" xml:space="preserve">
          <source>Too many or too little threads won't do it, you need to benchmark and profile yourself.</source>
          <target state="translated">线程太多或太少都不行,你需要自己做标杆,做轮廓。</target>
        </trans-unit>
        <trans-unit id="5e1b2a6b6d80f8d0e183b885c337e18d7f131c6e" translate="yes" xml:space="preserve">
          <source>Try using &lt;code&gt;SQLITE_STATIC&lt;/code&gt; instead of &lt;code&gt;SQLITE_TRANSIENT&lt;/code&gt; for those inserts.</source>
          <target state="translated">对于这些插入，请尝试使用 &lt;code&gt;SQLITE_STATIC&lt;/code&gt; 而不是 &lt;code&gt;SQLITE_TRANSIENT&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="054498b4ab93a63c9779fb5e4449fc342376cebb" translate="yes" xml:space="preserve">
          <source>Use ContentProvider for inserting the bulk data in db.
The below method used for inserting bulk data in to database. This should Improve INSERT-per-second performance of SQLite.</source>
          <target state="translated">使用ContentProvider将批量数据插入到数据库中。下面的方法用于将批量数据插入到数据库中。这应该可以提高SQLite的INSERT-per-second性能。</target>
        </trans-unit>
        <trans-unit id="86258bb2c189f1c4b45a4e393182d5e6be34a275" translate="yes" xml:space="preserve">
          <source>Using a Prepared Statement</source>
          <target state="translated">使用准备好的报表</target>
        </trans-unit>
        <trans-unit id="27bfc98a791c846a44ceefffbe8088440beb0489" translate="yes" xml:space="preserve">
          <source>Using a Transaction</source>
          <target state="translated">使用交易</target>
        </trans-unit>
        <trans-unit id="efa798e2337da1f597bb94d66f1cd12d1cc78d2f" translate="yes" xml:space="preserve">
          <source>Using a transaction was a huge improvement, but recompiling the SQL statement for every insert doesn't make sense if we using the same SQL over-and-over. Let's use &lt;code&gt;sqlite3_prepare_v2&lt;/code&gt; to compile our SQL statement once and then bind our parameters to that statement using &lt;code&gt;sqlite3_bind_text&lt;/code&gt;:</source>
          <target state="translated">使用事务是一个巨大的改进，但是如果我们反复使用相同的SQL，则对于每个插入都重新编译SQL语句是没有意义的。 让我们使用 &lt;code&gt;sqlite3_prepare_v2&lt;/code&gt; 一次编译我们的SQL语句，然后使用 &lt;code&gt;sqlite3_bind_text&lt;/code&gt; 将参数绑定到该语句：</target>
        </trans-unit>
        <trans-unit id="674be12a68db41f5e65e8f6f5288f5307643a49c" translate="yes" xml:space="preserve">
          <source>Using an In-Memory Database</source>
          <target state="translated">使用内存中的数据库</target>
        </trans-unit>
        <trans-unit id="a989490dc1086fbe60acc040f2077ee40bc910a4" translate="yes" xml:space="preserve">
          <source>We're going to generate the SQL string using the values read from the file and invoke that SQL operation using sqlite3_exec:</source>
          <target state="translated">我们将使用从文件中读取的值生成SQL字符串,并使用sqlite3_exec来调用该SQL操作。</target>
        </trans-unit>
        <trans-unit id="51a38e71a882481707d999fc28f7f0fcdeaf78ba" translate="yes" xml:space="preserve">
          <source>Yikes! 2 hours and 45 minutes! That's only &lt;strong&gt;85 inserts per second.&lt;/strong&gt;</source>
          <target state="translated">kes！ 2小时45分钟！ &lt;strong&gt;每秒&lt;/strong&gt;只有&lt;strong&gt;85次插入。&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="cc4e57454f2e22aeb719d5949b1a38ae2ba3ce02" translate="yes" xml:space="preserve">
          <source>You have to be quite careful if you have concurrent access to SQLite, as the whole database is locked when writes are done, and although multiple readers are possible, writes will be locked out. This has been improved somewhat with the addition of a WAL in newer SQLite versions.</source>
          <target state="translated">如果你对SQLite有并发访问,你必须相当小心,因为在写的时候,整个数据库都会被锁定,虽然可以有多个读取器,但写的时候会被锁定。在较新的SQLite版本中增加了WAL,这一点已经有了一定的改善。</target>
        </trans-unit>
        <trans-unit id="f84952b103a6b1fa35391ad802ceb45691b6a3c4" translate="yes" xml:space="preserve">
          <source>for each thread:</source>
          <target state="translated">对每条线来说。</target>
        </trans-unit>
        <trans-unit id="718651ac1e4d28eed11decc1cc7b263cdac1558f" translate="yes" xml:space="preserve">
          <source>then read in pages (LIMIT/OFFSET):</source>
          <target state="translated">然后以页数(LIMITOFFSET)读取。</target>
        </trans-unit>
        <trans-unit id="779e8933edaa2a8b59f69f6c48510a0f1a6cf67a" translate="yes" xml:space="preserve">
          <source>where  and  are calculated per-thread, like this:</source>
          <target state="translated">其中和是按线程计算的,就像这样。</target>
        </trans-unit>
        <trans-unit id="f4c7bc63d48e550900e1d3bed8080e1b721d1e85" translate="yes" xml:space="preserve">
          <source>which bulk loads an array of ActiveRecords into &lt;a href=&quot;http://en.wikipedia.org/wiki/MySQL&quot;&gt;MySQL&lt;/a&gt;, SQLite or &lt;a href=&quot;http://en.wikipedia.org/wiki/PostgreSQL&quot;&gt;PostgreSQL&lt;/a&gt; databases. It includes an option to ignore existing records, overwrite them or raise an error. My rudimentary benchmarks show a 10x speed improvement compared to sequential writes -- YMMV.</source>
          <target state="translated">它将大量ActiveRecords加载到&lt;a href=&quot;http://en.wikipedia.org/wiki/MySQL&quot;&gt;MySQL&lt;/a&gt; ，SQLite或&lt;a href=&quot;http://en.wikipedia.org/wiki/PostgreSQL&quot;&gt;PostgreSQL&lt;/a&gt;数据库中。 它包括一个忽略现有记录，覆盖它们或引发错误的选项。 我的基本基准显示，与顺序写入YMMV相比，速度提高了10倍。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
